diff --git a/python/torch_mlir/extras/fx_importer.py b/python/torch_mlir/extras/fx_importer.py
index 91d81de0..6f5e041f 100644
--- a/python/torch_mlir/extras/fx_importer.py
+++ b/python/torch_mlir/extras/fx_importer.py
@@ -54,6 +54,10 @@ from torch._subclasses import (
     FakeTensor as TorchFakeTensor,
 )
 
+from torch.distributed._functional_collectives import (
+    AsyncCollectiveTensor as TorchAsyncCollectiveTensor
+)
+
 from torch.fx import (
     Graph,
     GraphModule,
@@ -2096,6 +2100,8 @@ def _make_vtensor_literal_op(
 ) -> Operation:
     mapping = py_attr_tracker.track(tensor)
     if mapping.is_empty:
+        # unwrap from TorchAsyncCollectiveTensor
+        tensor = tensor.elem if isinstance(tensor, TorchAsyncCollectiveTensor) else tensor
         # check support for bfloat16
         assert not (
             tensor.dtype == torch.bfloat16 and ml_dtypes is None
@@ -2111,11 +2117,17 @@ def _make_vtensor_literal_op(
         # detach() which throws an error as we are operating in a FakeTensorMode, hence the simplest way to get this raw
         # buffer is via the indirection: Tensor -> list -> numpy array. This allows us to create a vtensor literal as
         # desired, but also limits which data types we can support in this function (see TORCH_DTYPE_TO_NPY_TYPE above)
-        np_tensor = np.array(tensor.tolist()).astype(npy_dtype)
+
+        # NOTE: if we torch.export a torch.nn.Module under fake mode, the parameters in the fx.GraphModule will be FakeTensor.
+        # So we specifically handle FakeTensor here by randomly generating a tensor of the same shape and dtype.
+        if isinstance(tensor, TorchFakeTensor):
+            np_tensor = np.random.rand(*list(tensor.shape)).astype(npy_dtype)
+        else:
+            np_tensor = np.array(tensor.tolist()).astype(npy_dtype)
         # One element constants are more optimizable as splat DenseElementsAttr. DenseResourceElementsAttr does not
         # support splats, so don't use it for that case. In addition, at the time of writing, it has bugs with handling
         # 0d tensors.
-        if np_tensor.size == 1:
+        if True:
             try:
                 dtype = tensor.dtype
                 element_type = TORCH_DTYPE_TO_MLIR_TYPE[dtype]()
