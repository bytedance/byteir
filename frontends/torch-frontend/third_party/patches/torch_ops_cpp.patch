diff --git a/lib/Dialect/Torch/IR/TorchOps.cpp b/lib/Dialect/Torch/IR/TorchOps.cpp
index a8769def..5883bc17 100644
--- a/lib/Dialect/Torch/IR/TorchOps.cpp
+++ b/lib/Dialect/Torch/IR/TorchOps.cpp
@@ -1847,6 +1847,81 @@ OpFoldResult AtenTruncOp::fold(FoldAdaptor adaptor) {
   return {};
 }
 
+//===----------------------------------------------------------------------===//
+// AtenLogOp
+//===----------------------------------------------------------------------===//
+
+using UnaryPromoteFpOperator = std::function<double(double)>;
+using UnaryPromoteIntOperator = std::function<double(APInt, bool)>;
+
+static OpFoldResult unaryPromoteFolder(DenseElementsAttr operand,
+                                       ValueTensorType resultTy,
+                                       UnaryPromoteFpOperator fpFolder,
+                                       UnaryPromoteIntOperator intFolder) {
+  constexpr int64_t kMaxFold = 16;
+  if (!resultTy.hasDtype() || !resultTy.hasSizes())
+    return nullptr;
+  if (!isa<mlir::FloatType>(resultTy.getDtype()))
+    return nullptr;
+
+  auto fpTy = dyn_cast<mlir::FloatType>(operand.getType().getElementType());
+  auto intTy = dyn_cast<mlir::IntegerType>(operand.getType().getElementType());
+  if (!fpTy && !intTy)
+    return nullptr;
+
+  auto resultBTy = resultTy.toBuiltinTensor().clone(resultTy.getDtype());
+  bool splat = operand.isSplat();
+  bool withinMaxFold =
+      resultBTy.hasStaticShape() && resultBTy.getNumElements() <= kMaxFold;
+  if (!splat && !withinMaxFold)
+    return nullptr;
+
+  const int64_t numValues = splat ? 1 : resultBTy.getNumElements();
+
+  llvm::SmallVector<Attribute> operands = {operand};
+  llvm::SmallVector<APFloat> folded;
+  for (int i = 0, s = numValues; i < s; ++i) {
+    double fold = 0.0;
+    if (fpTy) {
+      auto inputs = getFoldValueAtIndexFp(operands, i);
+      fold = fpFolder(inputs[0]);
+    }
+    if (intTy) {
+      auto inputs =
+          getFoldValueAtIndexInt(operands, intTy.getIntOrFloatBitWidth(), i);
+      fold = intFolder(inputs[0], intTy.isSigned());
+    }
+
+    APFloat val(fold);
+    bool unused;
+    val.convert(
+        cast<mlir::FloatType>(resultBTy.getElementType()).getFloatSemantics(),
+        APFloat::rmNearestTiesToEven, &unused);
+    folded.push_back(val);
+  }
+  return DenseElementsAttr::get(resultBTy, folded);
+}
+
+OpFoldResult AtenLogOp::fold(FoldAdaptor adaptor) {
+  auto self = dyn_cast_or_null<DenseElementsAttr>(adaptor.getSelf());
+  auto resultType = dyn_cast<ValueTensorType>(getType());
+  if (!self || !resultType)
+    return nullptr;
+
+  // Note that i1 is neither signed nor unsigned.
+  // But we should trait i1 as unsigned, otherwise that
+  // APInt(1,1).getSExtValue() return allOnes 64-bit integer.
+  auto intFold = [](APInt a, bool isSigned) -> double {
+    if (isSigned)
+      return std::log(a.getSExtValue());
+    else
+      return std::log(a.getZExtValue());
+  };
+  auto fpFold = [](double a) -> double { return std::log(a); };
+
+  return unaryPromoteFolder(self, resultType, fpFold, intFold);
+}
+
 //===----------------------------------------------------------------------===//
 // AtenSignOp
 //===----------------------------------------------------------------------===//
@@ -1957,14 +2032,21 @@ void AtenScalarImplicitOp::getCanonicalizationPatterns(
     Location loc = op.getLoc();
     Value a = op.getA();
     auto outType = op.getResult().getType();
-    Value scalarValue = getScalarIntValue(a, loc, rewriter);
-    if (!scalarValue)
-      return failure();
-    rewriter.replaceOpWithNewOp<Torch::DerefineOp>(op, outType, scalarValue);
-    return success();
+    Value scalarIntValue = getScalarIntValue(a, loc, rewriter);
+    if (scalarIntValue) {
+      rewriter.replaceOpWithNewOp<Torch::DerefineOp>(op, outType,
+                                                     scalarIntValue);
+      return success();
+    }
+    Value scalarFloatValue = getScalarFloatValue(a, loc, rewriter);
+    if (scalarFloatValue) {
+      rewriter.replaceOpWithNewOp<Torch::DerefineOp>(op, outType,
+                                                     scalarFloatValue);
+      return success();
+    }
+    return failure();
   });
 }
-
 //===----------------------------------------------------------------------===//
 // AtenFloatImplicitOp
 //===----------------------------------------------------------------------===//
