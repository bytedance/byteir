diff --git a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
index c669e5ec..26c392b8 100644
--- a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
@@ -71,6 +71,11 @@ struct MhloDialectOp<ONNXMulOp> {
   using Op = mhlo::MulOp;
 };
 
+template <>
+struct MhloDialectOp<ONNXNegOp> {
+  using Op = mhlo::NegOp;
+};
+
 template <>
 struct MhloDialectOp<ONNXPowOp> {
   using Op = mhlo::PowOp;
@@ -81,6 +86,11 @@ struct MhloDialectOp<ONNXSigmoidOp> {
   using Op = mhlo::LogisticOp;
 };
 
+template <>
+struct MhloDialectOp<ONNXSinOp> {
+  using Op = mhlo::SineOp;
+};
+
 template <>
 struct MhloDialectOp<ONNXSqrtOp> {
   using Op = mhlo::SqrtOp;
@@ -321,7 +331,9 @@ void populateLoweringONNXElementwiseOpToMhloPattern(
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXExpOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLeakyReluOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLogOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXNegOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXSigmoidOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXSinOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXSqrtOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXReluOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXTanhOp>,
diff --git a/src/Conversion/ONNXToMhlo/Math/Reduction.cpp b/src/Conversion/ONNXToMhlo/Math/Reduction.cpp
index 3b734e0b..66f6e438 100644
--- a/src/Conversion/ONNXToMhlo/Math/Reduction.cpp
+++ b/src/Conversion/ONNXToMhlo/Math/Reduction.cpp
@@ -86,10 +86,9 @@ llvm::SmallVector<int64_t, 4> getDefinedAxes<ONNXReduceSumOp>(Operation *op) {
   // Assume it is verified that axes are known. Convert DenseElementsAttr to
   // ArrayAttr.
   if (!isFromNone(axesValue) && getONNXConstantOp(axesValue)) {
-    mlir::DenseElementsAttr constAxes =
+    mlir::ElementsAttr constAxes =
         getONNXConstantOp(axesValue)
-            .getValueAttr()
-            .dyn_cast_or_null<mlir::DenseElementsAttr>();
+            .getValueAttr().dyn_cast_or_null<mlir::ElementsAttr>();
     for (mlir::IntegerAttr element : constAxes.getValues<IntegerAttr>())
       definedAxes.push_back(element.getInt());
     return definedAxes;
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Constant.cpp b/src/Conversion/ONNXToMhlo/Tensor/Constant.cpp
index 910c0d40..49ebfee0 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Constant.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Constant.cpp
@@ -13,6 +13,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ElementsAttr/DisposableElementsAttr.hpp"
 
 using namespace mlir;
 
@@ -32,8 +33,12 @@ struct ONNXConstantOpLoweringToMhlo : public ConversionPattern {
     if (constantOp.getSparseValue().has_value())
       return constantOp.emitWarning("Only support dense values at this time");
     assert(constantOp.getValue().has_value() && "Value is not set");
-    Value result =
-        rewriter.create<mhlo::ConstantOp>(loc, constantOp.getValue().value());
+    Value result;
+    auto attr = constantOp.getValue().value();
+    if (auto disposable = attr.dyn_cast<DisposableElementsAttr>())
+      result = rewriter.create<mhlo::ConstantOp>(loc, disposable.toDenseElementsAttr());
+    else
+      result = rewriter.create<mhlo::ConstantOp>(loc, attr);
     rewriter.replaceOp(op, result);
     return success();
   }
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
index 6352e180..5c35110e 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
@@ -68,10 +68,8 @@ struct ONNXExpandOpLoweringToMhlo : public ConversionPattern {
     } else if (ONNXConstantOp shapeOp =
                    dyn_cast_or_null<ONNXConstantOp>(shapeDefOp)) {
       llvm::SmallVector<int64_t, 4> shapeValues;
-      mlir::DenseElementsAttr constShape =
-          getONNXConstantOp(shapeOp)
-              .getValueAttr()
-              .dyn_cast_or_null<mlir::DenseElementsAttr>();
+      mlir::ElementsAttr constShape =
+          shapeOp.getValueAttr().dyn_cast<ElementsAttr>();
       for (mlir::IntegerAttr element : constShape.getValues<IntegerAttr>())
         shapeValues.push_back(element.getInt());
       RankedTensorType broadcastedType =
diff --git a/src/Dialect/ONNX/ONNXOps/Quantize/DequantizeLinear.cpp b/src/Dialect/ONNX/ONNXOps/Quantize/DequantizeLinear.cpp
index 4ead6093..3f9002d2 100644
--- a/src/Dialect/ONNX/ONNXOps/Quantize/DequantizeLinear.cpp
+++ b/src/Dialect/ONNX/ONNXOps/Quantize/DequantizeLinear.cpp
@@ -68,8 +68,6 @@ LogicalResult ONNXDequantizeLinearOpShapeHelper::computeShape() {
     if (!outputDims[a].isLiteral()) {
       outputDims[a] = LiteralIndexExpr(d);
     }
-    llvm::dbgs() << "literal: " << outputDims[a].getLiteral() << " d = " << d
-                 << "\n";
     // Checked in verify.
     assert(outputDims[a].getLiteral() == d &&
            "x_scale and x_zero_point 1-D tensor length must match the input "
