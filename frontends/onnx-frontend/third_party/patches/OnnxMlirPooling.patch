diff --git a/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp b/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
index 3dc78f88..769b1813 100644
--- a/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
@@ -39,10 +39,30 @@ static Value createInitialValueForPoolingOp(
             APFloat::getZero(elemType.cast<FloatType>().getFloatSemantics(),
                 /*isNegative=*/false)));
   }
+  if (isa<ONNXCumSumOp>(op)) {
+    // returns zeros
+    auto constType = RankedTensorType::get({}, elemType);
+    if (isa<mlir::FloatType>(elemType)) {
+      auto constAttr = DenseElementsAttr::get(constType,
+          {APFloat::getZero(cast<mlir::FloatType>(elemType).getFloatSemantics(),
+              /*negative=*/false)});
+      return rewriter.create<stablehlo::ConstantOp>(
+          op->getLoc(), constType, constAttr);
+    } else if (isa<mlir::IntegerType>(elemType)) {
+      auto constAttr = DenseElementsAttr::get(
+          constType, {APInt::getZero(elemType.getIntOrFloatBitWidth())});
+      return rewriter.create<stablehlo::ConstantOp>(
+          op->getLoc(), constType, constAttr);
+    }
+  }
   op->emitError("unimplemented lowering for onnx pooling op\n");
   return nullptr;
 }
 
+inline int64_t positiveDim(int64_t dim, int64_t inputRank) {
+  return dim >= 0 ? dim : dim + inputRank;
+}
+
 // Builds body for reduce op by using the template binary op as the
 // reducer op.
 template <typename Op>
@@ -215,6 +235,92 @@ struct ONNXPoolOpLoweringToStablehlo : public ConversionPattern {
   }
 };
 
+//===----------------------------------------------------------------------===//
+// Convert ONNXCumSumOp to stablehlo.
+//===----------------------------------------------------------------------===//
+struct ONNXCumSumOpLowering : public ConversionPattern {
+  ONNXCumSumOpLowering(MLIRContext *ctx)
+      : ConversionPattern(ONNXCumSumOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+
+    Location loc = op->getLoc();
+    ONNXCumSumOpAdaptor adaptor(operands);
+    ONNXCumSumOp cumSumOp = llvm::cast<ONNXCumSumOp>(op);
+
+    Value input = adaptor.getX();
+    Value axis = adaptor.getAxis();
+    bool exclusive = cumSumOp.getExclusive() == 1;
+    bool reverse = cumSumOp.getReverse() == 1;
+
+    if (exclusive || reverse) {
+      return op->emitError("Not support exclusive and reverse yet while "
+                           "lowering ONNXCumSumOp to Stablehlo");
+    }
+
+    auto inputTy = cast<RankedTensorType>(input.getType());
+    auto outTy = cast<RankedTensorType>(cumSumOp.getType());
+    auto inputElemTy = inputTy.getElementType();
+    auto inputRank = inputTy.getRank();
+    auto inputShape = inputTy.getShape();
+
+    Value initVal = createInitialValueForPoolingOp(op, inputElemTy, rewriter);
+    SmallVector<int64_t> stablehloKernelSize(inputRank, 1);
+    DenseIntElementsAttr dimAttr;
+    if (!matchPattern(axis, m_Constant(&dimAttr))) {
+      return op->emitError("Not support dynamic axis yet.");
+    }
+    int64_t dim = dimAttr.getValues<APInt>()[0].getSExtValue();
+    dim = positiveDim(dim, inputRank);
+
+    if (inputTy.isDynamicDim(dim)) {
+      return op->emitError("Not support dynamic dim yet.");
+    }
+
+    stablehloKernelSize[dim] = inputShape[dim];
+    SmallVector<int64_t> stablehloStride(inputRank, 1);
+    SmallVector<int64_t> stablehloDilation(inputRank, 1);
+    SmallVector<int64_t> stablehloPadding(inputRank * 2, 0);
+    stablehloPadding[dim * 2] = inputShape[dim] - 1;
+
+    auto windowDimensions = rewriter.getI64TensorAttr(stablehloKernelSize);
+    auto windowStrides = rewriter.getI64TensorAttr(stablehloStride);
+    DenseIntElementsAttr baseDilations;
+    auto windowDilations = rewriter.getI64TensorAttr(stablehloDilation);
+    DenseIntElementsAttr pad = DenseIntElementsAttr::get(
+        RankedTensorType::get(
+            {static_cast<int64_t>(inputRank), static_cast<int64_t>(2)},
+            rewriter.getI64Type()),
+        stablehloPadding);
+
+    auto reduceWindowSum = rewriter.create<stablehlo::ReduceWindowOp>(
+        op->getLoc(), outTy, input, initVal, windowDimensions, windowStrides,
+        baseDilations, windowDilations, pad);
+
+    Block &sumBlock = reduceWindowSum.getBody().emplaceBlock();
+
+    // Add arguments
+    auto blockArgumentType = RankedTensorType::get({}, inputElemTy);
+    sumBlock.addArgument(blockArgumentType, op->getLoc());
+    sumBlock.addArgument(blockArgumentType, op->getLoc());
+    auto *firstArg = sumBlock.args_begin();
+    auto *secondArg = std::next(firstArg);
+
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&sumBlock);
+
+      Value sumResult = rewriter.create<stablehlo::AddOp>(
+          op->getLoc(), *firstArg, *secondArg);
+      rewriter.create<stablehlo::ReturnOp>(op->getLoc(), sumResult);
+    }
+
+    rewriter.replaceOp(op, reduceWindowSum.getResults());
+    return success();
+  }
+};
+
 } // namespace
 
 void populateLoweringONNXPoolingOpToStablehloPattern(
@@ -223,6 +329,7 @@ void populateLoweringONNXPoolingOpToStablehloPattern(
       ONNXMaxPoolSingleOutOpAdaptor, ONNXMaxPoolSingleOutOpShapeHelper>>(ctx);
   patterns.insert<ONNXPoolOpLoweringToStablehlo<ONNXAveragePoolOp,
       ONNXAveragePoolOpAdaptor, ONNXAveragePoolOpShapeHelper>>(ctx);
+  patterns.insert<ONNXCumSumOpLowering>(ctx);
 }
 
 } // namespace onnx_mlir
