diff --git a/src/Conversion/ONNXToMhlo/CMakeLists.txt b/src/Conversion/ONNXToMhlo/CMakeLists.txt
index bd7283a9..20756e93 100644
--- a/src/Conversion/ONNXToMhlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToMhlo/CMakeLists.txt
@@ -38,14 +38,20 @@ add_onnx_mlir_library(OMONNXToMhlo
   NN/ConvTranspose.cpp
   NN/Normalization.cpp
   NN/Pooling.cpp
+  RNN/LSTM.cpp
+  RNN/RNNBase.cpp
   Tensor/ArgMax.cpp
   Tensor/Concat.cpp
   Tensor/Constant.cpp
   Tensor/Expand.cpp
   Tensor/Flatten.cpp
   Tensor/Gather.cpp
+  Tensor/GatherElements.cpp
   Tensor/Identity.cpp
+  Tensor/OneHot.cpp
+  Tensor/Pad.cpp
   Tensor/Reshape.cpp
+  Tensor/ScatterND.cpp
   Tensor/Shape.cpp
   Tensor/Slice.cpp
   Tensor/Split.cpp
@@ -75,7 +81,3 @@ target_link_libraries(MhloDialect PUBLIC
 target_link_libraries(StablehloTypeInference PUBLIC
   StablehloBase
   )
-
-target_link_libraries(StablehloTypeInference PUBLIC
-  StablehloBase
-  )
diff --git a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
index 73330c37..7471748a 100644
--- a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
+++ b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
@@ -34,6 +34,8 @@ void populateONNXToMhloConversionPattern(
   populateLoweringONNXConvTransposeOpToMhloPattern(patterns, ctx);
   populateLoweringONNXNormalizationOpToMhloPattern(patterns, ctx);
   populateLoweringONNXPoolingOpToMhloPattern(patterns, ctx);
+  // Recurrent neural network
+  populateLoweringONNXLSTMOpToMhloPattern(patterns, ctx);
   // Tensor
   populateLoweringONNXArgMaxOpToMhloPattern(patterns, ctx);
   populateLoweringONNXConcatOpToMhloPattern(patterns, ctx);
@@ -41,8 +43,12 @@ void populateONNXToMhloConversionPattern(
   populateLoweringONNXExpandOpToMhloPattern(patterns, ctx);
   populateLoweringONNXFlattenOpToMhloPattern(patterns, ctx);
   populateLoweringONNXGatherOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXGatherElementsOpToMhloPattern(patterns, ctx);
   populateLoweringONNXIdentityOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXOneHotOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXPadOpToMhloPattern(patterns, ctx);
   populateLoweringONNXReshapeOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXScatterNDOpToMhloPattern(patterns, ctx);
   populateLoweringONNXShapeOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSliceOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSplitOpToMhloPattern(patterns, ctx);
@@ -89,10 +95,8 @@ void FrontendToMhloLoweringPass::runOnOperation() {
   // Added affine as some affine maps are generated by IndexExpression. It could
   // be disabled and/or replaced by shape max/min.
   target.addLegalDialect<mhlo::MhloDialect, func::FuncDialect,
-      arith::ArithDialect, shape::ShapeDialect, mlir::affine::AffineDialect>();
-  // Needed to support unsigned int computations. To be removed if we use a
-  // scheme that does not rely on the UnrealizedConversionCastOp.
-  target.addLegalOp<::mlir::UnrealizedConversionCastOp>();
+      arith::ArithDialect, shape::ShapeDialect, affine::AffineDialect,
+      tensor::TensorDialect>();

   // Now that the conversion target has been defined, we just need to provide
   // the set of patterns that will lower the frontend operations.
diff --git a/src/Conversion/ONNXToMhlo/DialectBuilder.cpp b/src/Conversion/ONNXToMhlo/DialectBuilder.cpp
index eae15683..a0547044 100644
--- a/src/Conversion/ONNXToMhlo/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToMhlo/DialectBuilder.cpp
@@ -13,6 +13,7 @@
 //===----------------------------------------------------------------------===//

 #include "mlir/Dialect/Arith/IR/Arith.h"
+#include "llvm/ADT/TypeSwitch.h"

 #include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
 #include "src/Dialect/ONNX/ONNXOps.hpp"
@@ -22,6 +23,62 @@ using namespace mlir;

 namespace onnx_mlir {

+Value MhloBuilder::constant(mlir::Type type, double val) const {
+  Value constant = nullptr;
+  // Could be a vector type; look at the element type.
+  Type elementType = type;
+  VectorType vectorType = type.dyn_cast<VectorType>();
+  if (vectorType)
+    elementType = vectorType.getElementType();
+  TypeSwitch<Type>(elementType)
+      .Case<Float16Type>([&](Type) {
+        constant =
+            b().create<mhlo::ConstantOp>(loc(), b().getF16FloatAttr(val));
+      })
+      .Case<Float32Type>([&](Type) {
+        constant =
+            b().create<mhlo::ConstantOp>(loc(), b().getF32FloatAttr(val));
+      })
+      .Case<Float64Type>([&](Type) {
+        constant =
+            b().create<mhlo::ConstantOp>(loc(), b().getF64FloatAttr(val));
+      })
+      .Case<IntegerType>([&](IntegerType elementType) {
+        assert(val == (int64_t)val && "value is ambiguous");
+        unsigned width = elementType.getWidth();
+
+        if (width == 1)
+          constant =
+              b().create<mhlo::ConstantOp>(loc(), b().getBoolAttr(val != 0));
+        else {
+          if (elementType.isUnsignedInteger()) {
+            constant = b().create<mhlo::ConstantOp>(loc(),
+                b().getIntegerAttr(elementType, APInt(width, (uint64_t)val, false)));
+          } else {
+            constant = b().create<mhlo::ConstantOp>(loc(),
+                b().getIntegerAttr(elementType, APInt(width, (int64_t)val, true)));
+          }
+        }
+      })
+      .Case<IndexType>([&](Type elementType) {
+        constant = b().create<mhlo::ConstantOp>(
+            loc(), b().getIntegerAttr(elementType, val));
+      })
+      .Default([](Type) { llvm_unreachable("unsupported element type"); });
+
+  assert(constant != nullptr && "Expecting valid constant value");
+  return constant;
+}
+
+Value MhloBuilder::constantIndex(int64_t val) const {
+  IntegerAttr constantAttr = b().getIntegerAttr(b().getIndexType(), val);
+  return b().create<mhlo::ConstantOp>(loc(), constantAttr);
+}
+
+Value MhloBuilder::shaped_zero(mlir::Type type) const {
+  return b().create<mhlo::ConstantOp>(loc(), b().getZeroAttr(type));
+}
+
 // =============================================================================
 // IndexExpr Builder for Lowering using Shape/MHLO Dialect.
 // =============================================================================
diff --git a/src/Conversion/ONNXToMhlo/DialectBuilder.hpp b/src/Conversion/ONNXToMhlo/DialectBuilder.hpp
index 7d7ee86d..143e2ad2 100644
--- a/src/Conversion/ONNXToMhlo/DialectBuilder.hpp
+++ b/src/Conversion/ONNXToMhlo/DialectBuilder.hpp
@@ -26,6 +26,34 @@

 namespace onnx_mlir {

+// =============================================================================
+// mhlo Builder
+// =============================================================================
+
+struct MhloBuilder : DialectBuilder {
+  MhloBuilder(mlir::Location loc) : DialectBuilder(loc) {}
+  MhloBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : DialectBuilder(b, loc), patternRewriter(&b) {}
+  MhloBuilder(const DialectBuilder &db) : DialectBuilder(db) {}
+  virtual ~MhloBuilder() {}
+
+  // ConstantOp
+  mlir::Value constant(mlir::Type type, double val) const;
+  mlir::Value constantIndex(int64_t val) const;
+  mlir::Value shaped_zero(mlir::Type type) const;
+
+protected:
+
+  // Private getters of builder (concise version).
+  mlir::OpBuilder &rewriter() const {
+    assert(patternRewriter && "rewriter is null");
+    return *patternRewriter;
+  }
+
+private:
+  mlir::OpBuilder *patternRewriter;
+};
+
 // =============================================================================
 // IndexExpr Builder for Shape lowering
 // =============================================================================
@@ -43,6 +71,22 @@ protected:
   mlir::Value getShapeVal(mlir::Value tensorOrMemrefValue, uint64_t i) final;
 };

+// =============================================================================
+// MultiDialectBuilder for Mhlo
+// =============================================================================
+
+// Recursive class specialized for MhloBuilder referred to as
+// mhlo.
+template <class... Ts>
+struct MultiDialectBuilder<MhloBuilder, Ts...>
+    : MultiDialectBuilder<Ts...> {
+  MultiDialectBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : MultiDialectBuilder<Ts...>(b, loc), mhlo(b, loc) {}
+  MultiDialectBuilder(const DialectBuilder &db)
+      : MultiDialectBuilder<Ts...>(db), mhlo(db) {}
+  MhloBuilder mhlo;
+};
+
 // Recursive class specialized for AffineBuilder refereed to as affine.
 template <class... Ts>
 struct MultiDialectBuilder<IndexExprBuilderForMhlo, Ts...>
diff --git a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
index 26c392b8..9d958ef5 100644
--- a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
@@ -66,6 +66,11 @@ struct MhloDialectOp<ONNXMaxOp> {
   using Op = mhlo::MaxOp;
 };

+template <>
+struct MhloDialectOp<ONNXMinOp> {
+  using Op = mhlo::MinOp;
+};
+
 template <>
 struct MhloDialectOp<ONNXMulOp> {
   using Op = mhlo::MulOp;
@@ -106,6 +111,11 @@ struct MhloDialectOp<ONNXTanhOp> {
   using Op = mhlo::TanhOp;
 };

+template <>
+struct MhloDialectOp<ONNXWhereOp> {
+  using Op = mhlo::SelectOp;
+};
+
 namespace {

 template <typename ONNXOp>
@@ -293,6 +303,40 @@ struct ONNXElementwiseBinaryOpLoweringToMhlo : public ConversionPattern {
   }
 };

+// ONNXPReluOp(x) = alpha * x if x < 0 else x.
+template <>
+struct ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPReluOp>
+    : public ConversionPattern {
+  ONNXElementwiseBinaryOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXPReluOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    // Prior code here used the "analysis" version that did not generate code.
+    // Since code is actually not needed here at this time, one could use
+    // IndexExprBuilderForAnalysis createIE(loc) instead.
+    IndexExprBuilderForMhlo createShapeIE(rewriter, loc);
+    ONNXBroadcastOpShapeHelper shapeHelper(op, operands, &createShapeIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    int64_t outputRank = shapeHelper.outputRank;
+    llvm::SmallVector<Value, 4> broadcastedOperands =
+        getBroadcastedOperands(op, rewriter, loc, outputRank);
+    Value inp = broadcastedOperands[0];
+    Value broadcastedSlope = broadcastedOperands[1];
+    Type resultType = *op->result_type_begin();
+    Value PReluActivationVal =
+        rewriter.create<mhlo::MulOp>(loc, inp, broadcastedSlope);
+    Value broadcastedZero = getShapedZero(loc, rewriter, inp);
+    Value compareGtZero = rewriter.create<mhlo::CompareOp>(
+        loc, inp, broadcastedZero, mhlo::ComparisonDirection::GT);
+    Value resultOp = rewriter.create<mhlo::SelectOp>(
+        loc, resultType, compareGtZero, inp, PReluActivationVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
 // Element-wise variadic ops lowering to Mhlo dialect.
 //===----------------------------------------------------------------------===//
 template <typename ElementwiseVariadicOp>
@@ -343,12 +387,15 @@ void populateLoweringONNXElementwiseOpToMhloPattern(
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXLessOp>,
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXLessOrEqualOp>,
       ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPowOp>,
+      ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPReluOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXAddOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXAndOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXDivOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMaxOp>,
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMinOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMulOp>,
-      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXSubOp>>(ctx);
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXSubOp>,
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXWhereOp>>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
index 4e4adfc1..1f10571f 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
@@ -14,6 +14,9 @@
 //===----------------------------------------------------------------------===//

 #include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/OpHelper.hpp"
+#include "src/Dialect/ONNX/OnnxElementsAttrBuilder.hpp"
+
 #include "stablehlo/dialect/BroadcastUtils.h"

 using namespace mlir;
@@ -44,11 +47,6 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(Operation *op,
   Type outputType = *op->result_type_begin();
   assert(outputType.isa<ShapedType>() && "output type is not shaped");
   ShapedType outputShapedType = outputType.cast<ShapedType>();
-  Type elementType =
-      op->getOperands()[0].getType().dyn_cast<ShapedType>().getElementType();
-  RankedTensorType broadcastedOutputType =
-      RankedTensorType::get(outputShapedType.getShape(), elementType);
-
   Value resultExtents =
       mlir::hlo::computeNaryElementwiseBroadcastingResultExtents(
           loc, op->getOperands(), rewriter);
@@ -58,6 +56,10 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(Operation *op,
     assert(operandType != nullptr && "operand type is not ranked");
     SmallVector<int64_t, 4> broadcastDimensions = llvm::to_vector<4>(
         llvm::seq<int64_t>(outputRank - operandType.getRank(), outputRank));
+    Type elementType =
+        operand.getType().dyn_cast<ShapedType>().getElementType();
+    RankedTensorType broadcastedOutputType =
+        RankedTensorType::get(outputShapedType.getShape(), elementType);
     Value broadcast = rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc,
         broadcastedOutputType, operand, resultExtents,
         rewriter.getI64TensorAttr(broadcastDimensions));
@@ -72,11 +74,6 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   llvm::SmallVector<Value, 4> broadcastedOperands;
   assert(outputType.isa<ShapedType>() && "output type is not shaped");
   ShapedType outputShapedType = outputType.cast<ShapedType>();
-  Type elementType =
-      operands[0].getType().dyn_cast<ShapedType>().getElementType();
-  RankedTensorType broadcastedOutputType =
-      RankedTensorType::get(outputShapedType.getShape(), elementType);
-
   Value resultExtents =
       mlir::hlo::computeNaryElementwiseBroadcastingResultExtents(
           loc, operands, rewriter);
@@ -86,6 +83,10 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     assert(operandType != nullptr && "operand type is not ranked");
     SmallVector<int64_t, 4> broadcastDimensions = llvm::to_vector<4>(
         llvm::seq<int64_t>(outputRank - operandType.getRank(), outputRank));
+    Type elementType =
+        operand.getType().dyn_cast<ShapedType>().getElementType();
+    RankedTensorType broadcastedOutputType =
+        RankedTensorType::get(outputShapedType.getShape(), elementType);
     Value broadcast = rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc,
         broadcastedOutputType, operand, resultExtents,
         rewriter.getI64TensorAttr(broadcastDimensions));
@@ -93,4 +94,142 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   }
   return broadcastedOperands;
 }
+
+ElementsAttr getElementAttributeFromConstValue(Value value) {
+  auto definingOp = value.getDefiningOp();
+  if (auto constantOp = dyn_cast_or_null<mhlo::ConstantOp>(definingOp)) {
+    return constantOp.getValue().dyn_cast<ElementsAttr>();
+  } else if (auto constantOp =
+                 dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+    if (constantOp.getValue().has_value())
+      return constantOp.getValueAttr().dyn_cast<ElementsAttr>();
+  }
+  return nullptr;
+}
+
+DenseIntElementsAttr GetI64ElementsAttr(
+    ArrayRef<int64_t> values, Builder *builder) {
+  RankedTensorType ty = RankedTensorType::get(
+      {static_cast<int64_t>(values.size())}, builder->getIntegerType(64));
+  return DenseIntElementsAttr::get(ty, values);
+}
+
+namespace {
+// Returns the DenseElementsAttr of input if it's a mhlo constant or
+// onnx.Constant. Otherwise returns a nullptr attribute.
+DenseElementsAttr getDenseElementAttrFromConstValue(mlir::Value value) {
+  Operation *definingOp = value.getDefiningOp();
+  if (auto globalOp = dyn_cast_or_null<mhlo::ConstantOp>(definingOp)) {
+    return globalOp.getValueAttr().dyn_cast<DenseElementsAttr>();
+  } else if (auto constOp = dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+    if (constOp.getValue().has_value())
+      return constOp.getValueAttr().dyn_cast<DenseElementsAttr>();
+  }
+  return nullptr;
+}
+} // namespace
+
+// Emit an ONNXSqueezeOp. If the input is constant, do const propagation,
+/// and return a constant.
+Value foldOrEmitONNXSqueezeOpMhlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr squeezedElements = inputElements.reshape(tensorType);
+    Value constVal = create.onnx.constant(squeezedElements);
+    return constVal;
+  } else {
+    return rewriter.create<ONNXSqueezeOp>(loc, tensorType,
+            create.onnx.toTensor(input), create.onnx.constantInt64({axis}))
+            .getResult();
+  }
+}
+
+/// Emit an ONNXUnsqueezeOp. If the input is constant, do const
+/// propagation, and return a constant.
+Value foldOrEmitONNXUnsqueezeOpMhlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr unsqueezedElements = inputElements.reshape(tensorType);
+    Value constVal = create.onnx.constant(unsqueezedElements);
+    return constVal;
+  } else {
+    return rewriter.create<ONNXUnsqueezeOp>(loc, tensorType,
+            create.onnx.toTensor(input), create.onnx.constantInt64({axis}))
+            .getResult();
+  }
+}
+
+/// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
+/// return constants.
+/// Only support evenly splitting.
+std::vector<Value> foldOrEmitONNXSplitOpMhlo(ConversionPatternRewriter &rewriter,
+    Location loc, ArrayRef<Type> resultTypes, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  std::vector<Value> resVals;
+  int outputNum = resultTypes.size();
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    auto inputShape = inputElements.getType().getShape();
+    assert(outputNum == 0 || inputShape[axis] % outputNum == 0);
+    int64_t sizeOfEachSplit = outputNum != 0 ? inputShape[axis] / outputNum : 0;
+    SmallVector<int64_t, 4> sizes(outputNum, sizeOfEachSplit);
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    std::vector<ElementsAttr> splits =
+        elementsBuilder.split(inputElements, axis, sizes);
+    for (ElementsAttr splitElements : splits) {
+      // Avoid DisposableElementsAttr during conversion.
+      DenseElementsAttr denseSplitElements =
+          elementsBuilder.toDenseElementsAttr(splitElements);
+      Value constVal = create.onnx.constant(denseSplitElements);
+      resVals.emplace_back(constVal);
+    }
+  } else {
+    SmallVector<Type, 4> convertedTypes;
+    SmallVector<int64_t> splitSizesI64;
+    for (auto t : resultTypes) {
+      convertedTypes.emplace_back(create.onnx.toTensor(t));
+      splitSizesI64.emplace_back(t.cast<ShapedType>().getShape()[axis]);
+    }
+    Value splitSizes = create.onnx.constantInt64(splitSizesI64);
+    ONNXSplitOp split = rewriter.create<ONNXSplitOp>(loc, convertedTypes,
+        create.onnx.toTensor(input), splitSizes,
+        /*axis=*/axis, nullptr);
+    for (int i = 0; i < outputNum; ++i)
+      resVals.emplace_back(split.getOutputs()[i]);
+  }
+  return resVals;
+}
+
+/// Emit an ONNXTransposeOp. If the input is constant, do const propagation,
+/// and return a constant.
+Value foldOrEmitONNXTransposeOpMhlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, ArrayAttr permAttr) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    SmallVector<uint64_t, 4> perm;
+    for (auto permVal : permAttr.getValue())
+      perm.emplace_back(permVal.cast<IntegerAttr>().getInt());
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    ElementsAttr transposedElements =
+        elementsBuilder.transpose(inputElements, perm);
+    // Avoid DisposableElementsAttr during conversion.
+    DenseElementsAttr denseTransposedElements =
+        elementsBuilder.toDenseElementsAttr(transposedElements);
+    Value constVal = create.onnx.constant(denseTransposedElements);
+    return constVal;
+  } else {
+    return rewriter.create<ONNXTransposeOp>(loc, create.onnx.toTensor(resultType),
+                create.onnx.toTensor(input), permAttr).getResult();
+  }
+}
+
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
index ec5a9f2b..9b6fd411 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
@@ -19,6 +19,7 @@
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Math/IR/Math.h"
 #include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/PatternMatch.h"
 #include "mlir/Pass/Pass.h"
 #include "mlir/Transforms/DialectConversion.h"
@@ -28,6 +29,8 @@

 #include "mhlo/IR/hlo_ops.h"

+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Dialect/Mlir/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/IndexExpr.hpp"
 #include "src/Dialect/ONNX/DialectBuilder.hpp"
 #include "src/Dialect/ONNX/ONNXOps.hpp"
@@ -113,6 +116,40 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     llvm::SmallVector<Value, 4> &operands, Type outputType,
     ConversionPatternRewriter &rewriter, Location loc, int64_t outputRank);

+mlir::ElementsAttr getElementAttributeFromConstValue(mlir::Value value);
+
+DenseIntElementsAttr GetI64ElementsAttr(
+    ArrayRef<int64_t> values, Builder *builder);
+
+//===----------------------------------------------------------------------===//
+// Fold and emit support.
+//===----------------------------------------------------------------------===//
+
+/// Emit an ONNXSqueezeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXSqueezeOpMhlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXUnsqueezeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXUnsqueezeOpMhlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
+/// return constants.
+/// Only support evenly splitting.
+std::vector<mlir::Value> foldOrEmitONNXSplitOpMhlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    llvm::ArrayRef<mlir::Type> resultTypes, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXTransposeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXTransposeOpMhlo(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Type resultType, mlir::Value input,
+    mlir::ArrayAttr permAttr);
+
 // `Math` directory methods:
 void populateLoweringONNXClipOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -133,6 +170,9 @@ void populateLoweringONNXNormalizationOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXPoolingOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+// `RNN` directory methods:
+void populateLoweringONNXLSTMOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 // `Tensor` directory methods:
 void populateLoweringONNXArgMaxOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -148,10 +188,17 @@ void populateLoweringONNXFlattenOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXGatherOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXGatherElementsOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXIdentityOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXOneHotOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXPadOpToMhloPattern(RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXReshapeOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXScatterNDOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXShapeOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXSliceOpToMhloPattern(
diff --git a/src/Conversion/ONNXToMhlo/RNN/LSTM.cpp b/src/Conversion/ONNXToMhlo/RNN/LSTM.cpp
new file mode 100644
index 00000000..9bc9f05a
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/RNN/LSTM.cpp
@@ -0,0 +1,625 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- LSTM.cpp - Lowering LSTM Op --------------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX LSTM Operators to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp"
+#include "src/Dialect/Mlir/DialectBuilder.hpp"
+
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "lstm"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace mhlo {
+
+struct LstmState {
+  // returned states.
+  SmallVector<Value, 512> forwardAllH;
+  SmallVector<Value, 512> reverseAllH;
+  Value ht;
+  Value ct;
+  // intermediate states.
+  Value forwardHt;
+  Value reverseHt;
+  Value forwardCt;
+  Value reverseCt;
+};
+
+struct LstmActivationPack {
+  RNNActivation f;
+  RNNActivation g;
+  RNNActivation h;
+};
+
+struct LstmWeightPack {
+  Value WT;
+  Value RT;
+};
+
+struct LstmBiasPack {
+  bool hasBias = false;
+  Value Wbi;
+  Value Wbo;
+  Value Wbf;
+  Value Wbc;
+  Value Rbi;
+  Value Rbo;
+  Value Rbf;
+  Value Rbc;
+  // Put peephole here.
+  bool hasPeephole = false;
+  Value Pi;
+  Value Po;
+  Value Pf;
+};
+
+template <>
+bool hasAllNoneOutput<ONNXLSTMOp>(ONNXLSTMOp *op) {
+  return (isNoneValue(op->getY()) && isNoneValue(op->getYH()) &&
+          isNoneValue(op->getYC()));
+}
+
+template <>
+std::tuple<LstmActivationPack, LstmActivationPack>
+getActivationPack<ONNXLSTMOp, LstmActivationPack>(ONNXLSTMOp *op) {
+  auto direction = op->getDirection();
+  auto activations = op->getActivations();
+  auto activationAlpha = op->getActivationAlpha();
+  auto activationBeta = op->getActivationBeta();
+
+  LstmActivationPack activationForward, activationReverse;
+
+  // Get activation function name.
+  // Default forward functions
+  activationForward.f.name = "sigmoid";
+  activationForward.g.name = "tanh";
+  activationForward.h.name = "tanh";
+  // Default backward functions
+  activationReverse.f.name = "sigmoid";
+  activationReverse.g.name = "tanh";
+  activationReverse.h.name = "tanh";
+  if (activations) {
+    ArrayAttr activationArrAttr = activations.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.name =
+            activationArrAttr[0].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.name =
+            activationArrAttr[1].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.name =
+            activationArrAttr[2].cast<StringAttr>().getValue();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.name =
+            activationArrAttr[startIndex].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.name =
+            activationArrAttr[startIndex + 1].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.name =
+            activationArrAttr[startIndex + 2].cast<StringAttr>().getValue();
+      }
+    }
+  }
+
+  // Get alpha attributes.
+  if (activationAlpha) {
+    ArrayAttr activationArrAttr = activationAlpha.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.alpha = activationArrAttr[0].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.alpha = activationArrAttr[1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.alpha = activationArrAttr[2].cast<FloatAttr>();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.alpha =
+            activationArrAttr[startIndex].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.alpha =
+            activationArrAttr[startIndex + 1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.alpha =
+            activationArrAttr[startIndex + 2].cast<FloatAttr>();
+      }
+    }
+  }
+
+  // Get beta attributes.
+  if (activationBeta) {
+    ArrayAttr activationArrAttr = activationBeta.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.beta = activationArrAttr[0].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.beta = activationArrAttr[1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.beta = activationArrAttr[2].cast<FloatAttr>();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.beta =
+            activationArrAttr[startIndex].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.beta =
+            activationArrAttr[startIndex + 1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.beta =
+            activationArrAttr[startIndex + 2].cast<FloatAttr>();
+      }
+    }
+  }
+
+  return std::make_tuple(activationForward, activationReverse);
+}
+
+template <>
+std::tuple<LstmWeightPack, LstmWeightPack>
+getWeightPack<ONNXLSTMOp, LstmWeightPack>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op) {
+  // Return values.
+  LstmWeightPack weightForward, weightReverse;
+
+  // parameter weight: [direction, 4*hiddenSize, inputSize]
+  Value W = op->getW();
+  // recurrence weight: [direction, 4*hiddenSize, hiddenSize]
+  Value R = op->getR();
+  // direction
+  StringRef direction = op->getDirection();
+
+  ArrayRef<int64_t> wShape = W.getType().cast<ShapedType>().getShape();
+  Type elementType = W.getType().cast<ShapedType>().getElementType();
+  int64_t hiddenSize = wShape[1] / 4;
+  int64_t inputSize = wShape[2];
+
+  // RankedTensorType types for parameter weights.
+  auto w3DTy =
+      RankedTensorType::get({1, 4 * hiddenSize, inputSize}, elementType);
+  auto w2DTy = RankedTensorType::get({4 * hiddenSize, inputSize}, elementType);
+  auto wTranspose2DTy =
+      RankedTensorType::get({inputSize, 4 * hiddenSize}, elementType);
+  SmallVector<Type, 4> w3D2Ty(2, w3DTy);
+
+  // RankedTensorType types for recurrence weights.
+  auto r3DTy =
+      RankedTensorType::get({1, 4 * hiddenSize, hiddenSize}, elementType);
+  auto r2DTy = RankedTensorType::get({4 * hiddenSize, hiddenSize}, elementType);
+  auto rTranspose2DTy =
+      RankedTensorType::get({hiddenSize, 4 * hiddenSize}, elementType);
+  SmallVector<Type, 4> r3D2Ty(2, r3DTy);
+
+  // Squeeze the direction axis from W and R.
+  Value fW, bW, fR, bR;
+  if (direction == FORWARD) {
+    fW = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, w2DTy, W, /*axis=*/0);
+    fR = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, r2DTy, R, /*axis=*/0);
+  } else if (direction == REVERSE) {
+    bW = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, w2DTy, W, /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, r2DTy, R, /*axis=*/0);
+  } else { // BIDIRECTIONAL
+    // W
+    std::vector<Value> vals =
+        foldOrEmitONNXSplitOpMhlo(rewriter, loc, w3D2Ty, W, 0);
+    fW = foldOrEmitONNXSqueezeOpMhlo(
+        rewriter, loc, w2DTy, vals[0], /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeOpMhlo(
+        rewriter, loc, w2DTy, vals[1], /*axis=*/0);
+    // R
+    vals.clear();
+    vals = foldOrEmitONNXSplitOpMhlo(rewriter, loc, r3D2Ty, R, 0);
+    fR = foldOrEmitONNXSqueezeOpMhlo(
+        rewriter, loc, r2DTy, vals[0], /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeOpMhlo(
+        rewriter, loc, r2DTy, vals[1], /*axis=*/0);
+  }
+
+  // Transpose W and R.
+  ArrayAttr permAttr = rewriter.getI64ArrayAttr({1, 0});
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    // W
+    weightForward.WT = foldOrEmitONNXTransposeOpMhlo(
+        rewriter, loc, wTranspose2DTy, fW, permAttr);
+    // R
+    weightForward.RT = foldOrEmitONNXTransposeOpMhlo(
+        rewriter, loc, rTranspose2DTy, fR, permAttr);
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    // W
+    weightReverse.WT = foldOrEmitONNXTransposeOpMhlo(
+        rewriter, loc, wTranspose2DTy, bW, permAttr);
+    // R
+    weightReverse.RT = foldOrEmitONNXTransposeOpMhlo(
+        rewriter, loc, rTranspose2DTy, bR, permAttr);
+  }
+  return std::make_tuple(weightForward, weightReverse);
+}
+
+template <>
+std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op) {
+  // Return values.
+  LstmBiasPack biasForward, biasReverse;
+
+  // bias: [direction, 8*hiddenSize] for both parameter and recurrence weights.
+  Value B = op->getB();
+  // peephold: [direction, 3*hiddenSize] for input, output and forget gates.
+  Value P = op->getP();
+
+  // direction
+  StringRef direction = op->getDirection();
+
+  // Split B.
+  if (!isNoneValue(B)) {
+    ArrayRef<int64_t> bShape = B.getType().cast<ShapedType>().getShape();
+    Type elementType = B.getType().cast<ShapedType>().getElementType();
+    int64_t hiddenSize = bShape[1] / 8;
+
+    // MemRef types.
+    auto bType2D = RankedTensorType::get({1, 8 * hiddenSize}, elementType);
+    auto bType1D = RankedTensorType::get({8 * hiddenSize}, elementType);
+    auto bSplitType1D = RankedTensorType::get({hiddenSize}, elementType);
+    SmallVector<Type, 4> split1D8Ty(8, bSplitType1D);
+    SmallVector<Type, 4> split2D2Ty(2, bType2D);
+
+    // Squeeze the direction axis from B.
+    Value fB, bB;
+    if (direction == FORWARD) {
+      fB =
+          foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, bType1D, B, /*axis=*/0);
+    } else if (direction == REVERSE) {
+      bB =
+          foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, bType1D, B, /*axis=*/0);
+    } else { // BIDIRECTIONAL
+      std::vector<Value> vals;
+      vals = foldOrEmitONNXSplitOpMhlo(rewriter, loc, split2D2Ty, B, 0);
+      fB = foldOrEmitONNXSqueezeOpMhlo(
+          rewriter, loc, bType1D, vals[0], /*axis=*/0);
+      bB = foldOrEmitONNXSqueezeOpMhlo(
+          rewriter, loc, bType1D, vals[1], /*axis=*/0);
+    }
+
+    // Split B into individual bias tensors.
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split1D8Ty, fB, 0);
+      biasForward.Wbi = vals[0];
+      biasForward.Wbo = vals[1];
+      biasForward.Wbf = vals[2];
+      biasForward.Wbc = vals[3];
+      biasForward.Rbi = vals[4];
+      biasForward.Rbo = vals[5];
+      biasForward.Rbf = vals[6];
+      biasForward.Rbc = vals[7];
+      biasForward.hasBias = true;
+    }
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split1D8Ty, bB, 0);
+      biasReverse.Wbi = vals[0];
+      biasReverse.Wbo = vals[1];
+      biasReverse.Wbf = vals[2];
+      biasReverse.Wbc = vals[3];
+      biasReverse.Rbi = vals[4];
+      biasReverse.Rbo = vals[5];
+      biasReverse.Rbf = vals[6];
+      biasReverse.Rbc = vals[7];
+      biasReverse.hasBias = true;
+    }
+  }
+
+  // Split P.
+  if (!isNoneValue(P)) {
+    ArrayRef<int64_t> pShape = P.getType().cast<ShapedType>().getShape();
+    Type elementType = P.getType().cast<ShapedType>().getElementType();
+    int64_t hiddenSize = pShape[1] / 3;
+
+    // MemRef types.
+    auto pType2D = RankedTensorType::get({1, 3 * hiddenSize}, elementType);
+    auto pType1D = RankedTensorType::get({3 * hiddenSize}, elementType);
+    auto pSplitType1D = RankedTensorType::get({hiddenSize}, elementType);
+    SmallVector<Type, 4> split1D3Ty(3, pSplitType1D);
+    SmallVector<Type, 4> split2D2Ty(2, pType2D);
+
+    // Squeeze the direction axis from P.
+    Value fP, bP;
+    if (direction == FORWARD) {
+      fP =
+          foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, pType1D, P, /*axis=*/0);
+    } else if (direction == REVERSE) {
+      bP =
+          foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, pType1D, P, /*axis=*/0);
+    } else { // BIDIRECTIONAL
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split2D2Ty, P, 0);
+      fP = foldOrEmitONNXSqueezeOpMhlo(
+          rewriter, loc, pType1D, vals[0], /*axis=*/0);
+      bP = foldOrEmitONNXSqueezeOpMhlo(
+          rewriter, loc, pType1D, vals[1], /*axis=*/0);
+    }
+
+    // Split P into individual tensors.
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split1D3Ty, fP, 0);
+      biasForward.Pi = vals[0];
+      biasForward.Po = vals[1];
+      biasForward.Pf = vals[2];
+      biasForward.hasPeephole = true;
+    }
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split1D3Ty, bP, 0);
+      biasReverse.Pi = vals[0];
+      biasReverse.Po = vals[1];
+      biasReverse.Pf = vals[2];
+      biasReverse.hasPeephole = true;
+    }
+  }
+
+  return std::make_tuple(biasForward, biasReverse);
+}
+
+template <>
+LstmState allocAndInitializeStates<ONNXLSTMOp, LstmState>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op,
+    typename ONNXLSTMOp::Adaptor operandAdaptor) {
+  LstmState state;
+
+  // direction
+  StringRef direction = op->getDirection();
+
+  // Insert allocation and deallocation for the results of this operation.
+  // If the result is not returned, then no allocation happens.
+  // Y :: [seq_length, num_directions, batch_size, hidden_size]
+  // Y_h :: [num_directions, batch_size, hidden_size]
+  state.ht = allocHiddenOrCell(rewriter, loc, operandAdaptor.getX(),
+      operandAdaptor.getW(), operandAdaptor.getR());
+  // Y_c :: [num_directions, batch_size, hidden_size]
+  state.ct = allocHiddenOrCell(rewriter, loc, operandAdaptor.getX(),
+      operandAdaptor.getW(), operandAdaptor.getR());
+
+  // Insert allocation and deallocation the intermediate Ht and Ct for the
+  // forward and reverse directions.
+  // Ht :: [batch_size, hidden_size]
+  // Ct :: [batch_size, hidden_size]
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    state.forwardHt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+    state.forwardCt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    state.reverseHt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+    state.reverseCt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+  }
+
+  // Initialize Ht and Ct.
+  initializeIntermediateStates(rewriter, loc, state.forwardHt, state.reverseHt,
+      state.forwardCt, state.reverseCt, operandAdaptor.getInitialH(),
+      operandAdaptor.getInitialC(),
+      operandAdaptor.getX().getType().cast<RankedTensorType>().getElementType(),
+      direction, /*onlyHidden=*/false);
+  return state;
+}
+
+template <>
+void calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+    LstmBiasPack>(ConversionPatternRewriter &rewriter, Location loc, Value Xt,
+    LstmState &state, LstmActivationPack activationPack,
+    LstmWeightPack weightPack, LstmBiasPack biasPack, Value sequenceIV,
+    Value directionIV, Value sequenceLens, Value initialH, bool isForward) {
+  // Equations for LSTM.
+  // it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)
+  // ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)
+  // ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)
+  // Ct = ft (.) Ct-1 + it (.) ct
+  // ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)
+  // Ht = ot (.) h(Ct)
+
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+
+  ArrayRef<int64_t> xtShape = Xt.getType().cast<ShapedType>().getShape();
+  int64_t batchSize = xtShape[0];
+
+  // Get Ht, Ct.
+  Value Ht = (isForward) ? state.forwardHt : state.reverseHt;
+  Value Ct = (isForward) ? state.forwardCt : state.reverseCt;
+
+  ArrayRef<int64_t> htShape = Ht.getType().cast<ShapedType>().getShape();
+  int64_t hiddenSize = htShape[1];
+
+  // Frequently used types.
+  RankedTensorType matrixType = Ht.getType().cast<RankedTensorType>();
+  Type elementType = matrixType.getElementType();
+  RankedTensorType matrixAllGatesType =
+      RankedTensorType::get({batchSize, 4 * hiddenSize}, elementType);
+
+  // Do matrix multiplications.
+  // Xt * (Wi^T ++ Wo^T ++ Wf^T ++ Wc^T)
+  // Ht * (Ri^T ++ Ro^T ++ Rf^T ++ Rc^T)
+  // where '++' is matrix concatenation.
+  // XtWT: [B, 4H], HtRT: [B, 4H]
+  Value XtWT = create.onnx.matmul(matrixAllGatesType, Xt, weightPack.WT);
+  Value HtRT = create.onnx.matmul(matrixAllGatesType, Ht, weightPack.RT);
+  Value commonSum = create.onnx.add(XtWT, HtRT);
+  RankedTensorType matrixSingleGateType =
+      RankedTensorType::get({batchSize, hiddenSize}, elementType);
+  Value zeroIndex = create.onnx.constantInt64({0});
+  Value oneIndex = create.onnx.constantInt64({1});
+  Value oneHiddenIndex = create.onnx.constantInt64({hiddenSize});
+  Value twoHiddenIndex = create.onnx.constantInt64({2 * hiddenSize});
+  Value threeHiddenIndex = create.onnx.constantInt64({3 * hiddenSize});
+  Value fourHiddenIndex = create.onnx.constantInt64({4 * hiddenSize});
+  Value it = create.onnx.slice(matrixSingleGateType, commonSum, zeroIndex,
+      oneHiddenIndex, oneIndex, oneIndex);
+  Value ot = create.onnx.slice(matrixSingleGateType, commonSum, oneHiddenIndex,
+      twoHiddenIndex, oneIndex, oneIndex);
+  Value ft = create.onnx.slice(matrixSingleGateType, commonSum, twoHiddenIndex,
+      threeHiddenIndex, oneIndex, oneIndex);
+  Value ct = create.onnx.slice(matrixSingleGateType, commonSum,
+      threeHiddenIndex, fourHiddenIndex, oneIndex, oneIndex);
+  if (biasPack.hasBias) {
+    it = create.onnx.add(it, biasPack.Wbi);
+    it = create.onnx.add(it, biasPack.Rbi);
+  }
+  if (biasPack.hasPeephole) {
+    Value PiCt = create.onnx.mul(biasPack.Pi, Ct);
+    it = create.onnx.add(it, PiCt);
+  }
+  it = applyActivation(rewriter, loc, activationPack.f, it);
+  if (biasPack.hasBias) {
+    ft = create.onnx.add(ft, biasPack.Wbf);
+    ft = create.onnx.add(ft, biasPack.Rbf);
+  }
+  if (biasPack.hasPeephole) {
+    Value PfCt = create.onnx.mul(biasPack.Pf, Ct);
+    ft = create.onnx.add(ft, PfCt);
+  }
+  ft = applyActivation(rewriter, loc, activationPack.f, ft);
+  if (biasPack.hasBias) {
+    ct = create.onnx.add(ct, biasPack.Wbc);
+    ct = create.onnx.add(ct, biasPack.Rbc);
+  }
+  ct = applyActivation(rewriter, loc, activationPack.g, ct);
+
+  Value ftCt = create.onnx.mul(ft, Ct);
+  Value itct = create.onnx.mul(it, ct);
+  Value nextCt = create.onnx.add(ftCt, itct);
+
+  if (biasPack.hasBias) {
+    ot = create.onnx.add(ot, biasPack.Wbo);
+    ot = create.onnx.add(ot, biasPack.Rbo);
+  }
+  if (biasPack.hasPeephole) {
+    Value PoCt = create.onnx.mul(biasPack.Po, nextCt);
+    ot = create.onnx.add(ot, PoCt);
+  }
+  ot = applyActivation(rewriter, loc, activationPack.f, ot);
+  // Ht = ot (.) h(Ct)
+  Value nextHt = applyActivation(rewriter, loc, activationPack.h, nextCt);
+  nextHt = create.onnx.mul(ot, nextHt);
+  if (isForward) {
+    state.forwardHt = nextHt;
+    state.forwardCt = nextCt;
+  } else {
+    state.reverseHt = nextHt;
+    state.reverseCt = nextCt;
+  }
+  RankedTensorType unsqueezedHtType =
+      RankedTensorType::get({1, 1, batchSize, hiddenSize}, elementType);
+  if (isForward)
+    state.forwardAllH.emplace_back(create.onnx.unsqueeze(
+        unsqueezedHtType, nextHt, create.onnx.constantInt64({0, 1})));
+  else
+    state.reverseAllH.insert(state.reverseAllH.begin(),
+        create.onnx.unsqueeze(unsqueezedHtType, nextHt,
+        create.onnx.constantInt64({0, 1})));
+}
+
+template <>
+void stateToOutput<ONNXLSTMOp, LstmState>(ConversionPatternRewriter &rewriter,
+    Location loc, ONNXLSTMOp *op, LstmState state,
+    std::vector<Value> &outputs) {
+  Value noneValue;
+  auto direction = op->getDirection();
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  // First output: all sequences.
+  if (isNoneValue(op->getY()))
+    outputs.emplace_back(noneValue);
+  else {
+    if (direction == FORWARD) {
+      outputs.emplace_back(create.onnx.concat(
+          op->getY().getType(), ValueRange(state.forwardAllH), 0));
+    } else if (direction == REVERSE) {
+      outputs.emplace_back(create.onnx.concat(
+          op->getY().getType(), ValueRange(state.reverseAllH), 0));
+    } else {
+      auto outputShape = op->getY().getType().cast<ShapedType>().getShape();
+      RankedTensorType singleDirectionType = RankedTensorType::get(
+          {outputShape[0], 1, outputShape[2], outputShape[3]},
+          op->getY().getType().cast<ShapedType>().getElementType());
+      outputs.emplace_back(create.onnx.concat(op->getY().getType(),
+          {create.onnx.concat(
+               singleDirectionType, ValueRange(state.forwardAllH), 0),
+              create.onnx.concat(
+                  singleDirectionType, ValueRange(state.reverseAllH), 0)},
+          1));
+    }
+  }
+  // Second output: hidden.
+  if (isNoneValue(op->getYH()))
+    outputs.emplace_back(noneValue);
+  else {
+    stateToOutputForHiddenOrCell(
+        rewriter, loc, state.forwardHt, state.reverseHt, direction, state.ht);
+    outputs.emplace_back(state.ht);
+  }
+  // Third output: cell.
+  if (isNoneValue(op->getYC()))
+    outputs.emplace_back(noneValue);
+  else {
+    stateToOutputForHiddenOrCell(
+        rewriter, loc, state.forwardCt, state.reverseCt, direction, state.ct);
+    outputs.emplace_back(state.ct);
+  }
+}
+
+} // namespace mhlo
+
+void populateLoweringONNXLSTMOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<onnx_mlir::mhlo::ONNXRNNOpLowering<ONNXLSTMOp,
+      onnx_mlir::mhlo::LstmState, onnx_mlir::mhlo::LstmActivationPack,
+      onnx_mlir::mhlo::LstmWeightPack, onnx_mlir::mhlo::LstmBiasPack>>(
+      ctx, UNROLL);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/RNN/RNNBase.cpp b/src/Conversion/ONNXToMhlo/RNN/RNNBase.cpp
new file mode 100644
index 00000000..002d2673
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/RNN/RNNBase.cpp
@@ -0,0 +1,221 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- RNNBase.cpp - Lowering RNN Ops -----------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file defines base functions for lowering the ONNX RNN Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "lstm"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace mhlo {
+
+// Get a dimension of the tensor's shape.
+int64_t dimAt(Value val, int index) {
+  return val.getType().cast<ShapedType>().getShape()[index];
+}
+
+/// Insert Allocate and Deallocate for the hidden or cell output.
+mlir::Value allocHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value W, mlir::Value R) {
+  LLVM_DEBUG(llvm::dbgs() << "allocHiddenOrCell\n");
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+  RankedTensorType zeroType = RankedTensorType::get(
+      {/*num_directions=*/dimAt(W, 0), /*batch_size=*/dimAt(X, 1),
+          /*hidden_size=*/dimAt(R, 2)},
+      X.getType().cast<ShapedType>().getElementType());
+  DenseElementsAttr zeroAttr = DenseElementsAttr::get(zeroType, 0.0f);
+  return create.onnx.constant(zeroAttr);
+}
+
+/// Insert Allocate and Deallocate for the intermediate hidden or cell states.
+/// Shape :: [batch_size, hidden_size]
+Value allocIntermediateState(
+    ConversionPatternRewriter &rewriter, Location loc, Value X, Value R) {
+  LLVM_DEBUG(llvm::dbgs() << "allocIntermediateState\n");
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+  RankedTensorType zeroType =
+      RankedTensorType::get({/*batch_size=*/dimAt(X, 1),
+                                /*hidden_size=*/dimAt(R, 2)},
+          X.getType().cast<ShapedType>().getElementType());
+  DenseElementsAttr zeroAttr = DenseElementsAttr::get(zeroType, 0.0f);
+  return create.onnx.constant(zeroAttr);
+}
+
+/// Initialize the intermediate hidden and cell states.
+/// forward(reverse)Ht, forward(reverse)Ct
+void initializeIntermediateStates(ConversionPatternRewriter &rewriter,
+    Location loc, Value &forwardHt, Value &reverseHt, Value &forwardCt,
+    Value &reverseCt, Value initialH, Value initialC, Type elementType,
+    StringRef direction, bool onlyHidden) {
+  LLVM_DEBUG(llvm::dbgs() << "initializeIntermediateStates\n");
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+
+  Value zeroIndex = create.onnx.constantInt64({0});
+  Value oneIndex = create.onnx.constantInt64({1});
+  Value twoIndex = create.onnx.constantInt64({2});
+
+  Value boundVal = (direction == FORWARD || direction == BIDIRECTIONAL)
+                       ? forwardHt
+                       : reverseHt;
+  auto valShape = boundVal.getType().cast<ShapedType>().getShape();
+  RankedTensorType sliceType =
+      RankedTensorType::get({1, valShape[0], valShape[1]},
+          boundVal.getType().cast<RankedTensorType>().getElementType());
+  RankedTensorType valType = boundVal.getType().cast<RankedTensorType>();
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    if (!isNoneValue(initialH)) {
+      forwardHt = create.onnx.slice(
+          sliceType, initialH, zeroIndex, oneIndex, zeroIndex, oneIndex);
+      forwardHt = create.onnx.squeeze(valType, forwardHt, zeroIndex);
+    }
+    if (!onlyHidden && !isNoneValue(initialC)) {
+      forwardCt = create.onnx.slice(
+          sliceType, initialC, zeroIndex, oneIndex, zeroIndex, oneIndex);
+      forwardCt = create.onnx.squeeze(valType, forwardCt, zeroIndex);
+    }
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    if (!isNoneValue(initialH)) {
+      if (direction == REVERSE) {
+        reverseHt = create.onnx.slice(
+            sliceType, initialH, zeroIndex, oneIndex, zeroIndex, oneIndex);
+        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex);
+      } else {
+        reverseHt = create.onnx.slice(
+            sliceType, initialH, oneIndex, twoIndex, zeroIndex, oneIndex);
+        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex);
+      }
+    }
+    if (!onlyHidden and !isNoneValue(initialC)) {
+      if (direction == REVERSE) {
+        reverseCt = create.onnx.slice(
+            sliceType, initialC, zeroIndex, oneIndex, zeroIndex, oneIndex);
+        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex);
+      } else {
+        reverseCt = create.onnx.slice(
+            sliceType, initialC, oneIndex, twoIndex, zeroIndex, oneIndex);
+        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex);
+      }
+    }
+  }
+}
+
+/// Store a state into the output of the RNN op.
+/// The input state is 2D and the output state is 3D with '1' or '2' is
+/// pretended, depending on 'direction'.
+void stateToOutputForHiddenOrCell(ConversionPatternRewriter &rewriter,
+    Location loc, Value forwardVal, Value reverseVal, StringRef direction,
+    Value &output) {
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+  if (direction == FORWARD || direction == REVERSE) {
+    Value val = (direction == FORWARD) ? forwardVal : reverseVal;
+    output = val;
+  } else { // BIDIRECTIONAL
+    SmallVector<int64_t, 4> bForwardValShape(
+        forwardVal.getType().cast<ShapedType>().getShape());
+    SmallVector<int64_t, 4> bValShape(
+        forwardVal.getType().cast<ShapedType>().getShape());
+    SmallVector<int64_t, 4> bReverseValShape(
+        reverseVal.getType().cast<ShapedType>().getShape());
+    bForwardValShape.insert(bForwardValShape.begin(), 1);
+    bReverseValShape.insert(bReverseValShape.begin(), 1);
+    bValShape.insert(bValShape.begin(), 2);
+    Type valElementType =
+        forwardVal.getType().cast<ShapedType>().getElementType();
+    Value zero = create.onnx.constantInt64({0});
+    Value bForwardVal = create.onnx.unsqueeze(
+        RankedTensorType::get(bForwardValShape, valElementType), forwardVal,
+        zero);
+    Value bReverseVal = create.onnx.unsqueeze(
+        RankedTensorType::get(bReverseValShape, valElementType), reverseVal,
+        zero);
+    output =
+        create.onnx.concat(RankedTensorType::get(bValShape, valElementType),
+            {bForwardVal, bReverseVal}, 0);
+  }
+}
+
+// Apply an activation function on a given scalar operand.
+Value applyActivation(OpBuilder &rewriter, Location loc,
+    RNNActivation activation, Value operand) {
+  Value res;
+
+  std::vector<mlir::NamedAttribute> attributes;
+  if (activation.alpha) {
+    attributes.emplace_back(
+        rewriter.getNamedAttr("alpha", activation.alpha.value()));
+  }
+  if (activation.beta) {
+    attributes.emplace_back(
+        rewriter.getNamedAttr("beta", activation.beta.value()));
+  }
+  Type resType = operand.getType();
+
+  // Change equality to be case insensitive.
+  if (activation.name.equals_insensitive("relu"))
+    res = rewriter.create<ONNXReluOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("tanh"))
+    res = rewriter.create<ONNXTanhOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("sigmoid"))
+    res = rewriter.create<ONNXSigmoidOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("affine"))
+    llvm_unreachable("Unsupported activation");
+  else if (activation.name.equals_insensitive("leakyrelu"))
+    res = rewriter.create<ONNXLeakyReluOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("thresholdedrelu"))
+    res = rewriter.create<ONNXThresholdedReluOp>(
+        loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("scaledtanh"))
+    llvm_unreachable("Unsupported activation");
+  else if (activation.name.equals_insensitive("hardsigmoid"))
+    res = rewriter.create<ONNXHardSigmoidOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("elu"))
+    res = rewriter.create<ONNXEluOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("softsign"))
+    res = rewriter.create<ONNXSoftsignOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("softplus"))
+    res = rewriter.create<ONNXSoftplusOp>(loc, resType, operand);
+  else
+    llvm_unreachable("Unsupported activation");
+
+  return res;
+}
+
+/// Create a copy of a slice of X at a specific timestep.
+Value emitXSliceAt(ConversionPatternRewriter &rewriter, Location loc, Value X,
+    Value timestepIV) {
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+  int64_t batchSize = dimAt(X, 1);
+  int64_t inputSize = dimAt(X, 2);
+  Type elementType = X.getType().cast<ShapedType>().getElementType();
+  RankedTensorType sliceXType =
+      RankedTensorType::get({1, batchSize, inputSize}, elementType);
+  RankedTensorType squeezedXType =
+      RankedTensorType::get({batchSize, inputSize}, elementType);
+  Value sliceX = create.onnx.slice(sliceXType, X, timestepIV,
+      create.onnx.add(timestepIV, create.onnx.constantInt64({1})),
+      create.onnx.constantInt64({0}), create.onnx.constantInt64({1}));
+  sliceX = create.onnx.squeeze(
+      squeezedXType, sliceX, create.onnx.constantInt64({0}));
+  return sliceX;
+}
+
+} // namespace mhlo
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp b/src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp
new file mode 100644
index 00000000..8da9aedd
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp
@@ -0,0 +1,211 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- RNNBase.hpp - Lowering RNN Ops -----------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file defines base functions for lowering the ONNX RNN Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+
+static constexpr llvm::StringRef FORWARD = "forward";
+static constexpr llvm::StringRef REVERSE = "reverse";
+static constexpr llvm::StringRef BIDIRECTIONAL = "bidirectional";
+
+static constexpr llvm::StringRef USELOOP = "useloop";
+static constexpr llvm::StringRef UNROLL = "unroll";
+
+namespace onnx_mlir {
+
+namespace mhlo {
+
+struct RNNActivation {
+  llvm::StringRef name;
+  std::optional<mlir::FloatAttr> alpha;
+  std::optional<mlir::FloatAttr> beta;
+};
+
+/// Get a dimension of the tensor's shape.
+int64_t dimAt(mlir::Value val, int index);
+
+/// Insert Allocate and Deallocate for the hidden or cell output.
+mlir::Value allocHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value W, mlir::Value R);
+
+/// Allocate the intermediate hidden or cell state.
+mlir::Value allocIntermediateState(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value R);
+
+/// Initialize the intermediate hidden and cell states.
+void initializeIntermediateStates(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value &forwardHt, mlir::Value &reverseHt,
+    mlir::Value &forwardCt, mlir::Value &reverseCt, mlir::Value initialH,
+    mlir::Value initialC, mlir::Type elementType, llvm::StringRef direction,
+    bool onlyHidden);
+
+/// Store a state into the output of the RNN op.
+/// The input state is 2D and the output state is 3D with '1' or '2' is
+/// pretended, depending on 'direction'.
+void stateToOutputForHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value forwardVal, mlir::Value reverseVal,
+    llvm::StringRef direction, mlir::Value &output);
+
+/// Apply an activation function on a given operand.
+mlir::Value applyActivation(mlir::OpBuilder &rewriter, mlir::Location loc,
+    RNNActivation activation, mlir::Value operand);
+
+/// Get a slice of X at a specific timestep.
+mlir::Value emitXSliceAt(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value timestep);
+
+// Override the following methods when lowering an RNN operation:
+// - hasAllNoneOutput
+// - getActivationPack
+// - getWeightPack
+// - getBiasPack
+// - allocAndInitializeStates
+// - calculateState
+// - stateToOutput
+
+// Check whether all outputs have NoneType or not.
+template <typename RNNOp>
+bool hasAllNoneOutput(RNNOp *op);
+
+// Obtain activations functions for a specific operation.
+template <typename RNNOp, typename A>
+std::tuple<A, A> getActivationPack(RNNOp *op);
+
+/// Obtain weight tensors in 2D for each gate.
+/// In ONNX, weights for gates and directions are combined in a single tensor.
+/// This function splits them into 2D tensors.
+template <typename RNNOp, typename W>
+std::tuple<W, W> getWeightPack(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc, RNNOp *op);
+
+/// Obtain biases in 1D for each gate.
+/// In ONNX, biases for gates and directions are combined in a single tensor.
+/// This function splits them into 1D tensors.
+template <typename RNNOp, typename B>
+std::tuple<B, B> getBiasPack(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc, RNNOp *op);
+
+// Allocate memory for RNN states and initialize them.
+template <typename RNNOp, typename S>
+S allocAndInitializeStates(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, RNNOp *op, typename RNNOp::Adaptor operandAdaptor);
+
+// Calculate new states from the current input and states.
+template <typename S, typename A, typename W, typename B>
+void calculateState(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value Xt, S &state, A activationSet, W weight,
+    B bias, mlir::Value sequenceIV, mlir::Value directionIV,
+    mlir::Value sequenceLens, mlir::Value initialH, bool isForward);
+
+// Write states to the RNN's outputs.
+template <typename RNNOp, typename S>
+void stateToOutput(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, RNNOp *op, S state, std::vector<mlir::Value> &outputs);
+
+// A common template for lowering an RNN operation.
+template <typename RNNOp, typename S, typename A, typename W, typename B>
+struct ONNXRNNOpLowering : public mlir::OpConversionPattern<RNNOp> {
+  using OpAdaptor = typename RNNOp::Adaptor;
+  StringRef loopExpansion;
+
+  ONNXRNNOpLowering(mlir::MLIRContext *ctx, const StringRef &expansion)
+      : mlir::OpConversionPattern<RNNOp>(ctx) {
+    loopExpansion = expansion;
+  }
+
+  mlir::LogicalResult matchAndRewrite(RNNOp rnnOp, OpAdaptor adaptor,
+      mlir::ConversionPatternRewriter &rewriter) const final {
+    mlir::Operation *op = rnnOp.getOperation();
+    mlir::Location loc = ONNXLoc<RNNOp>(op);
+    mlir::Value X = adaptor.getX();
+    mlir::Value sequenceLens = adaptor.getSequenceLens();
+    mlir::Value initialH = adaptor.getInitialH();
+
+    if (hasAllNoneOutput<RNNOp>(&rnnOp)) {
+      rewriter.eraseOp(op);
+      return mlir::success();
+    }
+
+    // Initialize output states.
+    S state =
+        allocAndInitializeStates<RNNOp, S>(rewriter, loc, &rnnOp, adaptor);
+
+    // Activation functions.
+    A activationForward, activationReverse;
+    std::tie(activationForward, activationReverse) =
+        getActivationPack<RNNOp, A>(&rnnOp);
+
+    // Prepare weights.
+    W weightForward, weightReverse;
+    std::tie(weightForward, weightReverse) =
+        getWeightPack<RNNOp, W>(rewriter, loc, &rnnOp);
+
+    // Prepare biases.
+    B biasForward, biasReverse;
+    std::tie(biasForward, biasReverse) =
+        getBiasPack<RNNOp, B>(rewriter, loc, &rnnOp);
+
+    int64_t sequenceDimSize = dimAt(rnnOp.getX(), 0);
+    auto direction = rnnOp.getDirection();
+
+    MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+
+    if (loopExpansion != UNROLL) {
+      rnnOp.emitError("only unroll is supported for now");
+      return failure();
+    }
+    assert(!mlir::ShapedType::isDynamic(sequenceDimSize) &&
+           "Only static sequenceDimSize is supported for unroll");
+
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      for (int64_t i = 0; i < sequenceDimSize; i++) {
+        mlir::Value directionIV = create.onnx.constantInt64({0});
+        mlir::Value sequenceIV = create.onnx.constantInt64({i});
+        // Get a slice of X at the current timestep.
+        mlir::Value Xt = emitXSliceAt(rewriter, loc, X, sequenceIV);
+        // Emit calculation for one RNN step.
+        calculateState<S, A, W, B>(rewriter, loc, Xt, state, activationForward,
+            weightForward, biasForward, sequenceIV, directionIV, sequenceLens,
+            initialH,
+            /*isForward=*/true);
+      }
+    }
+
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      for (int64_t i = 0; i < sequenceDimSize; i++) {
+        mlir::Value directionIV =
+            create.onnx.constantInt64({(direction == REVERSE) ? 0 : 1});
+        mlir::Value reverseSequenceIV =
+            create.onnx.constantInt64({sequenceDimSize - i - 1});
+        // Get a slice of X at the current timestep.
+        mlir::Value Xt = emitXSliceAt(rewriter, loc, X, reverseSequenceIV);
+        // Emit calculation for one RNN step.
+        calculateState<S, A, W, B>(rewriter, loc, Xt, state, activationReverse,
+            weightReverse, biasReverse, reverseSequenceIV, directionIV,
+            sequenceLens, initialH,
+            /*isForward=*/false);
+      }
+    }
+
+    std::vector<mlir::Value> outputs;
+    stateToOutput<RNNOp, S>(rewriter, loc, &rnnOp, state, outputs);
+    rewriter.replaceOp(op, outputs);
+    return mlir::success();
+  }
+};
+
+} // namespace mhlo
+
+} // namespace onnx_mlir
diff --git a/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM.mlir b/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM.mlir
new file mode 100644
index 00000000..14a38607
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM.mlir
@@ -0,0 +1,185 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-mhlo --canonicalize -split-input-file %s | FileCheck %s
+func.func @test_lstm(%arg0 : tensor<2x16x512xf32>, %arg1 : tensor<2x2048xf32>, %arg2 : tensor<2x1024x512xf32>, %arg3 : tensor<2x1024x256xf32>) -> tensor<2x2x16x256xf32> {
+  %0 = onnx.Constant dense<0.000000e+00> : tensor<2x16x256xf32>
+  %1 = "onnx.NoValue"() {value} : () -> none
+  %Y, %Y_h, %Y_c = "onnx.LSTM"(%arg0, %arg2, %arg3, %arg1, %1, %0, %0, %1) {direction = "bidirectional", hidden_size = 256 : si64, input_forget = 0 : si64, layout = 0 : si64} : (tensor<2x16x512xf32>, tensor<2x1024x512xf32>, tensor<2x1024x256xf32>, tensor<2x2048xf32>, none, tensor<2x16x256xf32>, tensor<2x16x256xf32>, none) -> (tensor<2x2x16x256xf32>, tensor<2x16x256xf32>, tensor<2x16x256xf32>)
+  return %Y : tensor<2x2x16x256xf32>
+// CHECK:  func.func @test_lstm(%arg0: tensor<2x16x512xf32>, %arg1: tensor<2x2048xf32>, %arg2: tensor<2x1024x512xf32>, %arg3: tensor<2x1024x256xf32>) -> tensor<2x2x16x256xf32> {
+// CHECK:  %0 = mhlo.constant dense<0.000000e+00> : tensor<16x256xf32>
+// CHECK:  %1 = "mhlo.slice"(%arg2) {limit_indices = dense<[1, 1024, 512]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK:  %2 = "mhlo.slice"(%arg2) {limit_indices = dense<[2, 1024, 512]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK:  %3 = mhlo.reshape %1 : (tensor<1x1024x512xf32>) -> tensor<1024x512xf32>
+// CHECK:  %4 = mhlo.reshape %2 : (tensor<1x1024x512xf32>) -> tensor<1024x512xf32>
+// CHECK:  %5 = "mhlo.slice"(%arg3) {limit_indices = dense<[1, 1024, 256]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK:  %6 = "mhlo.slice"(%arg3) {limit_indices = dense<[2, 1024, 256]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK:  %7 = mhlo.reshape %5 : (tensor<1x1024x256xf32>) -> tensor<1024x256xf32>
+// CHECK:  %8 = mhlo.reshape %6 : (tensor<1x1024x256xf32>) -> tensor<1024x256xf32>
+// CHECK:  %9 = "mhlo.transpose"(%3) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK:  %10 = "mhlo.transpose"(%7) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK:  %11 = "mhlo.transpose"(%4) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK:  %12 = "mhlo.transpose"(%8) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK:  %13 = "mhlo.slice"(%arg1) {limit_indices = dense<[1, 2048]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK:  %14 = "mhlo.slice"(%arg1) {limit_indices = dense<[2, 2048]> : tensor<2xi64>, start_indices = dense<[1, 0]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK:  %15 = mhlo.reshape %13 : (tensor<1x2048xf32>) -> tensor<2048xf32>
+// CHECK:  %16 = mhlo.reshape %14 : (tensor<1x2048xf32>) -> tensor<2048xf32>
+// CHECK:  %17 = "mhlo.slice"(%15) {limit_indices = dense<256> : tensor<1xi64>, start_indices = dense<0> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %18 = "mhlo.slice"(%15) {limit_indices = dense<512> : tensor<1xi64>, start_indices = dense<256> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %19 = "mhlo.slice"(%15) {limit_indices = dense<768> : tensor<1xi64>, start_indices = dense<512> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %20 = "mhlo.slice"(%15) {limit_indices = dense<1024> : tensor<1xi64>, start_indices = dense<768> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %21 = "mhlo.slice"(%15) {limit_indices = dense<1280> : tensor<1xi64>, start_indices = dense<1024> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %22 = "mhlo.slice"(%15) {limit_indices = dense<1536> : tensor<1xi64>, start_indices = dense<1280> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %23 = "mhlo.slice"(%15) {limit_indices = dense<1792> : tensor<1xi64>, start_indices = dense<1536> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %24 = "mhlo.slice"(%15) {limit_indices = dense<2048> : tensor<1xi64>, start_indices = dense<1792> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %25 = "mhlo.slice"(%16) {limit_indices = dense<256> : tensor<1xi64>, start_indices = dense<0> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %26 = "mhlo.slice"(%16) {limit_indices = dense<512> : tensor<1xi64>, start_indices = dense<256> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %27 = "mhlo.slice"(%16) {limit_indices = dense<768> : tensor<1xi64>, start_indices = dense<512> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %28 = "mhlo.slice"(%16) {limit_indices = dense<1024> : tensor<1xi64>, start_indices = dense<768> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %29 = "mhlo.slice"(%16) {limit_indices = dense<1280> : tensor<1xi64>, start_indices = dense<1024> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %30 = "mhlo.slice"(%16) {limit_indices = dense<1536> : tensor<1xi64>, start_indices = dense<1280> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %31 = "mhlo.slice"(%16) {limit_indices = dense<1792> : tensor<1xi64>, start_indices = dense<1536> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %32 = "mhlo.slice"(%16) {limit_indices = dense<2048> : tensor<1xi64>, start_indices = dense<1792> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK:  %33 = "mhlo.slice"(%arg0) {limit_indices = dense<[1, 16, 512]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x16x512xf32>) -> tensor<1x16x512xf32>
+// CHECK:  %34 = mhlo.reshape %33 : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK:  %35 = "mhlo.dot"(%34, %9) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:  %36 = "mhlo.dot"(%0, %10) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:  %37 = mhlo.add %35, %36 : tensor<16x1024xf32>
+// CHECK:  %38 = "mhlo.slice"(%37) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %39 = "mhlo.slice"(%37) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %40 = "mhlo.slice"(%37) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %41 = "mhlo.slice"(%37) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %42 = "mhlo.broadcast_in_dim"(%17) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %43 = mhlo.add %38, %42 : tensor<16x256xf32>
+// CHECK:  %44 = "mhlo.broadcast_in_dim"(%21) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %45 = mhlo.add %43, %44 : tensor<16x256xf32>
+// CHECK:  %46 = mhlo.logistic %45 : tensor<16x256xf32>
+// CHECK:  %47 = "mhlo.broadcast_in_dim"(%19) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %48 = mhlo.add %40, %47 : tensor<16x256xf32>
+// CHECK:  %49 = "mhlo.broadcast_in_dim"(%23) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %50 = mhlo.add %48, %49 : tensor<16x256xf32>
+// CHECK:  %51 = mhlo.logistic %50 : tensor<16x256xf32>
+// CHECK:  %52 = "mhlo.broadcast_in_dim"(%20) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %53 = mhlo.add %41, %52 : tensor<16x256xf32>
+// CHECK:  %54 = "mhlo.broadcast_in_dim"(%24) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %55 = mhlo.add %53, %54 : tensor<16x256xf32>
+// CHECK:  %56 = mhlo.tanh %55 : tensor<16x256xf32>
+// CHECK:  %57 = mhlo.multiply %51, %0 : tensor<16x256xf32>
+// CHECK:  %58 = mhlo.multiply %46, %56 : tensor<16x256xf32>
+// CHECK:  %59 = mhlo.add %57, %58 : tensor<16x256xf32>
+// CHECK:  %60 = "mhlo.broadcast_in_dim"(%18) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %61 = mhlo.add %39, %60 : tensor<16x256xf32>
+// CHECK:  %62 = "mhlo.broadcast_in_dim"(%22) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %63 = mhlo.add %61, %62 : tensor<16x256xf32>
+// CHECK:  %64 = mhlo.logistic %63 : tensor<16x256xf32>
+// CHECK:  %65 = mhlo.tanh %59 : tensor<16x256xf32>
+// CHECK:  %66 = mhlo.multiply %64, %65 : tensor<16x256xf32>
+// CHECK:  %67 = mhlo.reshape %66 : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK:  %68 = "mhlo.slice"(%arg0) {limit_indices = dense<[2, 16, 512]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x16x512xf32>) -> tensor<1x16x512xf32>
+// CHECK:  %69 = mhlo.reshape %68 : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK:  %70 = "mhlo.dot"(%69, %9) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:  %71 = "mhlo.dot"(%66, %10) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:  %72 = mhlo.add %70, %71 : tensor<16x1024xf32>
+// CHECK:  %73 = "mhlo.slice"(%72) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %74 = "mhlo.slice"(%72) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %75 = "mhlo.slice"(%72) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %76 = "mhlo.slice"(%72) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %77 = "mhlo.broadcast_in_dim"(%17) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %78 = mhlo.add %73, %77 : tensor<16x256xf32>
+// CHECK:  %79 = "mhlo.broadcast_in_dim"(%21) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %80 = mhlo.add %78, %79 : tensor<16x256xf32>
+// CHECK:  %81 = mhlo.logistic %80 : tensor<16x256xf32>
+// CHECK:  %82 = "mhlo.broadcast_in_dim"(%19) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %83 = mhlo.add %75, %82 : tensor<16x256xf32>
+// CHECK:  %84 = "mhlo.broadcast_in_dim"(%23) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %85 = mhlo.add %83, %84 : tensor<16x256xf32>
+// CHECK:  %86 = mhlo.logistic %85 : tensor<16x256xf32>
+// CHECK:  %87 = "mhlo.broadcast_in_dim"(%20) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %88 = mhlo.add %76, %87 : tensor<16x256xf32>
+// CHECK:  %89 = "mhlo.broadcast_in_dim"(%24) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %90 = mhlo.add %88, %89 : tensor<16x256xf32>
+// CHECK:  %91 = mhlo.tanh %90 : tensor<16x256xf32>
+// CHECK:  %92 = mhlo.multiply %86, %59 : tensor<16x256xf32>
+// CHECK:  %93 = mhlo.multiply %81, %91 : tensor<16x256xf32>
+// CHECK:  %94 = mhlo.add %92, %93 : tensor<16x256xf32>
+// CHECK:  %95 = "mhlo.broadcast_in_dim"(%18) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %96 = mhlo.add %74, %95 : tensor<16x256xf32>
+// CHECK:  %97 = "mhlo.broadcast_in_dim"(%22) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %98 = mhlo.add %96, %97 : tensor<16x256xf32>
+// CHECK:  %99 = mhlo.logistic %98 : tensor<16x256xf32>
+// CHECK:  %100 = mhlo.tanh %94 : tensor<16x256xf32>
+// CHECK:  %101 = mhlo.multiply %99, %100 : tensor<16x256xf32>
+// CHECK:  %102 = mhlo.reshape %101 : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK:  %103 = "mhlo.slice"(%arg0) {limit_indices = dense<[2, 16, 512]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x16x512xf32>) -> tensor<1x16x512xf32>
+// CHECK:  %104 = mhlo.reshape %103 : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK:  %105 = "mhlo.dot"(%104, %11) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:  %106 = "mhlo.dot"(%0, %12) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:  %107 = mhlo.add %105, %106 : tensor<16x1024xf32>
+// CHECK:  %108 = "mhlo.slice"(%107) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %109 = "mhlo.slice"(%107) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %110 = "mhlo.slice"(%107) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %111 = "mhlo.slice"(%107) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %112 = "mhlo.broadcast_in_dim"(%25) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %113 = mhlo.add %108, %112 : tensor<16x256xf32>
+// CHECK:  %114 = "mhlo.broadcast_in_dim"(%29) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %115 = mhlo.add %113, %114 : tensor<16x256xf32>
+// CHECK:  %116 = mhlo.logistic %115 : tensor<16x256xf32>
+// CHECK:  %117 = "mhlo.broadcast_in_dim"(%27) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %118 = mhlo.add %110, %117 : tensor<16x256xf32>
+// CHECK:  %119 = "mhlo.broadcast_in_dim"(%31) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %120 = mhlo.add %118, %119 : tensor<16x256xf32>
+// CHECK:  %121 = mhlo.logistic %120 : tensor<16x256xf32>
+// CHECK:  %122 = "mhlo.broadcast_in_dim"(%28) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %123 = mhlo.add %111, %122 : tensor<16x256xf32>
+// CHECK:  %124 = "mhlo.broadcast_in_dim"(%32) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %125 = mhlo.add %123, %124 : tensor<16x256xf32>
+// CHECK:  %126 = mhlo.tanh %125 : tensor<16x256xf32>
+// CHECK:  %127 = mhlo.multiply %121, %0 : tensor<16x256xf32>
+// CHECK:  %128 = mhlo.multiply %116, %126 : tensor<16x256xf32>
+// CHECK:  %129 = mhlo.add %127, %128 : tensor<16x256xf32>
+// CHECK:  %130 = "mhlo.broadcast_in_dim"(%26) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %131 = mhlo.add %109, %130 : tensor<16x256xf32>
+// CHECK:  %132 = "mhlo.broadcast_in_dim"(%30) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %133 = mhlo.add %131, %132 : tensor<16x256xf32>
+// CHECK:  %134 = mhlo.logistic %133 : tensor<16x256xf32>
+// CHECK:  %135 = mhlo.tanh %129 : tensor<16x256xf32>
+// CHECK:  %136 = mhlo.multiply %134, %135 : tensor<16x256xf32>
+// CHECK:  %137 = mhlo.reshape %136 : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK:  %138 = "mhlo.slice"(%arg0) {limit_indices = dense<[1, 16, 512]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x16x512xf32>) -> tensor<1x16x512xf32>
+// CHECK:  %139 = mhlo.reshape %138 : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK:  %140 = "mhlo.dot"(%139, %11) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:  %141 = "mhlo.dot"(%136, %12) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:  %142 = mhlo.add %140, %141 : tensor<16x1024xf32>
+// CHECK:  %143 = "mhlo.slice"(%142) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %144 = "mhlo.slice"(%142) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %145 = "mhlo.slice"(%142) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %146 = "mhlo.slice"(%142) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK:  %147 = "mhlo.broadcast_in_dim"(%25) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %148 = mhlo.add %143, %147 : tensor<16x256xf32>
+// CHECK:  %149 = "mhlo.broadcast_in_dim"(%29) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %150 = mhlo.add %148, %149 : tensor<16x256xf32>
+// CHECK:  %151 = mhlo.logistic %150 : tensor<16x256xf32>
+// CHECK:  %152 = "mhlo.broadcast_in_dim"(%27) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %153 = mhlo.add %145, %152 : tensor<16x256xf32>
+// CHECK:  %154 = "mhlo.broadcast_in_dim"(%31) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %155 = mhlo.add %153, %154 : tensor<16x256xf32>
+// CHECK:  %156 = mhlo.logistic %155 : tensor<16x256xf32>
+// CHECK:  %157 = "mhlo.broadcast_in_dim"(%28) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %158 = mhlo.add %146, %157 : tensor<16x256xf32>
+// CHECK:  %159 = "mhlo.broadcast_in_dim"(%32) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %160 = mhlo.add %158, %159 : tensor<16x256xf32>
+// CHECK:  %161 = mhlo.tanh %160 : tensor<16x256xf32>
+// CHECK:  %162 = mhlo.multiply %156, %129 : tensor<16x256xf32>
+// CHECK:  %163 = mhlo.multiply %151, %161 : tensor<16x256xf32>
+// CHECK:  %164 = mhlo.add %162, %163 : tensor<16x256xf32>
+// CHECK:  %165 = "mhlo.broadcast_in_dim"(%26) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %166 = mhlo.add %144, %165 : tensor<16x256xf32>
+// CHECK:  %167 = "mhlo.broadcast_in_dim"(%30) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:  %168 = mhlo.add %166, %167 : tensor<16x256xf32>
+// CHECK:  %169 = mhlo.logistic %168 : tensor<16x256xf32>
+// CHECK:  %170 = mhlo.tanh %164 : tensor<16x256xf32>
+// CHECK:  %171 = mhlo.multiply %169, %170 : tensor<16x256xf32>
+// CHECK:  %172 = mhlo.reshape %171 : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK:  %173 = "mhlo.concatenate"(%67, %102) {dimension = 0 : i64} : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
+// CHECK:  %174 = "mhlo.concatenate"(%172, %137) {dimension = 0 : i64} : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
+// CHECK:  %175 = "mhlo.concatenate"(%173, %174) {dimension = 1 : i64} : (tensor<2x1x16x256xf32>, tensor<2x1x16x256xf32>) -> tensor<2x2x16x256xf32>
+// CHECK:  return %175 : tensor<2x2x16x256xf32>
+}
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
index 0089fee1..8d5dec5b 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
@@ -69,11 +69,8 @@ struct ONNXExpandOpLoweringToMhlo : public ConversionPattern {
       RankedTensorType onesType = RankedTensorType::get(onesShape, elementType);
       broadcastedOnes = rewriter.create<mhlo::DynamicBroadcastInDimOp>(
           loc, onesType, ones, shape, rewriter.getI64TensorAttr({}));
-    } else if (ONNXConstantOp shapeOp =
-                   dyn_cast_or_null<ONNXConstantOp>(shapeDefOp)) {
+    } else if (mlir::ElementsAttr constShape = getElementAttributeFromConstValue(shape)) {
       llvm::SmallVector<int64_t, 4> shapeValues;
-      mlir::ElementsAttr constShape =
-          shapeOp.getValueAttr().cast<ElementsAttr>();
       for (mlir::IntegerAttr element : constShape.getValues<IntegerAttr>())
         shapeValues.push_back(element.getInt());
       RankedTensorType broadcastedType =
@@ -84,7 +81,7 @@ struct ONNXExpandOpLoweringToMhlo : public ConversionPattern {
       assert(
           false &&
           "Shape argument of Expand is the output of an unexpected operation. "
-          "Supported operations are: onnx.Constant and onnx.Shape");
+          "Supported operations are: Constant and onnx.Shape");
     }
     llvm::SmallVector<Value, 4> newOperands = {input, broadcastedOnes};
     llvm::SmallVector<Value, 4> broadcastedOperands = getBroadcastedOperands(
diff --git a/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp b/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp
new file mode 100644
index 00000000..09d7a6c2
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp
@@ -0,0 +1,139 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===-------- GatherElements.cpp - Lowering GatherElements Op -------------===//
+//
+// Copyright 2020-2022 The IBM Research Authors.
+//
+// =============================================================================
+//
+// This file lowers the ONNX GatherElements Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXGatherElementsOpLoweringToMhlo : public ConversionPattern {
+  ONNXGatherElementsOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(
+            mlir::ONNXGatherElementsOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    ONNXGatherElementsOpAdaptor operandAdaptor(operands);
+    ONNXGatherElementsOp gatherOp = cast<ONNXGatherElementsOp>(op);
+    Location loc = op->getLoc();
+
+    IndexExprBuilderForMhlo createIE(rewriter, loc);
+    ONNXGatherElementsOpShapeHelper shapeHelper(op, operands, &createIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+
+    // Operands and attributes.
+    Value data = operandAdaptor.getData();
+    Value indices = operandAdaptor.getIndices();
+    int64_t axisLit = gatherOp.getAxis();
+
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    int64_t rank = inputType.getRank(); // indices has the same rank
+    ShapedType indicesType = indices.getType().cast<ShapedType>();
+    Type indexElemType = indicesType.getElementType();
+    // Negative value means counting dimensions from the back.
+    axisLit = axisLit < 0 ? axisLit + rank : axisLit;
+
+    // make sure all index values >= 0
+    Value zero = getShapedZero(loc, rewriter, indices);
+    Value inputShape = rewriter.create<shape::ShapeOfOp>(loc, data);
+    Value indicesShape = rewriter.create<shape::ShapeOfOp>(loc, indices);
+    Value axisDimSize =
+        rewriter.create<shape::GetExtentOp>(loc, inputShape, axisLit);
+    axisDimSize =
+        rewriter.create<arith::IndexCastOp>(loc, indexElemType, axisDimSize);
+    axisDimSize = rewriter.create<tensor::FromElementsOp>(loc, axisDimSize);
+    axisDimSize = rewriter.create<mhlo::ReshapeOp>(loc,
+        RankedTensorType::get(SmallVector<int64_t>{}, indexElemType),
+        axisDimSize);
+    Value broadcastedAxisDimSize =
+        rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc, indicesType,
+            axisDimSize, indicesShape, rewriter.getI64TensorAttr({}));
+    Value isNegative = rewriter.create<mhlo::CompareOp>(
+        loc, indices, zero, mhlo::ComparisonDirection::LT);
+    Value positiveIndices = rewriter.create<mhlo::AddOp>(
+        loc, indicesType, indices, broadcastedAxisDimSize);
+    indices = rewriter.create<mhlo::SelectOp>(
+        loc, indicesType, isNegative, positiveIndices, indices);
+
+    // start indices
+    Value toConcatIndexShape;
+    SmallVector<Value> toConcatIndexShapeValueVec;
+    for (size_t i = 0; i < rank; i++) {
+      toConcatIndexShapeValueVec.push_back(
+          rewriter.create<shape::GetExtentOp>(loc, indicesShape, i));
+    }
+    toConcatIndexShapeValueVec.push_back(
+        rewriter.create<arith::ConstantIndexOp>(loc, 1));
+    toConcatIndexShape = rewriter.create<tensor::FromElementsOp>(
+        loc, toConcatIndexShapeValueVec);
+
+    ArrayRef<int64_t> indicesShapeVec = indicesType.getShape();
+    SmallVector<int64_t> toConcatIndexShapeVec(
+        indicesShapeVec.begin(), indicesShapeVec.end());
+    toConcatIndexShapeVec.push_back(1);
+    RankedTensorType toConcatIndexType =
+        RankedTensorType::get(toConcatIndexShapeVec, indexElemType);
+
+    SmallVector<Value> toConcat;
+    for (int64_t i = 0; i < inputType.getRank(); ++i) {
+      if (i == axisLit) {
+        toConcat.push_back(rewriter.create<mhlo::DynamicReshapeOp>(
+            loc, toConcatIndexType, indices, toConcatIndexShape));
+      } else {
+        toConcat.push_back(
+            rewriter.create<mhlo::DynamicIotaOp>(loc, toConcatIndexType,
+                toConcatIndexShape, rewriter.getI64IntegerAttr(i)));
+      }
+    }
+    auto gatherIndicies = rewriter.create<mhlo::ConcatenateOp>(
+        loc, toConcat, static_cast<uint64_t>(inputType.getRank()));
+
+    // dimsAttr
+    SmallVector<int64_t> collapsedDims;
+    SmallVector<int64_t> startIndexMap;
+    for (int64_t i = 0; i < rank; i++) {
+      collapsedDims.push_back(i);
+      startIndexMap.push_back(i);
+    }
+    auto dimsAttr = mhlo::GatherDimensionNumbersAttr::get(rewriter.getContext(),
+        /*offsetDims=*/{},
+        /*collapsedSliceDims=*/collapsedDims,
+        /*startIndexMap=*/startIndexMap,
+        /*indexVecDim=*/rank);
+    SmallVector<int64_t> sliceSizes(inputType.getRank(), 1);
+
+    Value gatherValue = rewriter.create<mhlo::GatherOp>(loc, outputType, data,
+        gatherIndicies, dimsAttr, rewriter.getI64TensorAttr(sliceSizes));
+    rewriter.replaceOp(op, gatherValue);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXGatherElementsOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXGatherElementsOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp b/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp
new file mode 100644
index 00000000..f4537916
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp
@@ -0,0 +1,127 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===---------------- OneHot.cpp - Lowering OneHot Op -------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX OneHot Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+#include <numeric>
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+struct ONNXOneHotOpLoweringToMhlo : public OpConversionPattern<ONNXOneHotOp> {
+  ONNXOneHotOpLoweringToMhlo(MLIRContext *ctx) : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXOneHotOp onehotOp,
+      ONNXOneHotOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = onehotOp.getOperation();
+    Location loc = ONNXLoc<ONNXOneHotOp>(op);
+    ValueRange operands = adaptor.getOperands();
+    Value indices = adaptor.getIndices();
+    Value depthValue = adaptor.getDepth();
+    Value values = adaptor.getValues();
+    Type outputType = *op->result_type_begin();
+
+    IndexExprBuilderForMhlo createIE(rewriter, loc);
+    ONNXOneHotOpShapeHelper shapeHelper(op, operands, &createIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+    int64_t axis = shapeHelper.axis;
+
+    RankedTensorType indicesType =
+        indices.getType().dyn_cast<RankedTensorType>();
+    if (!indicesType || !indicesType.hasStaticShape())
+      return failure();
+    ArrayRef<int64_t> indicesShape = indicesType.getShape();
+    Type indicesElementType = indicesType.getElementType();
+
+    DenseIntElementsAttr depthAttr;
+    if (!matchPattern(depthValue, m_Constant(&depthAttr))) {
+      return failure();
+    }
+
+    int64_t depth = depthAttr.getValues<APInt>()[0].getSExtValue();
+
+    llvm::SmallVector<int64_t, 4> broadcastDims(indicesShape.size());
+    std::iota(broadcastDims.begin(), broadcastDims.begin() + axis, 0);
+    std::iota(broadcastDims.begin() + axis, broadcastDims.end(), axis + 1);
+
+    llvm::SmallVector<int64_t, 4> outputDims = llvm::to_vector<4>(indicesShape);
+    outputDims.insert(outputDims.begin() + axis, depth);
+
+    RankedTensorType indexType =
+        RankedTensorType::get(llvm::ArrayRef(outputDims), indicesElementType);
+
+    Value iota = rewriter.create<mhlo::IotaOp>(
+        loc, indexType, IntegerAttr::get(rewriter.getIntegerType(64), axis));
+    Value broadcastIndices = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, indices, GetI64ElementsAttr(broadcastDims, &rewriter));
+    Value zero = rewriter.create<mhlo::ConstantOp>(loc,
+        DenseIntElementsAttr::get(RankedTensorType::get({}, indicesElementType),
+            ArrayRef<int64_t>{0}));
+    Value broadcastZero = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, zero, rewriter.getI64TensorAttr({}));
+    Value broadcastDepth;
+    int64_t depthRank = depthValue.getType().cast<RankedTensorType>().getRank();
+    if (depthRank == 1)
+      broadcastDepth = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, depthValue, rewriter.getI64TensorAttr({0}));
+    else
+      broadcastDepth = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, depthValue, rewriter.getI64TensorAttr({}));
+    Value compareGeZero = rewriter.create<mhlo::CompareOp>(
+        loc, broadcastIndices, broadcastZero, mhlo::ComparisonDirection::GE);
+    Value positiveIndices =
+        rewriter.create<mhlo::AddOp>(loc, broadcastIndices, broadcastDepth);
+    Value normalizedIndices = rewriter.create<mhlo::SelectOp>(
+        loc, indexType, compareGeZero, broadcastIndices, positiveIndices);
+    Value compare = rewriter.create<mhlo::CompareOp>(
+        loc, normalizedIndices, iota, mhlo::ComparisonDirection::EQ);
+    Type indexElementType = rewriter.getI64Type();
+    Type valueType = values.getType().cast<ShapedType>().getElementType();
+    Value offValue = rewriter.create<mhlo::SliceOp>(loc,
+        RankedTensorType::get({1}, valueType), values,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{0}),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
+        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
+            ArrayRef<int64_t>{1}));
+    Value onValue = rewriter.create<mhlo::SliceOp>(loc,
+        RankedTensorType::get({1}, valueType), values,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{2}),
+        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
+            ArrayRef<int64_t>{1}));
+    Value offValueBroadcast = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, outputType, offValue, rewriter.getI64TensorAttr({0}));
+    Value onValueBroadcast = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, outputType, onValue, rewriter.getI64TensorAttr({0}));
+    Value result = rewriter.create<mhlo::SelectOp>(
+        loc, outputType, compare, onValueBroadcast, offValueBroadcast);
+    rewriter.replaceOp(op, {result});
+    return success();
+  }
+};
+
+void populateLoweringONNXOneHotOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXOneHotOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
new file mode 100644
index 00000000..40006578
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
@@ -0,0 +1,103 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===----------- Pad.cpp - Lowering Pad Op ------------===//
+//
+// Copyright 2022
+//
+// =============================================================================
+//
+// This file lowers ONNX Pad Operators to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ElementsAttr/DisposableElementsAttr.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXPadOpLoweringToMhlo : public ConversionPattern {
+  ONNXPadOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(mlir::ONNXPadOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+
+    Location loc = op->getLoc();
+    ONNXPadOpAdaptor operandAdaptor(operands, op->getAttrDictionary());
+    Value data = operandAdaptor.getData();
+    Value constantValue = operandAdaptor.getConstantValue();
+    Value pads = operandAdaptor.getPads();
+    Value axes = operandAdaptor.getAxes();
+    StringRef padMode = operandAdaptor.getMode();
+
+    if (!padMode.equals_insensitive("constant"))
+        return failure();
+    // only support axes is None
+    if (!isa<ONNXNoneOp>(axes.getDefiningOp()))
+        return failure();
+    assert(isRankedShapedType(data.getType()) && "Expected Ranked ShapedType");
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    Type elemType = inputType.getElementType();
+    int64_t rank = inputType.getRank();
+
+    Type outputType = *op->result_type_begin();
+    if (!constantValue || isNoneValue(constantValue)) {
+      // Pad with zeros by default
+      constantValue = rewriter.create<mhlo::ConstantOp>(
+          loc, DenseElementsAttr::get(mlir::RankedTensorType::get({}, elemType),
+              rewriter.getZeroAttr(elemType)));
+    } else {
+      // constantValue might be 1D tensor, reshape it to scalar
+      constantValue = rewriter.create<mhlo::ReshapeOp>(
+        loc, RankedTensorType::get({}, elemType), constantValue);
+    }
+    SmallVector<int64_t> edgePaddingLowVec(rank, 0);
+    SmallVector<int64_t> edgePaddingHighVec(rank, 0);
+    SmallVector<int64_t> interiorPaddingVec(rank, 0);
+    if (auto valueAttribute = getElementAttributeFromConstValue(pads)) {
+      // If `pads` are constants, read them."
+      int64_t idx = 0;
+      for (IntegerAttr value : valueAttribute.getValues<IntegerAttr>()) {
+        int64_t padValue = value.getInt();
+        if (padValue < 0)
+          return failure();
+        if (idx < rank)
+          edgePaddingLowVec[idx] = padValue;
+        else
+          edgePaddingHighVec[idx - rank] = padValue;
+        idx++;
+      }
+    } else {
+      assert(false && "Pads must be known at compile time");
+    }
+
+    mlir::DenseIntElementsAttr edgePaddingLow =
+        rewriter.getI64VectorAttr(edgePaddingLowVec);
+    mlir::DenseIntElementsAttr edgePaddingHigh =
+        rewriter.getI64VectorAttr(edgePaddingHighVec);
+    mlir::DenseIntElementsAttr interiorPadding =
+        rewriter.getI64VectorAttr(interiorPaddingVec);
+    Value padResult = rewriter.create<mhlo::PadOp>(loc, outputType, data,
+        constantValue, edgePaddingLow, edgePaddingHigh, interiorPadding);
+
+    rewriter.replaceOp(op, padResult);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXPadOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXPadOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp b/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp
new file mode 100644
index 00000000..acb95fe9
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp
@@ -0,0 +1,93 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- ScatterND.cpp - Lowering ScatterND Op ----------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX ScatterND Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+struct ONNXScatterNDOpLoweringToMhlo
+    : public OpConversionPattern<ONNXScatterNDOp> {
+  ONNXScatterNDOpLoweringToMhlo(MLIRContext *ctx) : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXScatterNDOp scatterNDOp,
+      ONNXScatterNDOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = scatterNDOp.getOperation();
+    Location loc = ONNXLoc<ONNXScatterNDOp>(op);
+
+    // Operands and attributes.
+    Value data = adaptor.getData();
+    Value updates = adaptor.getUpdates();
+    Value indices = adaptor.getIndices();
+    auto dataType = data.getType().cast<ShapedType>();
+    auto indicesType = indices.getType().cast<ShapedType>();
+    auto updatesType = updates.getType().cast<ShapedType>();
+    int64_t dataRank = dataType.getRank();
+    int64_t updatesRank = updatesType.getRank();
+    int64_t indicesRank = indicesType.getRank();
+    assert(indicesType.hasStaticShape() &&
+           "only support indices with static shape");
+    int64_t partialIdxDim = indicesType.getDimSize(indicesRank - 1);
+
+    assert(dataRank >= 1 && "The rank of 'data' must be >= 1");
+    assert(indicesRank >= 1 && "The rank of 'indices' must be >= 1");
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+    ShapedType outputShapedType = outputType.cast<ShapedType>();
+    int64_t outputRank = outputShapedType.getRank();
+    assert(outputRank == dataRank && "Output rank not equal to data rank");
+    auto scatter_dimension_numbers =
+        mlir::mhlo::ScatterDimensionNumbersAttr::get(
+            /*context=*/rewriter.getContext(),
+            /*updateWindowDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(partialIdxDim, dataRank)),
+            /*insertedWindowDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(0, partialIdxDim)),
+            /*scatterDimsToOperandDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(0, partialIdxDim)),
+            /*indexVectorDim=*/indicesRank - 1);
+    auto scatterOp = rewriter.create<mhlo::ScatterOp>(
+        loc, outputType, data, indices, updates, scatter_dimension_numbers);
+    // config update computation function: just return the element from src.
+    Block &block = scatterOp.getUpdateComputation().emplaceBlock();
+    // add block arguments
+    auto blockArgumentType =
+        RankedTensorType::get({}, dataType.getElementType());
+    block.addArgument(blockArgumentType, loc);
+    block.addArgument(blockArgumentType, loc);
+
+    auto *lhsArg = block.args_begin();
+    auto *rhsArg = std::next(lhsArg);
+
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&block);
+      rewriter.create<mhlo::ReturnOp>(loc, *rhsArg);
+    }
+
+    rewriter.replaceOp(op, scatterOp.getResults());
+    return success();
+  }
+};
+
+void populateLoweringONNXScatterNDOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXScatterNDOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
index beb50392..b821ec34 100644
--- a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
+++ b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
@@ -544,6 +544,16 @@ struct ONNXPadOpShapeHelper : public ONNXOpShapeHelper {
   llvm::SmallVector<IndexExpr, 4> pads;
 };

+struct ONNXPadV13OpShapeHelper : public ONNXOpShapeHelper {
+  ONNXPadV13OpShapeHelper(mlir::Operation *op, mlir::ValueRange operands,
+      IndexExprBuilder *ieBuilder = nullptr, IndexExprScope *scope = nullptr)
+      : ONNXOpShapeHelper(op, operands, ieBuilder, scope), pads() {}
+  virtual ~ONNXPadV13OpShapeHelper() {}
+  mlir::LogicalResult computeShape() final;
+  // Additional data for PadOp.
+  llvm::SmallVector<IndexExpr, 4> pads;
+};
+
 //===----------------------------------------------------------------------===//
 // OneHot Op
 //===----------------------------------------------------------------------===//
diff --git a/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp b/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
index b00edc4a..f33bc18a 100644
--- a/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
+++ b/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
@@ -67,6 +67,49 @@ LogicalResult ONNXPadOpShapeHelper::computeShape() {
   return success();
 }

+LogicalResult ONNXPadV13OpShapeHelper::computeShape() {
+  ONNXPadOpAdaptor operandAdaptor(operands);
+  Value dataOperand = operandAdaptor.getData();
+  Value padsOperand = operandAdaptor.getPads();
+  DimsExpr outputDims;
+
+  // Get info about input data operand.
+  uint64_t dataRank = createIE->getShapedTypeRank(dataOperand);
+
+  // Initialize context and results (pads & output)
+  pads.resize(2 * dataRank); // pads two sides of each axis.
+  outputDims.resize(dataRank);
+
+  // `pads` format is : [x1_begin, x2_begin,...,x1_end, x2_end,...],
+  // where
+  // - xi_begin: the number of pad values added at the beginning of axis `i`
+  // - xi_end: the number of pad values added at the end of axis `i`.
+
+  // Calculate output dimension sizes.
+  for (uint64_t i = 0; i < dataRank; i++) {
+    // Get begin/end pads.
+    SymbolIndexExpr padBegin(createIE->getIntFromArrayAsSymbol(padsOperand, i));
+    SymbolIndexExpr padEnd(
+        createIE->getIntFromArrayAsSymbol(padsOperand, i + dataRank));
+    if (padBegin.isUndefined() || padEnd.isUndefined())
+      return op->emitError("pad parameter could not be processed");
+    // Get input dim.
+    DimIndexExpr dimInput(createIE->getShapeAsDim(dataOperand, i));
+
+    // Calculation for output size.
+    IndexExpr dimOutputFinal = (padBegin + dimInput) + padEnd;
+
+    // Save results.
+    pads[i] = padBegin;
+    pads[i + dataRank] = padEnd;
+    outputDims[i] = dimOutputFinal;
+  }
+
+  // Save the final result.
+  setOutputDims(outputDims);
+  return success();
+}
+
 } // namespace onnx_mlir

 //===----------------------------------------------------------------------===//
@@ -108,3 +151,15 @@ LogicalResult ONNXPadOp::inferShapes(
   ONNXPadOpShapeHelper shapeHelper(getOperation(), {});
   return shapeHelper.computeShapeAndUpdateType(elementType);
 }
+
+LogicalResult ONNXPadV13Op::inferShapes(
+    std::function<void(Region &)> doShapeInference) {
+  // Cannot infer shape if no shape exists.
+  if (!hasShapeAndRank(getData()) || !hasShapeAndRank(getPads()))
+    return success();
+
+  Type elementType = getData().getType().cast<ShapedType>().getElementType();
+
+  ONNXPadV13OpShapeHelper shapeHelper(getOperation(), {});
+  return shapeHelper.computeShapeAndUpdateType(elementType);
+}
diff --git a/src/Dialect/ONNX/ONNXUnsupportedOps.hpp b/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
index 758635e4..f5c07c76 100644
--- a/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
+++ b/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
@@ -56,7 +56,6 @@ UNSUPPORTED_OPS(ONNXMomentumOp)
 UNSUPPORTED_OPS(ONNXMultinomialOp)
 UNSUPPORTED_OPS(ONNXNegativeLogLikelihoodLossOp)
 UNSUPPORTED_OPS(ONNXNormalizerOp)
-UNSUPPORTED_OPS(ONNXPadV13Op)
 UNSUPPORTED_OPS(ONNXPadV11Op)
 UNSUPPORTED_OPS(ONNXPadV2Op)
 UNSUPPORTED_OPS(ONNXRandomUniformLikeOp)
diff --git a/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir b/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
index 834471e3..ff14f8d8 100644
--- a/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
+++ b/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
@@ -256,6 +256,15 @@ func.func @test_max(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> ten
 // CHECK-NEXT:      return [[VAR_0_]] : tensor<10x10xf32>
 }

+func.func @test_min(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> tensor<10x10xf32> {
+  %0 = "onnx.Min"(%arg0, %arg1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<10x10xf32>
+  "func.return"(%0) : (tensor<10x10xf32>) -> ()
+// CHECK-LABEL:  func @test_min
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<10x10xf32>, [[PARAM_1_:%.+]]: tensor<10x10xf32>) -> tensor<10x10xf32> {
+// CHECK-NEXT:      [[VAR_0_:%.+]] = mhlo.minimum [[PARAM_0_]], [[PARAM_1_]] : tensor<10x10xf32>
+// CHECK-NEXT:      return [[VAR_0_]] : tensor<10x10xf32>
+}
+
 func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
   %0 = "onnx.LeakyRelu"(%arg0) {alpha=0.5:f32} : (tensor<?x10xf32>) -> tensor<?x10xf32>
   "func.return"(%0) : (tensor<?x10xf32>) -> ()
@@ -275,6 +284,16 @@ func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32>

 // -----

+func.func @test_prelu_dynamic(%arg0 : tensor<?x10x12x12xf32>, %arg1: tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32> {
+  %0 = "onnx.PRelu"(%arg0, %arg1) : (tensor<?x10x12x12xf32>, tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32>
+  "func.return"(%0) : (tensor<?x10x12x12xf32>) -> ()
+// CHECK-LABEL:  func.func @test_prelu_dynamic
+// CHECK-SAME:   (%arg0: tensor<?x10x12x12xf32>, %arg1: tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32> {
+// CHECK:        [[VAR_0_:%.+]] = mhlo.multiply [[INP:%.+]], [[SLOPE:%.+]] : tensor<?x10x12x12xf32>
+// CHECK:        [[VAR_1_:%.+]] = mhlo.compare  GT, [[INP]], [[ZEROS:%.+]],  NOTYPE : (tensor<?x10x12x12xf32>, tensor<?x10x12x12xf32>) -> tensor<?x10x12x12xi1>
+// CHECK:        [[VAR_2_:%.+]] = mhlo.select [[VAR_1_]], [[INP]], [[VAR_0_]] : tensor<?x10x12x12xi1>, tensor<?x10x12x12xf32>
+}
+
 func.func @test_neg(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
   %0 = "onnx.Neg"(%arg0) : (tensor<10x10xf32>) -> tensor<10x10xf32>
   "func.return"(%0) : (tensor<10x10xf32>) -> ()
@@ -290,3 +309,10 @@ func.func @test_sin(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
 // CHECK-LABEL: func @test_sin
 // CHECK: %0 = mhlo.sine %arg0 : tensor<10x10xf32>
 }
+
+func.func @test_where(%arg0 : tensor<16x24x36xi1>, %arg1 : tensor<16x24x36xi64>, %arg2 : tensor<16x24x36xi64>) -> tensor<16x24x36xi64> {
+  %0 = "onnx.Where"(%arg0, %arg1, %arg2) : (tensor<16x24x36xi1>, tensor<16x24x36xi64>, tensor<16x24x36xi64>) -> tensor<16x24x36xi64>
+  "func.return"(%0) : (tensor<16x24x36xi64>) -> ()
+// CHECK-LABEL: func.func @test_where
+// CHECK:   %0 = mhlo.select %arg0, %arg1, %arg2 : tensor<16x24x36xi1>, tensor<16x24x36xi64>
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir b/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir
new file mode 100644
index 00000000..a4648439
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir
@@ -0,0 +1,19 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-mhlo %s --canonicalize -split-input-file | FileCheck %s
+
+func.func @test_onehot(%arg0 : tensor<2x3x4xi64>) -> tensor<*xi64> {
+  %0 = onnx.Constant dense<64> : tensor<1xi64>
+  %1 = onnx.Constant dense<[0, 1]> : tensor<2xi64>
+  %2 = "onnx.OneHot"(%arg0, %0, %1) {axis = -1 : si64} : (tensor<2x3x4xi64>, tensor<1xi64>, tensor<2xi64>) -> tensor<*xi64>
+  "func.return"(%2) : (tensor<*xi64>) -> ()
+// CHECK-LABEL: func.func @test_onehot
+// CHECK-SAME: (%[[ARG0:.+]]: tensor<2x3x4xi64>) -> tensor<2x3x4x64xi64> {
+// CHECK: %[[IOTA:.+]] = "mhlo.iota"() {iota_dimension = 0 : i64} : () -> tensor<64xi64>
+// CHECK: %[[BCAST_IOTA:.+]] = "mhlo.broadcast_in_dim"(%[[IOTA]]) {broadcast_dimensions = dense<3> : tensor<1xi64>} : (tensor<64xi64>) -> tensor<2x3x4x64xi64>
+// CHECK: %[[BCAST_ARG0:.+]] = "mhlo.broadcast_in_dim"(%[[ARG0]]) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<2x3x4xi64>) -> tensor<2x3x4x64xi64>
+// CHECK: %[[GE_ZERO:.+]] = mhlo.compare  GE, %[[BCAST_ARG0]], %[[BCAST_ZERO:.+]],  NOTYPE : (tensor<2x3x4x64xi64>, tensor<2x3x4x64xi64>) -> tensor<2x3x4x64xi1>
+// CHECK: %[[POS_ARG:.+]] = mhlo.add %[[BCAST_ARG0]], %[[BCAST_DEPTH:.+]] : tensor<2x3x4x64xi64>
+// CHECK: %[[NORM_ARG:.+]] = mhlo.select %[[GE_ZERO]], %[[BCAST_ARG0]], %[[POS_ARG]] : tensor<2x3x4x64xi1>, tensor<2x3x4x64xi64>
+// CHECK: %[[COMPARE:.+]] = mhlo.compare EQ, %[[NORM_ARG]], %[[BCAST_IOTA]], NOTYPE : (tensor<2x3x4x64xi64>, tensor<2x3x4x64xi64>) -> tensor<2x3x4x64xi1>
+// CHECK: %[[RESULT:.+]] = mhlo.select %[[COMPARE]], %[[ON_VALUE:.+]], %[[OFF_VALUE:.+]] : tensor<2x3x4x64xi1>, tensor<2x3x4x64xi64>
+// CHECK: return %[[RESULT]] : tensor<2x3x4x64xi64>
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir b/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir
new file mode 100644
index 00000000..bffb2b21
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir
@@ -0,0 +1,23 @@
+// RUN: onnx-mlir-opt --canonicalize --convert-onnx-to-mhlo %s -split-input-file | FileCheck %s
+
+func.func @test_scatternd_1(%arg0 : tensor<8xf32>, %arg1 : tensor<4x1xi64>, %arg2 : tensor<4xf32>) -> tensor<8xf32> {
+  %0 = "onnx.ScatterND"(%arg0, %arg1, %arg2) : (tensor<8xf32>, tensor<4x1xi64>, tensor<4xf32>) -> tensor<8xf32>
+  return %0 : tensor<8xf32>
+// CHECK-LABEL: func.func @test_scatternd_1(%arg0: tensor<8xf32>, %arg1: tensor<4x1xi64>, %arg2: tensor<4xf32>) -> tensor<8xf32> {
+// CHECK-NEXT:      %0 = "mhlo.scatter"(%arg0, %arg1, %arg2) ({
+// CHECK-NEXT:      ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+// CHECK-NEXT:        mhlo.return %arg4 : tensor<f32>
+// CHECK-NEXT:      }) {indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<8xf32>, tensor<4x1xi64>, tensor<4xf32>) -> tensor<8xf32>
+// CHECK-NEXT:      return %0 : tensor<8xf32>
+}
+
+func.func @test_scatternd_2(%arg0 : tensor<4x4x4xi32>, %arg1 : tensor<2x1xi64>, %arg2 : tensor<2x4x4xi32>) -> tensor<4x4x4xi32> {
+  %0 = "onnx.ScatterND"(%arg0, %arg1, %arg2) : (tensor<4x4x4xi32>, tensor<2x1xi64>, tensor<2x4x4xi32>) -> tensor<4x4x4xi32>
+  return %0 : tensor<4x4x4xi32>
+// CHECK-LABEL: func.func @test_scatternd_2(%arg0: tensor<4x4x4xi32>, %arg1: tensor<2x1xi64>, %arg2: tensor<2x4x4xi32>) -> tensor<4x4x4xi32> {
+// CHECK-NEXT:      %0 = "mhlo.scatter"(%arg0, %arg1, %arg2) ({
+// CHECK-NEXT:      ^bb0(%arg3: tensor<i32>, %arg4: tensor<i32>):
+// CHECK-NEXT:        mhlo.return %arg4 : tensor<i32>
+// CHECK-NEXT:      }) {indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<4x4x4xi32>, tensor<2x1xi64>, tensor<2x4x4xi32>) -> tensor<4x4x4xi32>
+// CHECK-NEXT:      return %0 : tensor<4x4x4xi32>
+}
