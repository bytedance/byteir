diff --git a/src/Conversion/ONNXToMhlo/CMakeLists.txt b/src/Conversion/ONNXToMhlo/CMakeLists.txt
index bd7283a9..1afd1521 100644
--- a/src/Conversion/ONNXToMhlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToMhlo/CMakeLists.txt
@@ -44,8 +44,10 @@ add_onnx_mlir_library(OMONNXToMhlo
   Tensor/Expand.cpp
   Tensor/Flatten.cpp
   Tensor/Gather.cpp
+  Tensor/GatherElements.cpp
   Tensor/Identity.cpp
   Tensor/Reshape.cpp
+  Tensor/Pad.cpp
   Tensor/Shape.cpp
   Tensor/Slice.cpp
   Tensor/Split.cpp
diff --git a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
index 98a9f71e..74425680 100644
--- a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
+++ b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
@@ -41,8 +41,10 @@ void populateONNXToMhloConversionPattern(
   populateLoweringONNXExpandOpToMhloPattern(patterns, ctx);
   populateLoweringONNXFlattenOpToMhloPattern(patterns, ctx);
   populateLoweringONNXGatherOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXGatherElementsOpToMhloPattern(patterns, ctx);
   populateLoweringONNXIdentityOpToMhloPattern(patterns, ctx);
   populateLoweringONNXReshapeOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXPadOpToMhloPattern(patterns, ctx);
   populateLoweringONNXShapeOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSliceOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSplitOpToMhloPattern(patterns, ctx);
@@ -89,7 +91,7 @@ void FrontendToMhloLoweringPass::runOnOperation() {
   // Added affine as some affine maps are generated by IndexExpression. It could
   // be disabled and/or replaced by shape max/min.
   target.addLegalDialect<mhlo::MhloDialect, func::FuncDialect,
-      arith::ArithDialect, shape::ShapeDialect, mlir::affine::AffineDialect>();
+      arith::ArithDialect, shape::ShapeDialect, affine::AffineDialect, tensor::TensorDialect>();
   // Needed to support unsigned int computations. To be removed if we use a
   // scheme that does not rely on the UnrealizedConversionCastOp.
   target.addLegalOp<::mlir::UnrealizedConversionCastOp>();
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
index 4e4adfc1..5b39e407 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
@@ -93,4 +93,17 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   }
   return broadcastedOperands;
 }
+
+ElementsAttr getElementAttributeFromMhloValue(Value value) {
+  auto definingOp = value.getDefiningOp();
+  if (auto constantOp = dyn_cast_or_null<mhlo::ConstantOp>(definingOp)) {
+    return constantOp.getValue().dyn_cast<ElementsAttr>();
+  } else if (auto constantOp =
+                 dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+    if (constantOp.getValue().has_value())
+      return constantOp.getValueAttr().dyn_cast<ElementsAttr>();
+  }
+  return nullptr;
+}
+
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
index ec5a9f2b..c63e1da8 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
@@ -19,6 +19,7 @@
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Math/IR/Math.h"
 #include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/PatternMatch.h"
 #include "mlir/Pass/Pass.h"
 #include "mlir/Transforms/DialectConversion.h"
@@ -113,6 +114,8 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     llvm::SmallVector<Value, 4> &operands, Type outputType,
     ConversionPatternRewriter &rewriter, Location loc, int64_t outputRank);

+mlir::ElementsAttr getElementAttributeFromMhloValue(mlir::Value value);
+
 // `Math` directory methods:
 void populateLoweringONNXClipOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -148,8 +151,12 @@ void populateLoweringONNXFlattenOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXGatherOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXGatherElementsOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXIdentityOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXPadOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXReshapeOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXShapeOpToMhloPattern(
diff --git a/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp b/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp
new file mode 100644
index 00000000..b7133e4f
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp
@@ -0,0 +1,134 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===-------- GatherElements.cpp - Lowering GatherElements Op -------------===//
+//
+// Copyright 2020-2022 The IBM Research Authors.
+//
+// =============================================================================
+//
+// This file lowers the ONNX GatherElements Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXGatherElementsOpLoweringToMhlo : public ConversionPattern {
+  ONNXGatherElementsOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(mlir::ONNXGatherElementsOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    ONNXGatherElementsOpAdaptor operandAdaptor(operands);
+    ONNXGatherElementsOp gatherOp = cast<ONNXGatherElementsOp>(op);
+    Location loc = op->getLoc();
+
+    IndexExprBuilderForMhlo createIE(rewriter, loc);
+    ONNXGatherElementsOpShapeHelper shapeHelper(op, operands, &createIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+
+    // Operands and attributes.
+    Value data = operandAdaptor.getData();
+    Value indices = operandAdaptor.getIndices();
+    int64_t axisLit = gatherOp.getAxis();
+
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    int64_t rank = inputType.getRank();  // indices has the same rank
+    ShapedType indicesType = indices.getType().cast<ShapedType>();
+    Type indexElemType = indicesType.getElementType();
+    // Negative value means counting dimensions from the back.
+    axisLit = axisLit < 0 ? axisLit + rank : axisLit;
+
+    // make sure all index values >= 0
+    Value zero = getShapedZero(loc, rewriter, indices);
+    Value inputShape = rewriter.create<shape::ShapeOfOp>(loc, data);
+    Value indicesShape = rewriter.create<shape::ShapeOfOp>(loc, indices);
+    Value axisDimSize =
+        rewriter.create<shape::GetExtentOp>(loc, inputShape, axisLit);
+    axisDimSize = rewriter.create<arith::IndexCastOp>(
+        loc, indexElemType, axisDimSize);
+    axisDimSize = rewriter.create<tensor::FromElementsOp>(loc, axisDimSize);
+    axisDimSize = rewriter.create<mhlo::ReshapeOp>(
+        loc, RankedTensorType::get(SmallVector<int64_t>{}, indexElemType), axisDimSize);
+    Value broadcastedAxisDimSize =
+        rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc, indicesType,
+            axisDimSize, indicesShape, rewriter.getI64TensorAttr({}));
+    Value isNegative = rewriter.create<mhlo::CompareOp>(
+        loc, indices, zero, mhlo::ComparisonDirection::LT);
+    Value positiveIndices =
+        rewriter.create<mhlo::AddOp>(loc, indicesType, indices, broadcastedAxisDimSize);
+    indices = rewriter.create<mhlo::SelectOp>(
+        loc, indicesType, isNegative, positiveIndices, indices);
+
+    // start indices
+    Value toConcatIndexShape;
+    SmallVector<Value> toConcatIndexShapeValueVec;
+    for (size_t i = 0; i < rank; i++) {
+      toConcatIndexShapeValueVec.push_back(rewriter.create<shape::GetExtentOp>(loc, indicesShape, i));
+    }
+    toConcatIndexShapeValueVec.push_back(rewriter.create<arith::ConstantIndexOp>(loc, 1));
+    toConcatIndexShape = rewriter.create<tensor::FromElementsOp>(loc, toConcatIndexShapeValueVec);
+
+    ArrayRef<int64_t> indicesShapeVec = indicesType.getShape();
+    SmallVector<int64_t> toConcatIndexShapeVec(indicesShapeVec.begin(), indicesShapeVec.end());
+    toConcatIndexShapeVec.push_back(1);
+    RankedTensorType toConcatIndexType = RankedTensorType::get(toConcatIndexShapeVec, indexElemType);
+
+    SmallVector<Value> toConcat;
+    for (int64_t i = 0; i < inputType.getRank(); ++i) {
+      if (i == axisLit) {
+        toConcat.push_back(rewriter.create<mhlo::DynamicReshapeOp>(
+            loc, toConcatIndexType, indices, toConcatIndexShape));
+      } else {
+        toConcat.push_back(rewriter.create<mhlo::DynamicIotaOp>(
+            loc, toConcatIndexType, toConcatIndexShape,
+            rewriter.getI64IntegerAttr(i)));
+      }
+    }
+    auto gatherIndicies = rewriter.create<mhlo::ConcatenateOp>(
+        loc, toConcat, static_cast<uint64_t>(inputType.getRank()));
+
+    // dimsAttr
+    SmallVector<int64_t> collapsedDims;
+    SmallVector<int64_t> startIndexMap;
+    for (int64_t i = 0; i < rank; i++) {
+      collapsedDims.push_back(i);
+      startIndexMap.push_back(i);
+    }
+    auto dimsAttr = mhlo::GatherDimensionNumbersAttr::get(
+      rewriter.getContext(),
+      /*offsetDims=*/{},
+      /*collapsedSliceDims=*/collapsedDims,
+      /*startIndexMap=*/startIndexMap,
+      /*indexVecDim=*/rank);
+    SmallVector<int64_t> sliceSizes(inputType.getRank(), 1);
+
+    Value gatherValue = rewriter.create<mhlo::GatherOp>(loc,
+        outputType, data, gatherIndicies, dimsAttr,
+        rewriter.getI64TensorAttr(sliceSizes));
+    rewriter.replaceOp(op, gatherValue);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXGatherElementsOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXGatherElementsOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
new file mode 100644
index 00000000..1482e9ee
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
@@ -0,0 +1,103 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===----------- Pad.cpp - Lowering Pad Op ------------===//
+//
+// Copyright 2022
+//
+// =============================================================================
+//
+// This file lowers ONNX Pad Operators to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ElementsAttr/DisposableElementsAttr.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXPadOpLoweringToMhlo : public ConversionPattern {
+  ONNXPadOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(mlir::ONNXPadOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+
+    Location loc = op->getLoc();
+    ONNXPadOpAdaptor operandAdaptor(operands, op->getAttrDictionary());
+    Value data = operandAdaptor.getData();
+    Value constantValue = operandAdaptor.getConstantValue();
+    Value pads = operandAdaptor.getPads();
+    Value axes = operandAdaptor.getAxes();
+    StringRef padMode = operandAdaptor.getMode();
+
+    if (!padMode.equals_insensitive("constant"))
+        return failure();
+    // only support axes is None
+    if (!isa<ONNXNoneOp>(axes.getDefiningOp()))
+        return failure();
+    assert(isRankedShapedType(data.getType()) && "Expected Ranked ShapedType");
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    Type elemType = inputType.getElementType();
+    int64_t rank = inputType.getRank();
+
+    Type outputType = *op->result_type_begin();
+    if (!constantValue || isNoneValue(constantValue)) {
+      // Pad with zeros by default
+      constantValue = rewriter.create<mhlo::ConstantOp>(
+          loc, DenseElementsAttr::get(mlir::RankedTensorType::get({}, elemType),
+              rewriter.getZeroAttr(elemType)));
+    } else {
+      // constantValue might be 1D tensor, reshape it to scalar
+      constantValue = rewriter.create<mhlo::ReshapeOp>(
+        loc, RankedTensorType::get({}, elemType), constantValue);
+    }
+    SmallVector<int64_t> edgePaddingLowVec(rank, 0);
+    SmallVector<int64_t> edgePaddingHighVec(rank, 0);
+    SmallVector<int64_t> interiorPaddingVec(rank, 0);
+    if (auto valueAttribute = getElementAttributeFromMhloValue(pads)) {
+      // If `pads` are constants, read them."
+      int64_t idx = 0;
+      for (IntegerAttr value : valueAttribute.getValues<IntegerAttr>()) {
+        int64_t padValue = value.getInt();
+        if (padValue < 0)
+          return failure();
+        if (idx < rank)
+          edgePaddingLowVec[idx] = padValue;
+        else
+          edgePaddingHighVec[idx - rank] = padValue;
+        idx++;
+      }
+    } else {
+      assert(false && "Pads must be known at compile time");
+    }
+
+    mlir::DenseIntElementsAttr edgePaddingLow =
+        rewriter.getI64VectorAttr(edgePaddingLowVec);
+    mlir::DenseIntElementsAttr edgePaddingHigh =
+        rewriter.getI64VectorAttr(edgePaddingHighVec);
+    mlir::DenseIntElementsAttr interiorPadding =
+        rewriter.getI64VectorAttr(interiorPaddingVec);
+    Value padResult = rewriter.create<mhlo::PadOp>(loc, outputType, data,
+        constantValue, edgePaddingLow, edgePaddingHigh, interiorPadding);
+
+    rewriter.replaceOp(op, padResult);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXPadOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXPadOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
index b5aa52f5..bbe5fd28 100644
--- a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
+++ b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
@@ -463,6 +463,16 @@ struct ONNXPadOpShapeHelper : public ONNXOpShapeHelper {
   llvm::SmallVector<IndexExpr, 4> pads;
 };

+struct ONNXPadV13OpShapeHelper : public ONNXOpShapeHelper {
+  ONNXPadV13OpShapeHelper(mlir::Operation *op, mlir::ValueRange operands,
+      IndexExprBuilder *ieBuilder = nullptr, IndexExprScope *scope = nullptr)
+      : ONNXOpShapeHelper(op, operands, ieBuilder, scope), pads() {}
+  virtual ~ONNXPadV13OpShapeHelper() {}
+  mlir::LogicalResult computeShape() final;
+  // Additional data for PadOp.
+  llvm::SmallVector<IndexExpr, 4> pads;
+};
+
 //===----------------------------------------------------------------------===//
 // OneHot Op
 //===----------------------------------------------------------------------===//
diff --git a/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp b/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
index b00edc4a..f33bc18a 100644
--- a/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
+++ b/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
@@ -67,6 +67,49 @@ LogicalResult ONNXPadOpShapeHelper::computeShape() {
   return success();
 }

+LogicalResult ONNXPadV13OpShapeHelper::computeShape() {
+  ONNXPadOpAdaptor operandAdaptor(operands);
+  Value dataOperand = operandAdaptor.getData();
+  Value padsOperand = operandAdaptor.getPads();
+  DimsExpr outputDims;
+
+  // Get info about input data operand.
+  uint64_t dataRank = createIE->getShapedTypeRank(dataOperand);
+
+  // Initialize context and results (pads & output)
+  pads.resize(2 * dataRank); // pads two sides of each axis.
+  outputDims.resize(dataRank);
+
+  // `pads` format is : [x1_begin, x2_begin,...,x1_end, x2_end,...],
+  // where
+  // - xi_begin: the number of pad values added at the beginning of axis `i`
+  // - xi_end: the number of pad values added at the end of axis `i`.
+
+  // Calculate output dimension sizes.
+  for (uint64_t i = 0; i < dataRank; i++) {
+    // Get begin/end pads.
+    SymbolIndexExpr padBegin(createIE->getIntFromArrayAsSymbol(padsOperand, i));
+    SymbolIndexExpr padEnd(
+        createIE->getIntFromArrayAsSymbol(padsOperand, i + dataRank));
+    if (padBegin.isUndefined() || padEnd.isUndefined())
+      return op->emitError("pad parameter could not be processed");
+    // Get input dim.
+    DimIndexExpr dimInput(createIE->getShapeAsDim(dataOperand, i));
+
+    // Calculation for output size.
+    IndexExpr dimOutputFinal = (padBegin + dimInput) + padEnd;
+
+    // Save results.
+    pads[i] = padBegin;
+    pads[i + dataRank] = padEnd;
+    outputDims[i] = dimOutputFinal;
+  }
+
+  // Save the final result.
+  setOutputDims(outputDims);
+  return success();
+}
+
 } // namespace onnx_mlir

 //===----------------------------------------------------------------------===//
@@ -108,3 +151,15 @@ LogicalResult ONNXPadOp::inferShapes(
   ONNXPadOpShapeHelper shapeHelper(getOperation(), {});
   return shapeHelper.computeShapeAndUpdateType(elementType);
 }
+
+LogicalResult ONNXPadV13Op::inferShapes(
+    std::function<void(Region &)> doShapeInference) {
+  // Cannot infer shape if no shape exists.
+  if (!hasShapeAndRank(getData()) || !hasShapeAndRank(getPads()))
+    return success();
+
+  Type elementType = getData().getType().cast<ShapedType>().getElementType();
+
+  ONNXPadV13OpShapeHelper shapeHelper(getOperation(), {});
+  return shapeHelper.computeShapeAndUpdateType(elementType);
+}
diff --git a/src/Dialect/ONNX/ONNXUnsupportedOps.hpp b/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
index 1ca21570..d431f29d 100644
--- a/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
+++ b/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
@@ -55,7 +55,6 @@ UNSUPPORTED_OPS(ONNXMomentumOp)
 UNSUPPORTED_OPS(ONNXMultinomialOp)
 UNSUPPORTED_OPS(ONNXNegativeLogLikelihoodLossOp)
 UNSUPPORTED_OPS(ONNXNormalizerOp)
-UNSUPPORTED_OPS(ONNXPadV13Op)
 UNSUPPORTED_OPS(ONNXPadV11Op)
 UNSUPPORTED_OPS(ONNXPadV2Op)
 UNSUPPORTED_OPS(ONNXRandomUniformLikeOp)
