diff --git a/src/Conversion/ONNXToMhlo/CMakeLists.txt b/src/Conversion/ONNXToMhlo/CMakeLists.txt
index bd7283a9..4f526871 100644
--- a/src/Conversion/ONNXToMhlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToMhlo/CMakeLists.txt
@@ -38,14 +38,21 @@ add_onnx_mlir_library(OMONNXToMhlo
   NN/ConvTranspose.cpp
   NN/Normalization.cpp
   NN/Pooling.cpp
+  RNN/LSTM.cpp
+  RNN/RNNBase.cpp
   Tensor/ArgMax.cpp
   Tensor/Concat.cpp
   Tensor/Constant.cpp
+  Tensor/DepthToSpace.cpp
   Tensor/Expand.cpp
   Tensor/Flatten.cpp
   Tensor/Gather.cpp
+  Tensor/GatherElements.cpp
   Tensor/Identity.cpp
+  Tensor/OneHot.cpp
+  Tensor/Pad.cpp
   Tensor/Reshape.cpp
+  Tensor/ScatterND.cpp
   Tensor/Shape.cpp
   Tensor/Slice.cpp
   Tensor/Split.cpp
@@ -75,7 +82,3 @@ target_link_libraries(MhloDialect PUBLIC
 target_link_libraries(StablehloTypeInference PUBLIC
   StablehloBase
   )
-
-target_link_libraries(StablehloTypeInference PUBLIC
-  StablehloBase
-  )
diff --git a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
index 73330c37..ac1f51cc 100644
--- a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
+++ b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
@@ -22,7 +22,7 @@ using namespace mlir;
 namespace onnx_mlir {

 void populateONNXToMhloConversionPattern(
-    RewritePatternSet &patterns, MLIRContext *ctx) {
+    RewritePatternSet &patterns, MLIRContext *ctx, bool enableUnroll) {
   // Math
   populateLoweringONNXClipOpToMhloPattern(patterns, ctx);
   populateLoweringONNXElementwiseOpToMhloPattern(patterns, ctx);
@@ -34,15 +34,22 @@ void populateONNXToMhloConversionPattern(
   populateLoweringONNXConvTransposeOpToMhloPattern(patterns, ctx);
   populateLoweringONNXNormalizationOpToMhloPattern(patterns, ctx);
   populateLoweringONNXPoolingOpToMhloPattern(patterns, ctx);
+  // Recurrent neural network
+  populateLoweringONNXLSTMOpToMhloPattern(patterns, ctx, enableUnroll);
   // Tensor
   populateLoweringONNXArgMaxOpToMhloPattern(patterns, ctx);
   populateLoweringONNXConcatOpToMhloPattern(patterns, ctx);
   populateLoweringONNXConstantOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXDepthToSpaceOpToMhloPattern(patterns, ctx);
   populateLoweringONNXExpandOpToMhloPattern(patterns, ctx);
   populateLoweringONNXFlattenOpToMhloPattern(patterns, ctx);
   populateLoweringONNXGatherOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXGatherElementsOpToMhloPattern(patterns, ctx);
   populateLoweringONNXIdentityOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXOneHotOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXPadOpToMhloPattern(patterns, ctx);
   populateLoweringONNXReshapeOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXScatterNDOpToMhloPattern(patterns, ctx);
   populateLoweringONNXShapeOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSliceOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSplitOpToMhloPattern(patterns, ctx);
@@ -70,12 +77,26 @@ struct FrontendToMhloLoweringPass
   FrontendToMhloLoweringPass() = default;
   FrontendToMhloLoweringPass(const FrontendToMhloLoweringPass &pass)
       : PassWrapper<FrontendToMhloLoweringPass, OperationPass<ModuleOp>>() {}
+  FrontendToMhloLoweringPass(bool enableUnroll) {
+    // Below, need explicit assignment to enable implicit conversion of bool
+    // to Option<bool>.
+    this->enableUnroll = enableUnroll;
+  }

   void getDependentDialects(DialectRegistry &registry) const override {
     registry.insert<mlir::mhlo::MhloDialect>();
+    registry.insert<shape::ShapeDialect>();
   }

   void runOnOperation() final;
+
+public:
+  // Some ops (RNN ops for example) will have loops inside them. We can
+  // choose to unroll the loop, which means to expand the loops completely
+  // so there are no loops left, or to rewrite the loop into mhlo::WhileOp
+  Option<bool> enableUnroll{*this, "enable-unroll",
+      llvm::cl::desc("Enable unroll rather than lowering to mhlo::WhileOp."),
+      llvm::cl::init(true)};
 };

 void FrontendToMhloLoweringPass::runOnOperation() {
@@ -89,17 +110,15 @@ void FrontendToMhloLoweringPass::runOnOperation() {
   // Added affine as some affine maps are generated by IndexExpression. It could
   // be disabled and/or replaced by shape max/min.
   target.addLegalDialect<mhlo::MhloDialect, func::FuncDialect,
-      arith::ArithDialect, shape::ShapeDialect, mlir::affine::AffineDialect>();
-  // Needed to support unsigned int computations. To be removed if we use a
-  // scheme that does not rely on the UnrealizedConversionCastOp.
-  target.addLegalOp<::mlir::UnrealizedConversionCastOp>();
+      arith::ArithDialect, shape::ShapeDialect, affine::AffineDialect,
+      tensor::TensorDialect>();

   // Now that the conversion target has been defined, we just need to provide
   // the set of patterns that will lower the frontend operations.
   RewritePatternSet patterns(&getContext());

   // Define patterns.
-  populateONNXToMhloConversionPattern(patterns, &getContext());
+  populateONNXToMhloConversionPattern(patterns, &getContext(), enableUnroll);

   // add illegal op
   target.addIllegalOp<ONNXSoftmaxOp>();
@@ -116,4 +135,8 @@ std::unique_ptr<Pass> createLowerToMhloPass() {
   return std::make_unique<FrontendToMhloLoweringPass>();
 }

+std::unique_ptr<Pass> createLowerToMhloPass(bool enableUnroll) {
+  return std::make_unique<FrontendToMhloLoweringPass>(enableUnroll);
+}
+
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/DialectBuilder.cpp b/src/Conversion/ONNXToMhlo/DialectBuilder.cpp
index eae15683..c13a482b 100644
--- a/src/Conversion/ONNXToMhlo/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToMhlo/DialectBuilder.cpp
@@ -12,16 +12,193 @@
 //
 //===----------------------------------------------------------------------===//

-#include "mlir/Dialect/Arith/IR/Arith.h"
-
 #include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+
+#include "mlir/Dialect/Arith/IR/Arith.h"
 #include "src/Dialect/ONNX/ONNXOps.hpp"
 #include "src/Support/TypeUtilities.hpp"
+#include "llvm/ADT/TypeSwitch.h"

 using namespace mlir;

 namespace onnx_mlir {

+// =============================================================================
+// mhlo Builder
+// =============================================================================
+
+Value MhloBuilder::constant(Type type, double val) const {
+  Value constant = nullptr;
+  // Could be a vector type; look at the element type.
+  Type elementType = type;
+  VectorType vectorType = type.dyn_cast<VectorType>();
+  if (vectorType)
+    elementType = vectorType.getElementType();
+  TypeSwitch<Type>(elementType)
+      .Case<Float16Type>([&](Type) {
+        constant =
+            b().create<mhlo::ConstantOp>(loc(), b().getF16FloatAttr(val));
+      })
+      .Case<Float32Type>([&](Type) {
+        constant =
+            b().create<mhlo::ConstantOp>(loc(), b().getF32FloatAttr(val));
+      })
+      .Case<Float64Type>([&](Type) {
+        constant =
+            b().create<mhlo::ConstantOp>(loc(), b().getF64FloatAttr(val));
+      })
+      .Case<IntegerType>([&](IntegerType elementType) {
+        assert(val == (int64_t)val && "value is ambiguous");
+        unsigned width = elementType.getWidth();
+
+        if (width == 1)
+          constant =
+              b().create<mhlo::ConstantOp>(loc(), b().getBoolAttr(val != 0));
+        else {
+          if (elementType.isUnsignedInteger()) {
+            constant = b().create<mhlo::ConstantOp>(
+                loc(), b().getIntegerAttr(
+                           elementType, APInt(width, (uint64_t)val, false)));
+          } else {
+            constant = b().create<mhlo::ConstantOp>(
+                loc(), b().getIntegerAttr(
+                           elementType, APInt(width, (int64_t)val, true)));
+          }
+        }
+      })
+      .Case<IndexType>([&](Type elementType) {
+        constant = b().create<mhlo::ConstantOp>(
+            loc(), b().getIntegerAttr(elementType, val));
+      })
+      .Default([](Type) { llvm_unreachable("unsupported element type"); });
+
+  assert(constant != nullptr && "Expecting valid constant value");
+  return constant;
+}
+
+Value MhloBuilder::constantI64(int64_t val) const {
+  IntegerAttr constantAttr = b().getIntegerAttr(b().getI64Type(), val);
+  return b().create<mhlo::ConstantOp>(loc(), constantAttr);
+}
+
+Value MhloBuilder::shaped_zero(Type type) const {
+  return b().create<mhlo::ConstantOp>(loc(), b().getZeroAttr(type));
+}
+
+Value MhloBuilder::reshape(Type type, Value input) const {
+  return b().create<mhlo::ReshapeOp>(loc(), type, input);
+}
+
+Value MhloBuilder::dynamic_reshape(Type type, Value input, Value shape) const {
+  return b().create<mhlo::DynamicReshapeOp>(loc(), type, input, shape);
+}
+
+Value MhloBuilder::real_dynamic_slice(Type type, Value operand,
+    Value startIndices, Value limitIndices, Value strides) const {
+  return b().create<mhlo::RealDynamicSliceOp>(
+      loc(), type, operand, startIndices, limitIndices, strides);
+}
+
+Value MhloBuilder::dynamic_slice(Value operand, SmallVector<Value> startIndices,
+    SmallVector<int64_t> sliceSizes) const {
+  return b().create<mhlo::DynamicSliceOp>(
+      loc(), operand, startIndices, GetI64ElementsAttr(sliceSizes, &b()));
+}
+
+Value MhloBuilder::dynamic_slice(Value operand, SmallVector<Value> startIndices,
+    DenseIntElementsAttr sliceSizes) const {
+  return b().create<mhlo::DynamicSliceOp>(
+      loc(), operand, startIndices, sliceSizes);
+}
+
+Value MhloBuilder::slice(Value operand, SmallVector<int64_t> startIndices,
+    SmallVector<int64_t> limitIndices, SmallVector<int64_t> strides) const {
+  return b().create<mhlo::SliceOp>(loc(), operand,
+      GetI64ElementsAttr(startIndices, &b()),
+      GetI64ElementsAttr(limitIndices, &b()),
+      GetI64ElementsAttr(strides, &b()));
+}
+
+Value MhloBuilder::slice(Value operand, DenseIntElementsAttr startIndices,
+    DenseIntElementsAttr limitIndices, DenseIntElementsAttr strides) const {
+  return b().create<mhlo::SliceOp>(
+      loc(), operand, startIndices, limitIndices, strides);
+}
+
+//===----------------------------------------------------------------------===//
+// Extends OnnxBuilder with member functions that might generate Mhlo related
+// dialect operations.
+//===----------------------------------------------------------------------===//
+
+Value OnnxToMhloBuilder::reshape(
+    const Value input, const ArrayRef<DimIndexExpr> shapeDims) const {
+  assert(!shapeDims.empty() && "Shape dimensions should not be empty");
+
+  ShapedType inputType = input.getType().cast<ShapedType>();
+  Type elementType = inputType.getElementType();
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder, ShapeBuilder> create(
+      b(), loc());
+
+  // If the output dimensions are all literals the 'onnx/Reshape' operation
+  // can take the new shape via an 'onnx.Constant'.
+  if (llvm::all_of(
+          shapeDims, [](const DimIndexExpr &dim) { return dim.isLiteral(); })) {
+    SmallVector<int64_t, 6> shape;
+    for (const IndexExpr &dim : shapeDims)
+      shape.push_back(dim.getLiteral());
+
+    auto constantOp = create.onnx.constantInt64(shape);
+
+    Value reshapeRes = create.onnx.reshape(
+        RankedTensorType::get(shape, elementType), input, constantOp);
+
+    return reshapeRes;
+  }
+
+  // When the output dimensions aren't all literals we need to generate code
+  // to compute the shape.
+  int64_t length = shapeDims.size();
+  SmallVector<Value> dims;
+  for (int64_t i = 0; i < length; ++i) {
+    Value data = shapeDims[i].getValue();
+    dims.push_back(data);
+  }
+
+  Value shapeExtent = create.shape.fromExtents(dims);
+  Value shapeTensor = create.shape.toExtentTensor(
+      RankedTensorType::get({length}, b().getIndexType()), shapeExtent);
+  // result shape
+  SmallVector<int64_t, 6> outputShape;
+  for (const IndexExpr &dim : shapeDims)
+    outputShape.push_back(
+        dim.isLiteral() ? dim.getLiteral() : ShapedType::kDynamic);
+  Value res = create.mhlo.dynamic_reshape(
+      RankedTensorType::get(outputShape, elementType), input, shapeTensor);
+  return res;
+}
+
+Value OnnxToMhloBuilder::transpose(const Value input,
+    const ArrayRef<int64_t> perm,
+    const ArrayRef<DimIndexExpr> outputDims) const {
+  assert(!outputDims.empty() && "Output dimensions should not be empty");
+  assert(!perm.empty() && perm.size() == outputDims.size() &&
+         "Expecting valid permutation array");
+  MultiDialectBuilder<OnnxBuilder> create(b(), loc());
+
+  // Compute the shape of the 'onnx.Transpose' result.
+  SmallVector<int64_t, 6> shape;
+  for (const IndexExpr &dim : outputDims)
+    shape.push_back(dim.isLiteral() ? dim.getLiteral() : ShapedType::kDynamic);
+
+  // Create the "onnx.Transpose" operation.
+  ShapedType inputType = input.getType().cast<ShapedType>();
+  Value transposeRes = create.onnx.transpose(
+      RankedTensorType::get(shape, inputType.getElementType()), input,
+      b().getI64ArrayAttr(perm));
+
+  return transposeRes;
+}
+
 // =============================================================================
 // IndexExpr Builder for Lowering using Shape/MHLO Dialect.
 // =============================================================================
diff --git a/src/Conversion/ONNXToMhlo/DialectBuilder.hpp b/src/Conversion/ONNXToMhlo/DialectBuilder.hpp
index 7d7ee86d..9ab26b84 100644
--- a/src/Conversion/ONNXToMhlo/DialectBuilder.hpp
+++ b/src/Conversion/ONNXToMhlo/DialectBuilder.hpp
@@ -14,18 +14,91 @@

 #pragma once

+#include "mhlo/IR/hlo_ops.h"
 #include "mlir/IR/Builders.h"
 #include "mlir/IR/Location.h"
 #include "mlir/IR/Value.h"
-
-#include "mhlo/IR/hlo_ops.h"
-
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
 #include "src/Dialect/Mlir/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/IndexExpr.hpp"
 #include "src/Dialect/Mlir/IndexExprBuilder.hpp"
+#include "src/Dialect/ONNX/DialectBuilder.hpp"

 namespace onnx_mlir {

+// =============================================================================
+// mhlo Builder
+// =============================================================================
+
+struct MhloBuilder : DialectBuilder {
+  MhloBuilder(mlir::Location loc) : DialectBuilder(loc) {}
+  MhloBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : DialectBuilder(b, loc), patternRewriter(&b) {}
+  MhloBuilder(const DialectBuilder &db) : DialectBuilder(db) {}
+  virtual ~MhloBuilder() {}
+
+  // ConstantOp
+  mlir::Value constant(mlir::Type type, double val) const;
+  mlir::Value constantI64(int64_t val) const;
+  mlir::Value shaped_zero(mlir::Type type) const;
+  // reshape
+  mlir::Value reshape(mlir::Type type, mlir::Value input) const;
+  mlir::Value dynamic_reshape(
+      mlir::Type type, mlir::Value input, mlir::Value shape) const;
+  // SliceOp
+  mlir::Value real_dynamic_slice(mlir::Type type, mlir::Value operand,
+      mlir::Value startIndices, mlir::Value limitIndices,
+      mlir::Value strides) const;
+  mlir::Value dynamic_slice(mlir::Value operand,
+      mlir::SmallVector<mlir::Value> startIndices,
+      mlir::SmallVector<int64_t> sliceSizes) const;
+  mlir::Value dynamic_slice(mlir::Value operand,
+      mlir::SmallVector<mlir::Value> startIndices,
+      mlir::DenseIntElementsAttr sliceSizes) const;
+  mlir::Value slice(mlir::Value operand,
+      mlir::SmallVector<int64_t> startIndices,
+      mlir::SmallVector<int64_t> limitIndices,
+      mlir::SmallVector<int64_t> strides) const;
+  mlir::Value slice(mlir::Value operand,
+      mlir::DenseIntElementsAttr startIndices,
+      mlir::DenseIntElementsAttr limitIndices,
+      mlir::DenseIntElementsAttr strides) const;
+
+protected:
+  // Private getters of builder (concise version).
+  mlir::OpBuilder &rewriter() const {
+    assert(patternRewriter && "rewriter is null");
+    return *patternRewriter;
+  }
+
+private:
+  mlir::OpBuilder *patternRewriter;
+};
+
+//===----------------------------------------------------------------------===//
+// Extends OnnxBuilder with member functions that might generate Mhlo related
+// dialect operations.
+//===----------------------------------------------------------------------===//
+
+struct OnnxToMhloBuilder : public OnnxBuilder {
+  OnnxToMhloBuilder(mlir::Location loc) : OnnxBuilder(loc) {}
+  OnnxToMhloBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : OnnxBuilder(b, loc) {}
+  OnnxToMhloBuilder(const DialectBuilder &db) : OnnxBuilder(db) {}
+  virtual ~OnnxToMhloBuilder() {}
+
+  // Generate an 'onnx.reshape' operation on the 'input' tensor, the new shape
+  // is provided by 'shapeDims'.
+  mlir::Value reshape(const mlir::Value input,
+      const llvm::ArrayRef<DimIndexExpr> shapeDims) const;
+
+  // Generate a 'onnx.Transpose' operation on the 'input' tensor given the
+  // permutation array 'perm' and the operator output dimensions 'outputDims'.
+  mlir::Value transpose(const mlir::Value input,
+      const llvm::ArrayRef<int64_t> perm,
+      const llvm::ArrayRef<DimIndexExpr> outputDims) const;
+};
+
 // =============================================================================
 // IndexExpr Builder for Shape lowering
 // =============================================================================
@@ -43,7 +116,34 @@ protected:
   mlir::Value getShapeVal(mlir::Value tensorOrMemrefValue, uint64_t i) final;
 };

-// Recursive class specialized for AffineBuilder refereed to as affine.
+// =============================================================================
+// MultiDialectBuilder for Mhlo
+// =============================================================================
+
+// Recursive class specialized for MhloBuilder referred to as
+// mhlo.
+template <class... Ts>
+struct MultiDialectBuilder<MhloBuilder, Ts...> : MultiDialectBuilder<Ts...> {
+  MultiDialectBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : MultiDialectBuilder<Ts...>(b, loc), mhlo(b, loc) {}
+  MultiDialectBuilder(const DialectBuilder &db)
+      : MultiDialectBuilder<Ts...>(db), mhlo(db) {}
+  MhloBuilder mhlo;
+};
+
+// Recursive class specialized for OnnxToMhloBuilder referred to as mhloOnnx.
+template <class... Ts>
+struct MultiDialectBuilder<OnnxToMhloBuilder, Ts...>
+    : MultiDialectBuilder<Ts...> {
+  MultiDialectBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : MultiDialectBuilder<Ts...>(b, loc), mhloOnnx(b, loc) {}
+  MultiDialectBuilder(const DialectBuilder &db)
+      : MultiDialectBuilder<Ts...>(db), mhloOnnx(db) {}
+  OnnxToMhloBuilder mhloOnnx;
+};
+
+// Recursive class specialized for IndexExprBuilderForMhlo referred to as
+// mhloIE.
 template <class... Ts>
 struct MultiDialectBuilder<IndexExprBuilderForMhlo, Ts...>
     : MultiDialectBuilder<Ts...> {
diff --git a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
index 26c392b8..b06fcc30 100644
--- a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
@@ -66,6 +66,11 @@ struct MhloDialectOp<ONNXMaxOp> {
   using Op = mhlo::MaxOp;
 };

+template <>
+struct MhloDialectOp<ONNXMinOp> {
+  using Op = mhlo::MinOp;
+};
+
 template <>
 struct MhloDialectOp<ONNXMulOp> {
   using Op = mhlo::MulOp;
@@ -106,6 +111,11 @@ struct MhloDialectOp<ONNXTanhOp> {
   using Op = mhlo::TanhOp;
 };

+template <>
+struct MhloDialectOp<ONNXWhereOp> {
+  using Op = mhlo::SelectOp;
+};
+
 namespace {

 template <typename ONNXOp>
@@ -163,6 +173,70 @@ struct ONNXElementwiseUnaryOpLoweringToMhlo : public ConversionPattern {
   }
 };

+// ONNXElu(x) = alpha * (exp(x) - 1.) for x < 0, f(x) = x for x >= 0.
+template <>
+struct ONNXElementwiseUnaryOpLoweringToMhlo<ONNXEluOp>
+    : public ConversionPattern {
+  ONNXElementwiseUnaryOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXEluOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    ONNXEluOpAdaptor operandAdaptor(operands);
+    ONNXEluOp EluOp = llvm::cast<ONNXEluOp>(op);
+    double alpha = EluOp.getAlpha().convertToDouble();
+
+    Type resultType = *op->result_type_begin();
+    Value inp = operandAdaptor.getX();
+    ShapedType inpType = inp.getType().dyn_cast_or_null<ShapedType>();
+    if (inpType == nullptr)
+      return failure();
+    Value alphaVal = getShapedFloat(loc, rewriter, alpha, inp);
+    Value oneVal = getShapedFloat(loc, rewriter, 1.0f, inp);
+    Value expVal = rewriter.create<mhlo::ExpOp>(loc, inp);
+    Value subVal = rewriter.create<mhlo::SubtractOp>(loc, expVal, oneVal);
+    Value mulVal = rewriter.create<mhlo::MulOp>(loc, alphaVal, subVal);
+    Value broadcastedZero = getShapedZero(loc, rewriter, inp);
+    Value compareGeZero = rewriter.create<mhlo::CompareOp>(
+        loc, inp, broadcastedZero, mhlo::ComparisonDirection::GE);
+    Value resultOp = rewriter.create<mhlo::SelectOp>(
+        loc, resultType, compareGeZero, inp, mulVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
+// ONNXHardSigmoid(x) = max(0, min(1, alpha * x + beta))
+template <>
+struct ONNXElementwiseUnaryOpLoweringToMhlo<ONNXHardSigmoidOp>
+    : public ConversionPattern {
+  ONNXElementwiseUnaryOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXHardSigmoidOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    ONNXHardSigmoidOpAdaptor operandAdaptor(operands);
+    ONNXHardSigmoidOp HardSigmoidOp = llvm::cast<ONNXHardSigmoidOp>(op);
+    double alpha = HardSigmoidOp.getAlpha().convertToDouble();
+    double beta = HardSigmoidOp.getBeta().convertToDouble();
+    Type resultType = *op->result_type_begin();
+    Value inp = operandAdaptor.getX();
+    ShapedType inpType = inp.getType().dyn_cast_or_null<ShapedType>();
+    if (inpType == nullptr)
+      return failure();
+    Value alphaVal = getShapedFloat(loc, rewriter, alpha, inp);
+    Value betaVal = getShapedFloat(loc, rewriter, beta, inp);
+    Value zeroVal = getShapedFloat(loc, rewriter, 0.0f, inp);
+    Value oneVal = getShapedFloat(loc, rewriter, 1.0f, inp);
+    Value productVal = rewriter.create<mhlo::MulOp>(loc, inp, alphaVal);
+    Value sumVal = rewriter.create<mhlo::AddOp>(loc, productVal, betaVal);
+    Value resultOp =
+        rewriter.create<mhlo::ClampOp>(loc, zeroVal, sumVal, oneVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
 // ONNXReluOp(x) is implemented using MHLO Max(x, 0)
 template <>
 struct ONNXElementwiseUnaryOpLoweringToMhlo<ONNXReluOp>
@@ -293,6 +367,40 @@ struct ONNXElementwiseBinaryOpLoweringToMhlo : public ConversionPattern {
   }
 };

+// ONNXPReluOp(x) = alpha * x if x < 0 else x.
+template <>
+struct ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPReluOp>
+    : public ConversionPattern {
+  ONNXElementwiseBinaryOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXPReluOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    // Prior code here used the "analysis" version that did not generate code.
+    // Since code is actually not needed here at this time, one could use
+    // IndexExprBuilderForAnalysis createIE(loc) instead.
+    IndexExprBuilderForMhlo createShapeIE(rewriter, loc);
+    ONNXBroadcastOpShapeHelper shapeHelper(op, operands, &createShapeIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    int64_t outputRank = shapeHelper.outputRank;
+    llvm::SmallVector<Value, 4> broadcastedOperands =
+        getBroadcastedOperands(op, rewriter, loc, outputRank);
+    Value inp = broadcastedOperands[0];
+    Value broadcastedSlope = broadcastedOperands[1];
+    Type resultType = *op->result_type_begin();
+    Value PReluActivationVal =
+        rewriter.create<mhlo::MulOp>(loc, inp, broadcastedSlope);
+    Value broadcastedZero = getShapedZero(loc, rewriter, inp);
+    Value compareGtZero = rewriter.create<mhlo::CompareOp>(
+        loc, inp, broadcastedZero, mhlo::ComparisonDirection::GT);
+    Value resultOp = rewriter.create<mhlo::SelectOp>(
+        loc, resultType, compareGtZero, inp, PReluActivationVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
 // Element-wise variadic ops lowering to Mhlo dialect.
 //===----------------------------------------------------------------------===//
 template <typename ElementwiseVariadicOp>
@@ -328,7 +436,9 @@ void populateLoweringONNXElementwiseOpToMhloPattern(
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXCastOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXCeilOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXCosOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXEluOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXExpOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXHardSigmoidOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLeakyReluOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLogOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXNegOp>,
@@ -343,12 +453,15 @@ void populateLoweringONNXElementwiseOpToMhloPattern(
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXLessOp>,
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXLessOrEqualOp>,
       ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPowOp>,
+      ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPReluOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXAddOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXAndOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXDivOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMaxOp>,
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMinOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMulOp>,
-      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXSubOp>>(ctx);
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXSubOp>,
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXWhereOp>>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
index 4e4adfc1..eafccf0d 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
@@ -14,6 +14,9 @@
 //===----------------------------------------------------------------------===//

 #include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/OpHelper.hpp"
+#include "src/Dialect/ONNX/OnnxElementsAttrBuilder.hpp"
+
 #include "stablehlo/dialect/BroadcastUtils.h"

 using namespace mlir;
@@ -44,11 +47,6 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(Operation *op,
   Type outputType = *op->result_type_begin();
   assert(outputType.isa<ShapedType>() && "output type is not shaped");
   ShapedType outputShapedType = outputType.cast<ShapedType>();
-  Type elementType =
-      op->getOperands()[0].getType().dyn_cast<ShapedType>().getElementType();
-  RankedTensorType broadcastedOutputType =
-      RankedTensorType::get(outputShapedType.getShape(), elementType);
-
   Value resultExtents =
       mlir::hlo::computeNaryElementwiseBroadcastingResultExtents(
           loc, op->getOperands(), rewriter);
@@ -58,6 +56,10 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(Operation *op,
     assert(operandType != nullptr && "operand type is not ranked");
     SmallVector<int64_t, 4> broadcastDimensions = llvm::to_vector<4>(
         llvm::seq<int64_t>(outputRank - operandType.getRank(), outputRank));
+    Type elementType =
+        operand.getType().dyn_cast<ShapedType>().getElementType();
+    RankedTensorType broadcastedOutputType =
+        RankedTensorType::get(outputShapedType.getShape(), elementType);
     Value broadcast = rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc,
         broadcastedOutputType, operand, resultExtents,
         rewriter.getI64TensorAttr(broadcastDimensions));
@@ -72,11 +74,6 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   llvm::SmallVector<Value, 4> broadcastedOperands;
   assert(outputType.isa<ShapedType>() && "output type is not shaped");
   ShapedType outputShapedType = outputType.cast<ShapedType>();
-  Type elementType =
-      operands[0].getType().dyn_cast<ShapedType>().getElementType();
-  RankedTensorType broadcastedOutputType =
-      RankedTensorType::get(outputShapedType.getShape(), elementType);
-
   Value resultExtents =
       mlir::hlo::computeNaryElementwiseBroadcastingResultExtents(
           loc, operands, rewriter);
@@ -86,6 +83,10 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     assert(operandType != nullptr && "operand type is not ranked");
     SmallVector<int64_t, 4> broadcastDimensions = llvm::to_vector<4>(
         llvm::seq<int64_t>(outputRank - operandType.getRank(), outputRank));
+    Type elementType =
+        operand.getType().dyn_cast<ShapedType>().getElementType();
+    RankedTensorType broadcastedOutputType =
+        RankedTensorType::get(outputShapedType.getShape(), elementType);
     Value broadcast = rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc,
         broadcastedOutputType, operand, resultExtents,
         rewriter.getI64TensorAttr(broadcastDimensions));
@@ -93,4 +94,148 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   }
   return broadcastedOperands;
 }
+
+ElementsAttr getElementAttributeFromConstValue(Value value) {
+  auto definingOp = value.getDefiningOp();
+  if (auto constantOp = dyn_cast_or_null<mhlo::ConstantOp>(definingOp)) {
+    return constantOp.getValue().dyn_cast<ElementsAttr>();
+  } else if (auto constantOp =
+                 dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+    if (constantOp.getValue().has_value())
+      return constantOp.getValueAttr().dyn_cast<ElementsAttr>();
+  }
+  return nullptr;
+}
+
+DenseIntElementsAttr GetI64ElementsAttr(
+    ArrayRef<int64_t> values, Builder *builder) {
+  RankedTensorType ty = RankedTensorType::get(
+      {static_cast<int64_t>(values.size())}, builder->getIntegerType(64));
+  return DenseIntElementsAttr::get(ty, values);
+}
+
+namespace {
+// Returns the DenseElementsAttr of input if it's a mhlo constant or
+// onnx.Constant. Otherwise returns a nullptr attribute.
+DenseElementsAttr getDenseElementAttrFromConstValue(mlir::Value value) {
+  Operation *definingOp = value.getDefiningOp();
+  if (auto globalOp = dyn_cast_or_null<mhlo::ConstantOp>(definingOp)) {
+    return globalOp.getValueAttr().dyn_cast<DenseElementsAttr>();
+  } else if (auto constOp =
+                 dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+    if (constOp.getValue().has_value())
+      return constOp.getValueAttr().dyn_cast<DenseElementsAttr>();
+  }
+  return nullptr;
+}
+} // namespace
+
+// Emit an ONNXSqueezeOp. If the input is constant, do const propagation,
+/// and return a constant.
+Value foldOrEmitONNXSqueezeOpMhlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr squeezedElements = inputElements.reshape(tensorType);
+    Value constVal = create.onnx.constant(squeezedElements);
+    return constVal;
+  } else {
+    return rewriter
+        .create<ONNXSqueezeOp>(loc, tensorType, create.onnx.toTensor(input),
+            create.onnx.constantInt64({axis}))
+        .getResult();
+  }
+}
+
+/// Emit an ONNXUnsqueezeOp. If the input is constant, do const
+/// propagation, and return a constant.
+Value foldOrEmitONNXUnsqueezeOpMhlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr unsqueezedElements = inputElements.reshape(tensorType);
+    Value constVal = create.onnx.constant(unsqueezedElements);
+    return constVal;
+  } else {
+    return rewriter
+        .create<ONNXUnsqueezeOp>(loc, tensorType, create.onnx.toTensor(input),
+            create.onnx.constantInt64({axis}))
+        .getResult();
+  }
+}
+
+/// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
+/// return constants.
+/// Only support evenly splitting.
+std::vector<Value> foldOrEmitONNXSplitOpMhlo(
+    ConversionPatternRewriter &rewriter, Location loc,
+    ArrayRef<Type> resultTypes, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  std::vector<Value> resVals;
+  int outputNum = resultTypes.size();
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    auto inputShape = inputElements.getType().getShape();
+    assert(outputNum == 0 || inputShape[axis] % outputNum == 0);
+    int64_t sizeOfEachSplit = outputNum != 0 ? inputShape[axis] / outputNum : 0;
+    SmallVector<int64_t, 4> sizes(outputNum, sizeOfEachSplit);
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    std::vector<ElementsAttr> splits =
+        elementsBuilder.split(inputElements, axis, sizes);
+    for (ElementsAttr splitElements : splits) {
+      // Avoid DisposableElementsAttr during conversion.
+      DenseElementsAttr denseSplitElements =
+          elementsBuilder.toDenseElementsAttr(splitElements);
+      Value constVal = create.onnx.constant(denseSplitElements);
+      resVals.emplace_back(constVal);
+    }
+  } else {
+    SmallVector<Type, 4> convertedTypes;
+    SmallVector<int64_t> splitSizesI64;
+    for (auto t : resultTypes) {
+      convertedTypes.emplace_back(create.onnx.toTensor(t));
+      splitSizesI64.emplace_back(t.cast<ShapedType>().getShape()[axis]);
+    }
+    Value splitSizes = create.onnx.constantInt64(splitSizesI64);
+    ONNXSplitOp split = rewriter.create<ONNXSplitOp>(loc, convertedTypes,
+        create.onnx.toTensor(input), splitSizes,
+        /*axis=*/axis, nullptr);
+    for (int i = 0; i < outputNum; ++i)
+      resVals.emplace_back(split.getOutputs()[i]);
+  }
+  return resVals;
+}
+
+/// Emit an ONNXTransposeOp. If the input is constant, do const propagation,
+/// and return a constant.
+Value foldOrEmitONNXTransposeOpMhlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, ArrayAttr permAttr) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    SmallVector<uint64_t, 4> perm;
+    for (auto permVal : permAttr.getValue())
+      perm.emplace_back(permVal.cast<IntegerAttr>().getInt());
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    ElementsAttr transposedElements =
+        elementsBuilder.transpose(inputElements, perm);
+    // Avoid DisposableElementsAttr during conversion.
+    DenseElementsAttr denseTransposedElements =
+        elementsBuilder.toDenseElementsAttr(transposedElements);
+    Value constVal = create.onnx.constant(denseTransposedElements);
+    return constVal;
+  } else {
+    return rewriter
+        .create<ONNXTransposeOp>(loc, create.onnx.toTensor(resultType),
+            create.onnx.toTensor(input), permAttr)
+        .getResult();
+  }
+}
+
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
index ec5a9f2b..fbb06e69 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
@@ -15,24 +15,25 @@

 #pragma once

+#include "mhlo/IR/hlo_ops.h"
 #include "mlir/Dialect/Affine/IR/AffineOps.h"
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Math/IR/Math.h"
 #include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/PatternMatch.h"
 #include "mlir/Pass/Pass.h"
 #include "mlir/Transforms/DialectConversion.h"
-#include "llvm/ADT/ArrayRef.h"
-#include "llvm/ADT/Sequence.h"
-#include "llvm/ADT/TypeSwitch.h"
-
-#include "mhlo/IR/hlo_ops.h"
-
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Dialect/Mlir/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/IndexExpr.hpp"
 #include "src/Dialect/ONNX/DialectBuilder.hpp"
 #include "src/Dialect/ONNX/ONNXOps.hpp"
 #include "src/Dialect/ONNX/ONNXOps/OpHelper.hpp"
 #include "src/Pass/Passes.hpp"
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/Sequence.h"
+#include "llvm/ADT/TypeSwitch.h"

 using namespace mlir;

@@ -113,6 +114,40 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     llvm::SmallVector<Value, 4> &operands, Type outputType,
     ConversionPatternRewriter &rewriter, Location loc, int64_t outputRank);

+mlir::ElementsAttr getElementAttributeFromConstValue(mlir::Value value);
+
+DenseIntElementsAttr GetI64ElementsAttr(
+    ArrayRef<int64_t> values, Builder *builder);
+
+//===----------------------------------------------------------------------===//
+// Fold and emit support.
+//===----------------------------------------------------------------------===//
+
+/// Emit an ONNXSqueezeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXSqueezeOpMhlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXUnsqueezeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXUnsqueezeOpMhlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
+/// return constants.
+/// Only support evenly splitting.
+std::vector<mlir::Value> foldOrEmitONNXSplitOpMhlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    llvm::ArrayRef<mlir::Type> resultTypes, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXTransposeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXTransposeOpMhlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, mlir::ArrayAttr permAttr);
+
 // `Math` directory methods:
 void populateLoweringONNXClipOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -133,6 +168,9 @@ void populateLoweringONNXNormalizationOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXPoolingOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+// `RNN` directory methods:
+void populateLoweringONNXLSTMOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *, bool);
 // `Tensor` directory methods:
 void populateLoweringONNXArgMaxOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -140,7 +178,7 @@ void populateLoweringONNXConcatOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXConstantOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXReshapeOpToMhloPattern(
+void populateLoweringONNXDepthToSpaceOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXExpandOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -148,10 +186,17 @@ void populateLoweringONNXFlattenOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXGatherOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXGatherElementsOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXIdentityOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXOneHotOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXPadOpToMhloPattern(RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXReshapeOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXScatterNDOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXShapeOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXSliceOpToMhloPattern(
diff --git a/src/Conversion/ONNXToMhlo/RNN/LSTM.cpp b/src/Conversion/ONNXToMhlo/RNN/LSTM.cpp
new file mode 100644
index 00000000..2496c495
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/RNN/LSTM.cpp
@@ -0,0 +1,819 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- LSTM.cpp - Lowering LSTM Op --------------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX LSTM Operators to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp"
+#include "src/Dialect/Mlir/DialectBuilder.hpp"
+
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "lstm"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace mhlo {
+
+struct LstmState {
+  SmallVector<Value, 512> forwardAllH;
+  SmallVector<Value, 512> reverseAllH;
+
+  Value allHForward;
+  Value allHReverse;
+
+  Value ht;
+  Value ct;
+  // intermediate states.
+  Value forwardHt;
+  Value reverseHt;
+  Value forwardCt;
+  Value reverseCt;
+};
+
+struct LstmActivationPack {
+  RNNActivation f;
+  RNNActivation g;
+  RNNActivation h;
+};
+
+struct LstmWeightPack {
+  Value WT;
+  Value RT;
+};
+
+struct LstmBiasPack {
+  bool hasBias = false;
+  Value Wbi;
+  Value Wbo;
+  Value Wbf;
+  Value Wbc;
+  Value Rbi;
+  Value Rbo;
+  Value Rbf;
+  Value Rbc;
+  // Put peephole here.
+  bool hasPeephole = false;
+  Value Pi;
+  Value Po;
+  Value Pf;
+};
+
+template <>
+bool hasAllNoneOutput<ONNXLSTMOp>(ONNXLSTMOp *op) {
+  return (isNoneValue(op->getY()) && isNoneValue(op->getYH()) &&
+          isNoneValue(op->getYC()));
+}
+
+template <>
+std::tuple<LstmActivationPack, LstmActivationPack>
+getActivationPack<ONNXLSTMOp, LstmActivationPack>(ONNXLSTMOp *op) {
+  auto direction = op->getDirection();
+  auto activations = op->getActivations();
+  auto activationAlpha = op->getActivationAlpha();
+  auto activationBeta = op->getActivationBeta();
+
+  LstmActivationPack activationForward, activationReverse;
+
+  // Get activation function name.
+  // Default forward functions
+  activationForward.f.name = "sigmoid";
+  activationForward.g.name = "tanh";
+  activationForward.h.name = "tanh";
+  // Default backward functions
+  activationReverse.f.name = "sigmoid";
+  activationReverse.g.name = "tanh";
+  activationReverse.h.name = "tanh";
+  if (activations) {
+    ArrayAttr activationArrAttr = activations.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.name =
+            activationArrAttr[0].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.name =
+            activationArrAttr[1].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.name =
+            activationArrAttr[2].cast<StringAttr>().getValue();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.name =
+            activationArrAttr[startIndex].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.name =
+            activationArrAttr[startIndex + 1].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.name =
+            activationArrAttr[startIndex + 2].cast<StringAttr>().getValue();
+      }
+    }
+  }
+
+  // Get alpha attributes.
+  if (activationAlpha) {
+    ArrayAttr activationArrAttr = activationAlpha.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.alpha = activationArrAttr[0].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.alpha = activationArrAttr[1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.alpha = activationArrAttr[2].cast<FloatAttr>();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.alpha =
+            activationArrAttr[startIndex].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.alpha =
+            activationArrAttr[startIndex + 1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.alpha =
+            activationArrAttr[startIndex + 2].cast<FloatAttr>();
+      }
+    }
+  }
+
+  // Get beta attributes.
+  if (activationBeta) {
+    ArrayAttr activationArrAttr = activationBeta.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.beta = activationArrAttr[0].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.beta = activationArrAttr[1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.beta = activationArrAttr[2].cast<FloatAttr>();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.beta =
+            activationArrAttr[startIndex].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.beta =
+            activationArrAttr[startIndex + 1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.beta =
+            activationArrAttr[startIndex + 2].cast<FloatAttr>();
+      }
+    }
+  }
+
+  return std::make_tuple(activationForward, activationReverse);
+}
+
+template <>
+std::tuple<LstmWeightPack, LstmWeightPack>
+getWeightPack<ONNXLSTMOp, LstmWeightPack>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op) {
+  // Return values.
+  LstmWeightPack weightForward, weightReverse;
+
+  // parameter weight: [direction, 4*hiddenSize, inputSize]
+  Value W = op->getW();
+  // recurrence weight: [direction, 4*hiddenSize, hiddenSize]
+  Value R = op->getR();
+  // direction
+  StringRef direction = op->getDirection();
+
+  ArrayRef<int64_t> wShape = W.getType().cast<ShapedType>().getShape();
+  Type elementType = W.getType().cast<ShapedType>().getElementType();
+  int64_t hiddenSize = wShape[1] / 4;
+  int64_t inputSize = wShape[2];
+
+  // RankedTensorType types for parameter weights.
+  auto w3DTy =
+      RankedTensorType::get({1, 4 * hiddenSize, inputSize}, elementType);
+  auto w2DTy = RankedTensorType::get({4 * hiddenSize, inputSize}, elementType);
+  auto wTranspose2DTy =
+      RankedTensorType::get({inputSize, 4 * hiddenSize}, elementType);
+  SmallVector<Type, 4> w3D2Ty(2, w3DTy);
+
+  // RankedTensorType types for recurrence weights.
+  auto r3DTy =
+      RankedTensorType::get({1, 4 * hiddenSize, hiddenSize}, elementType);
+  auto r2DTy = RankedTensorType::get({4 * hiddenSize, hiddenSize}, elementType);
+  auto rTranspose2DTy =
+      RankedTensorType::get({hiddenSize, 4 * hiddenSize}, elementType);
+  SmallVector<Type, 4> r3D2Ty(2, r3DTy);
+
+  // Squeeze the direction axis from W and R.
+  Value fW, bW, fR, bR;
+  if (direction == FORWARD) {
+    fW = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, w2DTy, W, /*axis=*/0);
+    fR = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, r2DTy, R, /*axis=*/0);
+  } else if (direction == REVERSE) {
+    bW = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, w2DTy, W, /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, r2DTy, R, /*axis=*/0);
+  } else { // BIDIRECTIONAL
+    // W
+    std::vector<Value> vals =
+        foldOrEmitONNXSplitOpMhlo(rewriter, loc, w3D2Ty, W, 0);
+    fW = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, w2DTy, vals[0], /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, w2DTy, vals[1], /*axis=*/0);
+    // R
+    vals.clear();
+    vals = foldOrEmitONNXSplitOpMhlo(rewriter, loc, r3D2Ty, R, 0);
+    fR = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, r2DTy, vals[0], /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, r2DTy, vals[1], /*axis=*/0);
+  }
+
+  // Transpose W and R.
+  ArrayAttr permAttr = rewriter.getI64ArrayAttr({1, 0});
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    // W
+    weightForward.WT = foldOrEmitONNXTransposeOpMhlo(
+        rewriter, loc, wTranspose2DTy, fW, permAttr);
+    // R
+    weightForward.RT = foldOrEmitONNXTransposeOpMhlo(
+        rewriter, loc, rTranspose2DTy, fR, permAttr);
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    // W
+    weightReverse.WT = foldOrEmitONNXTransposeOpMhlo(
+        rewriter, loc, wTranspose2DTy, bW, permAttr);
+    // R
+    weightReverse.RT = foldOrEmitONNXTransposeOpMhlo(
+        rewriter, loc, rTranspose2DTy, bR, permAttr);
+  }
+  return std::make_tuple(weightForward, weightReverse);
+}
+
+template <>
+std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op) {
+  // Return values.
+  LstmBiasPack biasForward, biasReverse;
+
+  // bias: [direction, 8*hiddenSize] for both parameter and recurrence weights.
+  Value B = op->getB();
+  // peephold: [direction, 3*hiddenSize] for input, output and forget gates.
+  Value P = op->getP();
+
+  // direction
+  StringRef direction = op->getDirection();
+
+  // Split B.
+  if (!isNoneValue(B)) {
+    ArrayRef<int64_t> bShape = B.getType().cast<ShapedType>().getShape();
+    Type elementType = B.getType().cast<ShapedType>().getElementType();
+    int64_t hiddenSize = bShape[1] / 8;
+
+    // MemRef types.
+    auto bType2D = RankedTensorType::get({1, 8 * hiddenSize}, elementType);
+    auto bType1D = RankedTensorType::get({8 * hiddenSize}, elementType);
+    auto bSplitType1D = RankedTensorType::get({hiddenSize}, elementType);
+    SmallVector<Type, 4> split1D8Ty(8, bSplitType1D);
+    SmallVector<Type, 4> split2D2Ty(2, bType2D);
+
+    // Squeeze the direction axis from B.
+    Value fB, bB;
+    if (direction == FORWARD) {
+      fB = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, bType1D, B, /*axis=*/0);
+    } else if (direction == REVERSE) {
+      bB = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, bType1D, B, /*axis=*/0);
+    } else { // BIDIRECTIONAL
+      std::vector<Value> vals;
+      vals = foldOrEmitONNXSplitOpMhlo(rewriter, loc, split2D2Ty, B, 0);
+      fB = foldOrEmitONNXSqueezeOpMhlo(
+          rewriter, loc, bType1D, vals[0], /*axis=*/0);
+      bB = foldOrEmitONNXSqueezeOpMhlo(
+          rewriter, loc, bType1D, vals[1], /*axis=*/0);
+    }
+
+    // Split B into individual bias tensors.
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split1D8Ty, fB, 0);
+      biasForward.Wbi = vals[0];
+      biasForward.Wbo = vals[1];
+      biasForward.Wbf = vals[2];
+      biasForward.Wbc = vals[3];
+      biasForward.Rbi = vals[4];
+      biasForward.Rbo = vals[5];
+      biasForward.Rbf = vals[6];
+      biasForward.Rbc = vals[7];
+      biasForward.hasBias = true;
+    }
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split1D8Ty, bB, 0);
+      biasReverse.Wbi = vals[0];
+      biasReverse.Wbo = vals[1];
+      biasReverse.Wbf = vals[2];
+      biasReverse.Wbc = vals[3];
+      biasReverse.Rbi = vals[4];
+      biasReverse.Rbo = vals[5];
+      biasReverse.Rbf = vals[6];
+      biasReverse.Rbc = vals[7];
+      biasReverse.hasBias = true;
+    }
+  }
+
+  // Split P.
+  if (!isNoneValue(P)) {
+    ArrayRef<int64_t> pShape = P.getType().cast<ShapedType>().getShape();
+    Type elementType = P.getType().cast<ShapedType>().getElementType();
+    int64_t hiddenSize = pShape[1] / 3;
+
+    // MemRef types.
+    auto pType2D = RankedTensorType::get({1, 3 * hiddenSize}, elementType);
+    auto pType1D = RankedTensorType::get({3 * hiddenSize}, elementType);
+    auto pSplitType1D = RankedTensorType::get({hiddenSize}, elementType);
+    SmallVector<Type, 4> split1D3Ty(3, pSplitType1D);
+    SmallVector<Type, 4> split2D2Ty(2, pType2D);
+
+    // Squeeze the direction axis from P.
+    Value fP, bP;
+    if (direction == FORWARD) {
+      fP = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, pType1D, P, /*axis=*/0);
+    } else if (direction == REVERSE) {
+      bP = foldOrEmitONNXSqueezeOpMhlo(rewriter, loc, pType1D, P, /*axis=*/0);
+    } else { // BIDIRECTIONAL
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split2D2Ty, P, 0);
+      fP = foldOrEmitONNXSqueezeOpMhlo(
+          rewriter, loc, pType1D, vals[0], /*axis=*/0);
+      bP = foldOrEmitONNXSqueezeOpMhlo(
+          rewriter, loc, pType1D, vals[1], /*axis=*/0);
+    }
+
+    // Split P into individual tensors.
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split1D3Ty, fP, 0);
+      biasForward.Pi = vals[0];
+      biasForward.Po = vals[1];
+      biasForward.Pf = vals[2];
+      biasForward.hasPeephole = true;
+    }
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpMhlo(rewriter, loc, split1D3Ty, bP, 0);
+      biasReverse.Pi = vals[0];
+      biasReverse.Po = vals[1];
+      biasReverse.Pf = vals[2];
+      biasReverse.hasPeephole = true;
+    }
+  }
+
+  return std::make_tuple(biasForward, biasReverse);
+}
+
+template <>
+LstmState allocAndInitializeStates<ONNXLSTMOp, LstmState>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op,
+    typename ONNXLSTMOp::Adaptor operandAdaptor, bool enableUnroll) {
+  LstmState state;
+
+  // direction
+  StringRef direction = op->getDirection();
+
+  // allocation for the results of this operation.
+  // If the result is not returned, then no allocation happens.
+  if (!enableUnroll) {
+    if (direction == FORWARD || direction == BIDIRECTIONAL)
+      state.allHForward = allocAllHidden(
+          rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+    if (direction == REVERSE || direction == BIDIRECTIONAL)
+      state.allHReverse = allocAllHidden(
+          rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+  }
+  // Y :: [seq_length, num_directions, batch_size, hidden_size]
+  // Y_h :: [num_directions, batch_size, hidden_size]
+  state.ht = allocHiddenOrCell(rewriter, loc, operandAdaptor.getX(),
+      operandAdaptor.getW(), operandAdaptor.getR());
+  // Y_c :: [num_directions, batch_size, hidden_size]
+  state.ct = allocHiddenOrCell(rewriter, loc, operandAdaptor.getX(),
+      operandAdaptor.getW(), operandAdaptor.getR());
+
+  // Insert allocation and deallocation the intermediate Ht and Ct for the
+  // forward and reverse directions.
+  // Ht :: [batch_size, hidden_size]
+  // Ct :: [batch_size, hidden_size]
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    state.forwardHt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+    state.forwardCt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    state.reverseHt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+    state.reverseCt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+  }
+
+  // Initialize Ht and Ct.
+  initializeIntermediateStates(rewriter, loc, state.forwardHt, state.reverseHt,
+      state.forwardCt, state.reverseCt, operandAdaptor.getInitialH(),
+      operandAdaptor.getInitialC(),
+      operandAdaptor.getX().getType().cast<RankedTensorType>().getElementType(),
+      direction, /*onlyHidden=*/false);
+  return state;
+}
+
+template <>
+void calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+    LstmBiasPack>(ConversionPatternRewriter &rewriter, Location loc, Value Xt,
+    LstmState &state, LstmActivationPack activationPack,
+    LstmWeightPack weightPack, LstmBiasPack biasPack, Value sequenceIV,
+    Value directionIV, Value sequenceLens, Value initialH, bool enableUnroll,
+    bool isForward) {
+  // Equations for LSTM.
+  // it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)
+  // ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)
+  // ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)
+  // Ct = ft (.) Ct-1 + it (.) ct
+  // ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)
+  // Ht = ot (.) h(Ct)
+
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+
+  ArrayRef<int64_t> xtShape = Xt.getType().cast<ShapedType>().getShape();
+  int64_t batchSize = xtShape[0];
+
+  // Get Ht, Ct.
+  Value Ht = (isForward) ? state.forwardHt : state.reverseHt;
+  Value Ct = (isForward) ? state.forwardCt : state.reverseCt;
+
+  ArrayRef<int64_t> htShape = Ht.getType().cast<ShapedType>().getShape();
+  int64_t hiddenSize = htShape[1];
+
+  // Frequently used types.
+  RankedTensorType matrixType = Ht.getType().cast<RankedTensorType>();
+  Type elementType = matrixType.getElementType();
+  RankedTensorType matrixAllGatesType =
+      RankedTensorType::get({batchSize, 4 * hiddenSize}, elementType);
+
+  // Do matrix multiplications.
+  // Xt * (Wi^T ++ Wo^T ++ Wf^T ++ Wc^T)
+  // Ht * (Ri^T ++ Ro^T ++ Rf^T ++ Rc^T)
+  // where '++' is matrix concatenation.
+  // XtWT: [B, 4H], HtRT: [B, 4H]
+  Value XtWT = create.onnx.matmul(matrixAllGatesType, Xt, weightPack.WT);
+  Value HtRT = create.onnx.matmul(matrixAllGatesType, Ht, weightPack.RT);
+  Value commonSum = create.onnx.add(XtWT, HtRT);
+  Value zeroIndex = create.mhlo.constantI64(0);
+  Value oneHiddenIndex = create.mhlo.constantI64(hiddenSize);
+  Value twoHiddenIndex = create.mhlo.constantI64(2 * hiddenSize);
+  Value threeHiddenIndex = create.mhlo.constantI64(3 * hiddenSize);
+  SmallVector<int64_t> sliceSizes = {batchSize, hiddenSize};
+  SmallVector<Value> iStartIndices = {zeroIndex, zeroIndex};
+  SmallVector<Value> oStartIndices = {zeroIndex, oneHiddenIndex};
+  SmallVector<Value> fStartIndices = {zeroIndex, twoHiddenIndex};
+  SmallVector<Value> cStartIndices = {zeroIndex, threeHiddenIndex};
+  Value it = create.mhlo.dynamic_slice(commonSum, iStartIndices, sliceSizes);
+  Value ot = create.mhlo.dynamic_slice(commonSum, oStartIndices, sliceSizes);
+  Value ft = create.mhlo.dynamic_slice(commonSum, fStartIndices, sliceSizes);
+  Value ct = create.mhlo.dynamic_slice(commonSum, cStartIndices, sliceSizes);
+  if (biasPack.hasBias) {
+    it = create.onnx.add(it, biasPack.Wbi);
+    it = create.onnx.add(it, biasPack.Rbi);
+  }
+  if (biasPack.hasPeephole) {
+    Value PiCt = create.onnx.mul(biasPack.Pi, Ct);
+    it = create.onnx.add(it, PiCt);
+  }
+  it = applyActivation(rewriter, loc, activationPack.f, it);
+  if (biasPack.hasBias) {
+    ft = create.onnx.add(ft, biasPack.Wbf);
+    ft = create.onnx.add(ft, biasPack.Rbf);
+  }
+  if (biasPack.hasPeephole) {
+    Value PfCt = create.onnx.mul(biasPack.Pf, Ct);
+    ft = create.onnx.add(ft, PfCt);
+  }
+  ft = applyActivation(rewriter, loc, activationPack.f, ft);
+  if (biasPack.hasBias) {
+    ct = create.onnx.add(ct, biasPack.Wbc);
+    ct = create.onnx.add(ct, biasPack.Rbc);
+  }
+  ct = applyActivation(rewriter, loc, activationPack.g, ct);
+
+  Value ftCt = create.onnx.mul(ft, Ct);
+  Value itct = create.onnx.mul(it, ct);
+  Value nextCt = create.onnx.add(ftCt, itct);
+
+  if (biasPack.hasBias) {
+    ot = create.onnx.add(ot, biasPack.Wbo);
+    ot = create.onnx.add(ot, biasPack.Rbo);
+  }
+  if (biasPack.hasPeephole) {
+    Value PoCt = create.onnx.mul(biasPack.Po, nextCt);
+    ot = create.onnx.add(ot, PoCt);
+  }
+  ot = applyActivation(rewriter, loc, activationPack.f, ot);
+  // Ht = ot (.) h(Ct)
+  Value nextHt = applyActivation(rewriter, loc, activationPack.h, nextCt);
+  nextHt = create.onnx.mul(ot, nextHt);
+  if (isForward) {
+    state.forwardHt = nextHt;
+    state.forwardCt = nextCt;
+  } else {
+    state.reverseHt = nextHt;
+    state.reverseCt = nextCt;
+  }
+  if (enableUnroll) {
+    RankedTensorType unsqueezedHtType =
+        RankedTensorType::get({1, 1, batchSize, hiddenSize}, elementType);
+    if (isForward)
+      state.forwardAllH.emplace_back(create.onnx.unsqueeze(
+          unsqueezedHtType, nextHt, create.onnx.constantInt64({0, 1})));
+    else
+      state.reverseAllH.insert(state.reverseAllH.begin(),
+          create.onnx.unsqueeze(
+              unsqueezedHtType, nextHt, create.onnx.constantInt64({0, 1})));
+  } else {
+    RankedTensorType unsqueezedHtType =
+        RankedTensorType::get({1, 1, batchSize, hiddenSize}, elementType);
+    RankedTensorType unsqueezedIdxType =
+        RankedTensorType::get({1, 1}, rewriter.getI64Type());
+    Value unsqueezedHt = create.onnx.unsqueeze(
+        unsqueezedHtType, nextHt, create.onnx.constantInt64({0, 1}));
+    Value unsqueezedIdx = create.onnx.unsqueeze(
+        unsqueezedIdxType, sequenceIV, create.onnx.constantInt64({0}));
+    if (isForward)
+      state.allHForward =
+          rewriter.create<ONNXScatterNDOp>(loc, state.allHForward.getType(),
+              state.allHForward, unsqueezedIdx, unsqueezedHt);
+    else
+      state.allHReverse =
+          rewriter.create<ONNXScatterNDOp>(loc, state.allHReverse.getType(),
+              state.allHReverse, unsqueezedIdx, unsqueezedHt);
+  }
+}
+
+template <>
+void stateToOutput<ONNXLSTMOp, LstmState>(ConversionPatternRewriter &rewriter,
+    Location loc, ONNXLSTMOp *op, LstmState state, std::vector<Value> &outputs,
+    bool enableUnroll) {
+  Value noneValue;
+  auto direction = op->getDirection();
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  // First output: all sequences.
+  if (isNoneValue(op->getY()))
+    outputs.emplace_back(noneValue);
+  else {
+    if (enableUnroll) {
+      if (direction == FORWARD) {
+        outputs.emplace_back(create.onnx.concat(
+            op->getY().getType(), ValueRange(state.forwardAllH), 0));
+      } else if (direction == REVERSE) {
+        outputs.emplace_back(create.onnx.concat(
+            op->getY().getType(), ValueRange(state.reverseAllH), 0));
+      } else {
+        auto outputShape = op->getY().getType().cast<ShapedType>().getShape();
+        RankedTensorType singleDirectionType = RankedTensorType::get(
+            {outputShape[0], 1, outputShape[2], outputShape[3]},
+            op->getY().getType().cast<ShapedType>().getElementType());
+        outputs.emplace_back(create.onnx.concat(op->getY().getType(),
+            {create.onnx.concat(
+                 singleDirectionType, ValueRange(state.forwardAllH), 0),
+                create.onnx.concat(
+                    singleDirectionType, ValueRange(state.reverseAllH), 0)},
+            1));
+      }
+    } else {
+      if (direction == FORWARD) {
+        outputs.emplace_back(state.allHForward);
+      } else if (direction == REVERSE) {
+        outputs.emplace_back(state.allHReverse);
+      } else {
+        outputs.emplace_back(create.onnx.concat(
+            op->getY().getType(), {state.allHForward, state.allHReverse}, 1));
+      }
+    }
+  }
+  // Second output: hidden.
+  if (isNoneValue(op->getYH()))
+    outputs.emplace_back(noneValue);
+  else {
+    stateToOutputForHiddenOrCell(
+        rewriter, loc, state.forwardHt, state.reverseHt, direction, state.ht);
+    outputs.emplace_back(state.ht);
+  }
+  // Third output: cell.
+  if (isNoneValue(op->getYC()))
+    outputs.emplace_back(noneValue);
+  else {
+    stateToOutputForHiddenOrCell(
+        rewriter, loc, state.forwardCt, state.reverseCt, direction, state.ct);
+    outputs.emplace_back(state.ct);
+  }
+}
+
+template <>
+void calculateStateWithUnroll<ONNXLSTMOp, LstmState, LstmActivationPack,
+    LstmWeightPack, LstmBiasPack>(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, llvm::StringRef direction, int64_t sequenceDimSize,
+    Value X, LstmState &state, LstmActivationPack activationForward,
+    LstmActivationPack activationReverse, LstmWeightPack weightForward,
+    LstmWeightPack weightReverse, LstmBiasPack biasForward,
+    LstmBiasPack biasReverse, Value sequenceLens, Value initialH) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    for (int64_t i = 0; i < sequenceDimSize; i++) {
+      mlir::Value directionIV = create.onnx.constantInt64({0});
+      mlir::Value sequenceIV = create.onnx.constantInt64({i});
+      // Get a slice of X at the current timestep.
+      mlir::Value Xt = emitXSliceAt(rewriter, loc, X, sequenceIV);
+      // Emit calculation for one RNN step.
+      calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+          LstmBiasPack>(rewriter, loc, Xt, state, activationForward,
+          weightForward, biasForward, sequenceIV, directionIV, sequenceLens,
+          initialH, /*enableUnroll=*/true, /*isForward=*/true);
+    }
+  }
+
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    for (int64_t i = 0; i < sequenceDimSize; i++) {
+      mlir::Value directionIV =
+          create.onnx.constantInt64({(direction == REVERSE) ? 0 : 1});
+      mlir::Value reverseSequenceIV =
+          create.onnx.constantInt64({sequenceDimSize - i - 1});
+      // Get a slice of X at the current timestep.
+      mlir::Value Xt = emitXSliceAt(rewriter, loc, X, reverseSequenceIV);
+      // Emit calculation for one RNN step.
+      calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+          LstmBiasPack>(rewriter, loc, Xt, state, activationReverse,
+          weightReverse, biasReverse, reverseSequenceIV, directionIV,
+          sequenceLens, initialH, /*enableUnroll=*/true, /*isForward=*/false);
+    }
+  }
+}
+
+template <>
+void calculateStateWithLoop<ONNXLSTMOp, LstmState, LstmActivationPack,
+    LstmWeightPack, LstmBiasPack>(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, llvm::StringRef direction, int64_t sequenceDimSize,
+    Value X, LstmState &state, LstmActivationPack activationForward,
+    LstmActivationPack activationReverse, LstmWeightPack weightForward,
+    LstmWeightPack weightReverse, LstmBiasPack biasForward,
+    LstmBiasPack biasReverse, Value sequenceLens, Value initialH) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    mlir::Value directionIV = create.onnx.constantInt64({0});
+    mlir::Value sequenceIV = create.onnx.constantInt64({0});
+    SmallVector<Value> operands = {
+        sequenceIV, state.allHForward, state.forwardHt, state.forwardCt};
+    SmallVector<Type> returnedTypes = {sequenceIV.getType(),
+        state.allHForward.getType(), state.forwardHt.getType(),
+        state.forwardCt.getType()};
+    SmallVector<Location> locations(returnedTypes.size(), loc);
+    ::mhlo::WhileOp whileLoopOp =
+        rewriter.create<::mhlo::WhileOp>(loc, returnedTypes, operands);
+    Region &condRegion = whileLoopOp.getCond();
+    Region &bodyRegion = whileLoopOp.getBody();
+    Block &condBlock = condRegion.emplaceBlock();
+    Block &bodyBlock = bodyRegion.emplaceBlock();
+    condBlock.addArguments(returnedTypes, locations);
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&condBlock);
+      BlockArgument lhs = condBlock.getArgument(0);
+      mlir::Value rhs = create.onnx.constantInt64({sequenceDimSize});
+      Value compareResult = rewriter.create<::mhlo::CompareOp>(
+          loc, lhs, rhs, ::mhlo::ComparisonDirection::LT);
+      compareResult = rewriter.create<::mhlo::ReshapeOp>(
+          loc, RankedTensorType::get({}, rewriter.getI1Type()), compareResult);
+      rewriter.create<::mhlo::ReturnOp>(loc, compareResult);
+    }
+    bodyBlock.addArguments(returnedTypes, locations);
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&bodyBlock);
+      BlockArgument seqIV = bodyBlock.getArgument(0);
+      BlockArgument allH = bodyBlock.getArgument(1);
+      BlockArgument ht = bodyBlock.getArgument(2);
+      BlockArgument ct = bodyBlock.getArgument(3);
+      state.allHForward = allH;
+      state.forwardHt = ht;
+      state.forwardCt = ct;
+      mlir::Value Xt = emitXSliceAt(rewriter, loc, X, seqIV);
+      calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+          LstmBiasPack>(rewriter, loc, Xt, state, activationForward,
+          weightForward, biasForward, seqIV, directionIV, sequenceLens,
+          initialH, /*enableUnroll=*/false, /*isForward=*/true);
+      mlir::Value one = create.onnx.constantInt64({1});
+      Value newSeqIV = create.onnx.add(seqIV, one);
+      rewriter.create<::mhlo::ReturnOp>(loc,
+          ValueRange(
+              {newSeqIV, state.allHForward, state.forwardHt, state.forwardCt}));
+    }
+    state.allHForward = whileLoopOp.getResults()[1];
+    state.forwardHt = whileLoopOp.getResults()[2];
+    state.forwardCt = whileLoopOp.getResults()[3];
+  }
+
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    mlir::Value directionIV =
+        create.onnx.constantInt64({(direction == REVERSE) ? 0 : 1});
+    mlir::Value reverseSequenceIV =
+        create.onnx.constantInt64({sequenceDimSize - 1});
+
+    SmallVector<Value> operands = {
+        reverseSequenceIV, state.allHReverse, state.reverseHt, state.reverseCt};
+    SmallVector<Type> returnedTypes = {reverseSequenceIV.getType(),
+        state.allHReverse.getType(), state.reverseHt.getType(),
+        state.reverseCt.getType()};
+    SmallVector<Location> locations(returnedTypes.size(), loc);
+    ::mhlo::WhileOp whileLoopOp =
+        rewriter.create<::mhlo::WhileOp>(loc, returnedTypes, operands);
+    Region &condRegion = whileLoopOp.getCond();
+    Region &bodyRegion = whileLoopOp.getBody();
+    Block &condBlock = condRegion.emplaceBlock();
+    Block &bodyBlock = bodyRegion.emplaceBlock();
+    condBlock.addArguments(returnedTypes, locations);
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&condBlock);
+      BlockArgument lhs = condBlock.getArgument(0);
+      mlir::Value rhs = create.onnx.constantInt64({0});
+      Value compareResult = rewriter.create<::mhlo::CompareOp>(
+          loc, lhs, rhs, ::mhlo::ComparisonDirection::GE);
+      compareResult = rewriter.create<::mhlo::ReshapeOp>(
+          loc, RankedTensorType::get({}, rewriter.getI1Type()), compareResult);
+      rewriter.create<::mhlo::ReturnOp>(loc, compareResult);
+    }
+    bodyBlock.addArguments(returnedTypes, locations);
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&bodyBlock);
+      BlockArgument revseqIV = bodyBlock.getArgument(0);
+      BlockArgument allH = bodyBlock.getArgument(1);
+      BlockArgument ht = bodyBlock.getArgument(2);
+      BlockArgument ct = bodyBlock.getArgument(3);
+      state.allHReverse = allH;
+      state.reverseHt = ht;
+      state.reverseCt = ct;
+      mlir::Value Xt = emitXSliceAt(rewriter, loc, X, revseqIV);
+      calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+          LstmBiasPack>(rewriter, loc, Xt, state, activationReverse,
+          weightReverse, biasReverse, revseqIV, directionIV, sequenceLens,
+          initialH, /*enableUnroll=*/false, /*isForward=*/false);
+      mlir::Value one = create.onnx.constantInt64({1});
+      Value newrevseqIV = create.onnx.sub(revseqIV, one);
+      rewriter.create<::mhlo::ReturnOp>(
+          loc, ValueRange({newrevseqIV, state.allHReverse, state.reverseHt,
+                   state.reverseCt}));
+    }
+    state.allHReverse = whileLoopOp.getResults()[1];
+    state.reverseHt = whileLoopOp.getResults()[2];
+    state.reverseCt = whileLoopOp.getResults()[3];
+  }
+}
+
+} // namespace mhlo
+
+void populateLoweringONNXLSTMOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx, bool enableUnroll) {
+  patterns.insert<onnx_mlir::mhlo::ONNXRNNOpLowering<ONNXLSTMOp,
+      onnx_mlir::mhlo::LstmState, onnx_mlir::mhlo::LstmActivationPack,
+      onnx_mlir::mhlo::LstmWeightPack, onnx_mlir::mhlo::LstmBiasPack>>(
+      ctx, enableUnroll);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/RNN/RNNBase.cpp b/src/Conversion/ONNXToMhlo/RNN/RNNBase.cpp
new file mode 100644
index 00000000..986a0435
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/RNN/RNNBase.cpp
@@ -0,0 +1,228 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- RNNBase.cpp - Lowering RNN Ops -----------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file defines base functions for lowering the ONNX RNN Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace mhlo {
+
+// Get a dimension of the tensor's shape.
+int64_t dimAt(Value val, int index) {
+  return val.getType().cast<ShapedType>().getShape()[index];
+}
+
+/// Allocate the all hidden output.
+/// Shape :: [seq_length, num_directions, batch_size, hidden_size]
+Value allocAllHidden(
+    ConversionPatternRewriter &rewriter, Location loc, Value X, Value R) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  RankedTensorType zeroType =
+      RankedTensorType::get({dimAt(X, 0), 1, dimAt(X, 1), dimAt(R, 2)},
+          X.getType().cast<ShapedType>().getElementType());
+  DenseElementsAttr zeroAttr = DenseElementsAttr::get(zeroType, 0.0f);
+  return create.onnx.constant(zeroAttr);
+}
+
+/// Allocate the hidden or cell output.
+mlir::Value allocHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value W, mlir::Value R) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  RankedTensorType zeroType = RankedTensorType::get(
+      {/*num_directions=*/dimAt(W, 0), /*batch_size=*/dimAt(X, 1),
+          /*hidden_size=*/dimAt(R, 2)},
+      X.getType().cast<ShapedType>().getElementType());
+  DenseElementsAttr zeroAttr = DenseElementsAttr::get(zeroType, 0.0f);
+  return create.onnx.constant(zeroAttr);
+}
+
+/// Allocate the intermediate hidden or cell states.
+/// Shape :: [batch_size, hidden_size]
+Value allocIntermediateState(
+    ConversionPatternRewriter &rewriter, Location loc, Value X, Value R) {
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+  RankedTensorType zeroType =
+      RankedTensorType::get({/*batch_size=*/dimAt(X, 1),
+                                /*hidden_size=*/dimAt(R, 2)},
+          X.getType().cast<ShapedType>().getElementType());
+  DenseElementsAttr zeroAttr = DenseElementsAttr::get(zeroType, 0.0f);
+  return create.onnx.constant(zeroAttr);
+}
+
+/// Initialize the intermediate hidden and cell states.
+/// forward(reverse)Ht, forward(reverse)Ct
+void initializeIntermediateStates(ConversionPatternRewriter &rewriter,
+    Location loc, Value &forwardHt, Value &reverseHt, Value &forwardCt,
+    Value &reverseCt, Value initialH, Value initialC, Type elementType,
+    StringRef direction, bool onlyHidden) {
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+
+  Value zeroIndex = create.mhlo.constantI64(0);
+  Value zeroIndex1D = create.onnx.constantInt64({0});
+  Value oneIndex = create.mhlo.constantI64(1);
+
+  Value boundVal = (direction == FORWARD || direction == BIDIRECTIONAL)
+                       ? forwardHt
+                       : reverseHt;
+  auto valShape = boundVal.getType().cast<ShapedType>().getShape();
+  SmallVector<int64_t> sliceSizes = {1, valShape[0], valShape[1]};
+  SmallVector<Value> firstStartIndices = {zeroIndex, zeroIndex, zeroIndex};
+  SmallVector<Value> secondStartIndices = {oneIndex, zeroIndex, zeroIndex};
+
+  RankedTensorType valType = boundVal.getType().cast<RankedTensorType>();
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    if (!isNoneValue(initialH)) {
+      forwardHt =
+          create.mhlo.dynamic_slice(initialH, firstStartIndices, sliceSizes);
+      forwardHt = create.onnx.squeeze(valType, forwardHt, zeroIndex1D);
+    }
+    if (!onlyHidden && !isNoneValue(initialC)) {
+      forwardCt =
+          create.mhlo.dynamic_slice(initialC, firstStartIndices, sliceSizes);
+      forwardCt = create.onnx.squeeze(valType, forwardCt, zeroIndex1D);
+    }
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    if (!isNoneValue(initialH)) {
+      if (direction == REVERSE) {
+        reverseHt =
+            create.mhlo.dynamic_slice(initialH, firstStartIndices, sliceSizes);
+        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex1D);
+      } else {
+        reverseHt =
+            create.mhlo.dynamic_slice(initialH, secondStartIndices, sliceSizes);
+        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex1D);
+      }
+    }
+    if (!onlyHidden and !isNoneValue(initialC)) {
+      if (direction == REVERSE) {
+        reverseCt =
+            create.mhlo.dynamic_slice(initialC, firstStartIndices, sliceSizes);
+        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex1D);
+      } else {
+        reverseCt =
+            create.mhlo.dynamic_slice(initialC, secondStartIndices, sliceSizes);
+        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex1D);
+      }
+    }
+  }
+}
+
+/// Store a state into the output of the RNN op.
+/// The input state is 2D and the output state is 3D with '1' or '2' is
+/// pretended, depending on 'direction'.
+void stateToOutputForHiddenOrCell(ConversionPatternRewriter &rewriter,
+    Location loc, Value forwardVal, Value reverseVal, StringRef direction,
+    Value &output) {
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+  if (direction == FORWARD || direction == REVERSE) {
+    Value val = (direction == FORWARD) ? forwardVal : reverseVal;
+    output = val;
+  } else { // BIDIRECTIONAL
+    SmallVector<int64_t, 4> bForwardValShape(
+        forwardVal.getType().cast<ShapedType>().getShape());
+    SmallVector<int64_t, 4> bValShape(
+        forwardVal.getType().cast<ShapedType>().getShape());
+    SmallVector<int64_t, 4> bReverseValShape(
+        reverseVal.getType().cast<ShapedType>().getShape());
+    bForwardValShape.insert(bForwardValShape.begin(), 1);
+    bReverseValShape.insert(bReverseValShape.begin(), 1);
+    bValShape.insert(bValShape.begin(), 2);
+    Type valElementType =
+        forwardVal.getType().cast<ShapedType>().getElementType();
+    Value zero = create.onnx.constantInt64({0});
+    Value bForwardVal = create.onnx.unsqueeze(
+        RankedTensorType::get(bForwardValShape, valElementType), forwardVal,
+        zero);
+    Value bReverseVal = create.onnx.unsqueeze(
+        RankedTensorType::get(bReverseValShape, valElementType), reverseVal,
+        zero);
+    output =
+        create.onnx.concat(RankedTensorType::get(bValShape, valElementType),
+            {bForwardVal, bReverseVal}, 0);
+  }
+}
+
+// Apply an activation function on a given scalar operand.
+Value applyActivation(OpBuilder &rewriter, Location loc,
+    RNNActivation activation, Value operand) {
+  Value res;
+
+  std::vector<mlir::NamedAttribute> attributes;
+  if (activation.alpha) {
+    attributes.emplace_back(
+        rewriter.getNamedAttr("alpha", activation.alpha.value()));
+  }
+  if (activation.beta) {
+    attributes.emplace_back(
+        rewriter.getNamedAttr("beta", activation.beta.value()));
+  }
+  Type resType = operand.getType();
+
+  // Change equality to be case insensitive.
+  if (activation.name.equals_insensitive("relu"))
+    res = rewriter.create<ONNXReluOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("tanh"))
+    res = rewriter.create<ONNXTanhOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("sigmoid"))
+    res = rewriter.create<ONNXSigmoidOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("affine"))
+    llvm_unreachable("Unsupported activation");
+  else if (activation.name.equals_insensitive("leakyrelu"))
+    res = rewriter.create<ONNXLeakyReluOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("thresholdedrelu"))
+    res = rewriter.create<ONNXThresholdedReluOp>(
+        loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("scaledtanh"))
+    llvm_unreachable("Unsupported activation");
+  else if (activation.name.equals_insensitive("hardsigmoid"))
+    res = rewriter.create<ONNXHardSigmoidOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("elu"))
+    res = rewriter.create<ONNXEluOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("softsign"))
+    res = rewriter.create<ONNXSoftsignOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("softplus"))
+    res = rewriter.create<ONNXSoftplusOp>(loc, resType, operand);
+  else
+    llvm_unreachable("Unsupported activation");
+
+  return res;
+}
+
+/// Create a copy of a slice of X at a specific timestep.
+Value emitXSliceAt(ConversionPatternRewriter &rewriter, Location loc, Value X,
+    Value timestepIV) {
+  MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+  int64_t batchSize = dimAt(X, 1);
+  int64_t inputSize = dimAt(X, 2);
+  Type elementType = X.getType().cast<ShapedType>().getElementType();
+  RankedTensorType squeezedXType =
+      RankedTensorType::get({batchSize, inputSize}, elementType);
+  SmallVector<int64_t> sliceSizes = {1, batchSize, inputSize};
+  Value zeroIndex = create.mhlo.constantI64(0);
+  Value timestepIV0D = create.mhlo.reshape(
+      RankedTensorType::get({}, rewriter.getI64Type()), timestepIV);
+  SmallVector<Value> startIndices = {timestepIV0D, zeroIndex, zeroIndex};
+  Value sliceX = create.mhlo.dynamic_slice(X, startIndices, sliceSizes);
+  sliceX = create.onnx.squeeze(
+      squeezedXType, sliceX, create.onnx.constantInt64({0}));
+  return sliceX;
+}
+
+} // namespace mhlo
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp b/src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp
new file mode 100644
index 00000000..c3af7bce
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/RNN/RNNBase.hpp
@@ -0,0 +1,204 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- RNNBase.hpp - Lowering RNN Ops -----------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file defines base functions for lowering the ONNX RNN Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+
+static constexpr llvm::StringRef FORWARD = "forward";
+static constexpr llvm::StringRef REVERSE = "reverse";
+static constexpr llvm::StringRef BIDIRECTIONAL = "bidirectional";
+
+namespace onnx_mlir {
+
+namespace mhlo {
+
+struct RNNActivation {
+  llvm::StringRef name;
+  std::optional<mlir::FloatAttr> alpha;
+  std::optional<mlir::FloatAttr> beta;
+};
+
+/// Get a dimension of the tensor's shape.
+int64_t dimAt(mlir::Value val, int index);
+
+/// Allocate the all hidden output.
+mlir::Value allocAllHidden(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value R);
+
+/// Allocate the hidden or cell output.
+mlir::Value allocHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value W, mlir::Value R);
+
+/// Allocate the intermediate hidden or cell state.
+mlir::Value allocIntermediateState(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value R);
+
+/// Initialize the intermediate hidden and cell states.
+void initializeIntermediateStates(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value &forwardHt, mlir::Value &reverseHt,
+    mlir::Value &forwardCt, mlir::Value &reverseCt, mlir::Value initialH,
+    mlir::Value initialC, mlir::Type elementType, llvm::StringRef direction,
+    bool onlyHidden);
+
+/// Store a state into the output of the RNN op.
+/// The input state is 2D and the output state is 3D with '1' or '2' is
+/// pretended, depending on 'direction'.
+void stateToOutputForHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value forwardVal, mlir::Value reverseVal,
+    llvm::StringRef direction, mlir::Value &output);
+
+/// Apply an activation function on a given operand.
+mlir::Value applyActivation(mlir::OpBuilder &rewriter, mlir::Location loc,
+    RNNActivation activation, mlir::Value operand);
+
+/// Get a slice of X at a specific timestep.
+mlir::Value emitXSliceAt(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value timestep);
+
+// Override the following methods when lowering an RNN operation:
+// - hasAllNoneOutput
+// - getActivationPack
+// - getWeightPack
+// - getBiasPack
+// - allocAndInitializeStates
+// - calculateState
+// - stateToOutput
+
+// Check whether all outputs have NoneType or not.
+template <typename RNNOp>
+bool hasAllNoneOutput(RNNOp *op);
+
+// Obtain activations functions for a specific operation.
+template <typename RNNOp, typename A>
+std::tuple<A, A> getActivationPack(RNNOp *op);
+
+/// Obtain weight tensors in 2D for each gate.
+/// In ONNX, weights for gates and directions are combined in a single tensor.
+/// This function splits them into 2D tensors.
+template <typename RNNOp, typename W>
+std::tuple<W, W> getWeightPack(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc, RNNOp *op);
+
+/// Obtain biases in 1D for each gate.
+/// In ONNX, biases for gates and directions are combined in a single tensor.
+/// This function splits them into 1D tensors.
+template <typename RNNOp, typename B>
+std::tuple<B, B> getBiasPack(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc, RNNOp *op);
+
+// Allocate memory for RNN states and initialize them.
+template <typename RNNOp, typename S>
+S allocAndInitializeStates(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, RNNOp *op, typename RNNOp::Adaptor operandAdaptor,
+    bool enableUnroll);
+
+// Calculate new states from the current input and states.
+template <typename S, typename A, typename W, typename B>
+void calculateState(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value Xt, S &state, A activationSet, W weight,
+    B bias, mlir::Value sequenceIV, mlir::Value directionIV,
+    mlir::Value sequenceLens, mlir::Value initialH, bool enableUnroll,
+    bool isForward);
+
+// Write states to the RNN's outputs.
+template <typename RNNOp, typename S>
+void stateToOutput(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, RNNOp *op, S state, std::vector<mlir::Value> &outputs,
+    bool enableUnroll);
+
+// Calculate all states using unroll
+template <typename RNNOp, typename S, typename A, typename W, typename B>
+void calculateStateWithUnroll(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, llvm::StringRef direction, int64_t sequenceDimSize,
+    Value X, S &state, A activationForward, A activationReverse,
+    W weightForward, W weightReverse, B biasForward, B biasReverse,
+    Value sequenceLens, Value initialH);
+
+// Calculate all states using loop
+template <typename RNNOp, typename S, typename A, typename W, typename B>
+void calculateStateWithLoop(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, llvm::StringRef direction, int64_t sequenceDimSize,
+    Value X, S &state, A activationForward, A activationReverse,
+    W weightForward, W weightReverse, B biasForward, B biasReverse,
+    Value sequenceLens, Value initialH);
+
+// A common template for lowering an RNN operation.
+template <typename RNNOp, typename S, typename A, typename W, typename B>
+struct ONNXRNNOpLowering : public mlir::OpConversionPattern<RNNOp> {
+  using OpAdaptor = typename RNNOp::Adaptor;
+  bool enableUnroll;
+
+  ONNXRNNOpLowering(mlir::MLIRContext *ctx, bool enableUnroll)
+      : mlir::OpConversionPattern<RNNOp>(ctx) {
+    this->enableUnroll = enableUnroll;
+  }
+
+  mlir::LogicalResult matchAndRewrite(RNNOp rnnOp, OpAdaptor adaptor,
+      mlir::ConversionPatternRewriter &rewriter) const final {
+    mlir::Operation *op = rnnOp.getOperation();
+    mlir::Location loc = ONNXLoc<RNNOp>(op);
+    mlir::Value X = adaptor.getX();
+    mlir::Value sequenceLens = adaptor.getSequenceLens();
+    mlir::Value initialH = adaptor.getInitialH();
+
+    if (hasAllNoneOutput<RNNOp>(&rnnOp)) {
+      rewriter.eraseOp(op);
+      return mlir::success();
+    }
+
+    // Initialize output states.
+    S state = allocAndInitializeStates<RNNOp, S>(
+        rewriter, loc, &rnnOp, adaptor, this->enableUnroll);
+
+    // Activation functions.
+    A activationForward, activationReverse;
+    std::tie(activationForward, activationReverse) =
+        getActivationPack<RNNOp, A>(&rnnOp);
+
+    // Prepare weights.
+    W weightForward, weightReverse;
+    std::tie(weightForward, weightReverse) =
+        getWeightPack<RNNOp, W>(rewriter, loc, &rnnOp);
+
+    // Prepare biases.
+    B biasForward, biasReverse;
+    std::tie(biasForward, biasReverse) =
+        getBiasPack<RNNOp, B>(rewriter, loc, &rnnOp);
+
+    int64_t sequenceDimSize = dimAt(rnnOp.getX(), 0);
+    auto direction = rnnOp.getDirection();
+
+    MultiDialectBuilder<MhloBuilder, OnnxBuilder> create(rewriter, loc);
+    if (this->enableUnroll)
+      calculateStateWithUnroll<RNNOp, S, A, W, B>(rewriter, loc, direction,
+          sequenceDimSize, X, state, activationForward, activationReverse,
+          weightForward, weightReverse, biasForward, biasReverse, sequenceLens,
+          initialH);
+    else
+      calculateStateWithLoop<RNNOp, S, A, W, B>(rewriter, loc, direction,
+          sequenceDimSize, X, state, activationForward, activationReverse,
+          weightForward, weightReverse, biasForward, biasReverse, sequenceLens,
+          initialH);
+    std::vector<mlir::Value> outputs;
+    stateToOutput<RNNOp, S>(
+        rewriter, loc, &rnnOp, state, outputs, this->enableUnroll);
+    rewriter.replaceOp(op, outputs);
+    return mlir::success();
+  }
+};
+
+} // namespace mhlo
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/DepthToSpace.cpp b/src/Conversion/ONNXToMhlo/Tensor/DepthToSpace.cpp
new file mode 100644
index 00000000..8bc04b1e
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/DepthToSpace.cpp
@@ -0,0 +1,98 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===------------ DepthToSpace.cpp - Lowering DepthToSpace Op -------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX DepthToSpace Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXDepthToSpaceOpLoweringToMhlo
+    : public OpConversionPattern<ONNXDepthToSpaceOp> {
+  ONNXDepthToSpaceOpLoweringToMhlo(MLIRContext *ctx)
+      : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXDepthToSpaceOp depthToSpaceOp,
+      ONNXDepthToSpaceOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = depthToSpaceOp.getOperation();
+    Location loc = ONNXLoc<ONNXDepthToSpaceOp>(op);
+    ValueRange operands = adaptor.getOperands();
+    Value input = adaptor.getInput();
+
+    MultiDialectBuilder<IndexExprBuilderForMhlo, OnnxToMhloBuilder> create(
+        rewriter, loc);
+    ONNXDepthToSpaceOpShapeHelper shapeHelper(op, operands, &create.mhloIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    int64_t bs = depthToSpaceOp.getBlocksize();
+    StringRef mode = depthToSpaceOp.getMode();
+    assert(create.mhloIE.getShapedTypeRank(input) == 4 &&
+           "Input tensor should have rank equal to 4");
+
+    // Compute the new dimensions.
+
+    DimIndexExpr B(create.mhloIE.getShapeAsDim(input, 0));
+    DimIndexExpr C(create.mhloIE.getShapeAsDim(input, 1));
+    DimIndexExpr H(create.mhloIE.getShapeAsDim(input, 2));
+    DimIndexExpr W(create.mhloIE.getShapeAsDim(input, 3));
+    DimIndexExpr newC = C.floorDiv(bs * bs);
+    DimIndexExpr newH = H * bs;
+    DimIndexExpr newW = W * bs;
+
+    // Compute the output dimension of the first reshape operation, and the
+    // permutation array for the transpose operation.
+    LiteralIndexExpr bsLit(bs);
+    SmallVector<DimIndexExpr, 6> outputDims1;
+    SmallVector<int64_t, 6> perm;
+    if (mode == "DCR") {
+      outputDims1 = {B, bsLit, bsLit, newC, H, W};
+      perm = {0, 3, 4, 1, 5, 2};
+    } else {
+      assert(mode == "CRD" && "Unexpected mode");
+      outputDims1 = {B, newC, bsLit, bsLit, H, W};
+      perm = {0, 1, 4, 2, 5, 3};
+    }
+
+    // Reshape input tensor to shape:
+    //   [B, bs, bs, C/(bs*bs), H, W] when mode=DCR
+    //   [B, C/(bs*bs), bs, bs, H, W] when mode=CRD
+    Value reshapeRes1 = create.mhloOnnx.reshape(input, outputDims1);
+
+    // Transpose the reshape result into shape [B, C/(bs*bs), H, bs, W, bs].
+    SmallVector<DimIndexExpr> outputDims2({B, newC, H, bsLit, W, bsLit});
+    Value transposeRes =
+        create.mhloOnnx.transpose(reshapeRes1, perm, outputDims2);
+
+    // Reshape the transpose result into shape [B, C/(bs*bs), H*bs, W*bs].
+    SmallVector<DimIndexExpr> outputDims3({B, newC, newH, newW});
+    Value reshapeRes2 = create.mhloOnnx.reshape(transposeRes, outputDims3);
+
+    rewriter.replaceOp(op, reshapeRes2);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXDepthToSpaceOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXDepthToSpaceOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
index 0089fee1..d0dc81f9 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
@@ -69,11 +69,9 @@ struct ONNXExpandOpLoweringToMhlo : public ConversionPattern {
       RankedTensorType onesType = RankedTensorType::get(onesShape, elementType);
       broadcastedOnes = rewriter.create<mhlo::DynamicBroadcastInDimOp>(
           loc, onesType, ones, shape, rewriter.getI64TensorAttr({}));
-    } else if (ONNXConstantOp shapeOp =
-                   dyn_cast_or_null<ONNXConstantOp>(shapeDefOp)) {
+    } else if (mlir::ElementsAttr constShape =
+                   getElementAttributeFromConstValue(shape)) {
       llvm::SmallVector<int64_t, 4> shapeValues;
-      mlir::ElementsAttr constShape =
-          shapeOp.getValueAttr().cast<ElementsAttr>();
       for (mlir::IntegerAttr element : constShape.getValues<IntegerAttr>())
         shapeValues.push_back(element.getInt());
       RankedTensorType broadcastedType =
@@ -84,7 +82,7 @@ struct ONNXExpandOpLoweringToMhlo : public ConversionPattern {
       assert(
           false &&
           "Shape argument of Expand is the output of an unexpected operation. "
-          "Supported operations are: onnx.Constant and onnx.Shape");
+          "Supported operations are: Constant and onnx.Shape");
     }
     llvm::SmallVector<Value, 4> newOperands = {input, broadcastedOnes};
     llvm::SmallVector<Value, 4> broadcastedOperands = getBroadcastedOperands(
diff --git a/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp b/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp
new file mode 100644
index 00000000..09d7a6c2
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp
@@ -0,0 +1,139 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===-------- GatherElements.cpp - Lowering GatherElements Op -------------===//
+//
+// Copyright 2020-2022 The IBM Research Authors.
+//
+// =============================================================================
+//
+// This file lowers the ONNX GatherElements Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXGatherElementsOpLoweringToMhlo : public ConversionPattern {
+  ONNXGatherElementsOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(
+            mlir::ONNXGatherElementsOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    ONNXGatherElementsOpAdaptor operandAdaptor(operands);
+    ONNXGatherElementsOp gatherOp = cast<ONNXGatherElementsOp>(op);
+    Location loc = op->getLoc();
+
+    IndexExprBuilderForMhlo createIE(rewriter, loc);
+    ONNXGatherElementsOpShapeHelper shapeHelper(op, operands, &createIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+
+    // Operands and attributes.
+    Value data = operandAdaptor.getData();
+    Value indices = operandAdaptor.getIndices();
+    int64_t axisLit = gatherOp.getAxis();
+
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    int64_t rank = inputType.getRank(); // indices has the same rank
+    ShapedType indicesType = indices.getType().cast<ShapedType>();
+    Type indexElemType = indicesType.getElementType();
+    // Negative value means counting dimensions from the back.
+    axisLit = axisLit < 0 ? axisLit + rank : axisLit;
+
+    // make sure all index values >= 0
+    Value zero = getShapedZero(loc, rewriter, indices);
+    Value inputShape = rewriter.create<shape::ShapeOfOp>(loc, data);
+    Value indicesShape = rewriter.create<shape::ShapeOfOp>(loc, indices);
+    Value axisDimSize =
+        rewriter.create<shape::GetExtentOp>(loc, inputShape, axisLit);
+    axisDimSize =
+        rewriter.create<arith::IndexCastOp>(loc, indexElemType, axisDimSize);
+    axisDimSize = rewriter.create<tensor::FromElementsOp>(loc, axisDimSize);
+    axisDimSize = rewriter.create<mhlo::ReshapeOp>(loc,
+        RankedTensorType::get(SmallVector<int64_t>{}, indexElemType),
+        axisDimSize);
+    Value broadcastedAxisDimSize =
+        rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc, indicesType,
+            axisDimSize, indicesShape, rewriter.getI64TensorAttr({}));
+    Value isNegative = rewriter.create<mhlo::CompareOp>(
+        loc, indices, zero, mhlo::ComparisonDirection::LT);
+    Value positiveIndices = rewriter.create<mhlo::AddOp>(
+        loc, indicesType, indices, broadcastedAxisDimSize);
+    indices = rewriter.create<mhlo::SelectOp>(
+        loc, indicesType, isNegative, positiveIndices, indices);
+
+    // start indices
+    Value toConcatIndexShape;
+    SmallVector<Value> toConcatIndexShapeValueVec;
+    for (size_t i = 0; i < rank; i++) {
+      toConcatIndexShapeValueVec.push_back(
+          rewriter.create<shape::GetExtentOp>(loc, indicesShape, i));
+    }
+    toConcatIndexShapeValueVec.push_back(
+        rewriter.create<arith::ConstantIndexOp>(loc, 1));
+    toConcatIndexShape = rewriter.create<tensor::FromElementsOp>(
+        loc, toConcatIndexShapeValueVec);
+
+    ArrayRef<int64_t> indicesShapeVec = indicesType.getShape();
+    SmallVector<int64_t> toConcatIndexShapeVec(
+        indicesShapeVec.begin(), indicesShapeVec.end());
+    toConcatIndexShapeVec.push_back(1);
+    RankedTensorType toConcatIndexType =
+        RankedTensorType::get(toConcatIndexShapeVec, indexElemType);
+
+    SmallVector<Value> toConcat;
+    for (int64_t i = 0; i < inputType.getRank(); ++i) {
+      if (i == axisLit) {
+        toConcat.push_back(rewriter.create<mhlo::DynamicReshapeOp>(
+            loc, toConcatIndexType, indices, toConcatIndexShape));
+      } else {
+        toConcat.push_back(
+            rewriter.create<mhlo::DynamicIotaOp>(loc, toConcatIndexType,
+                toConcatIndexShape, rewriter.getI64IntegerAttr(i)));
+      }
+    }
+    auto gatherIndicies = rewriter.create<mhlo::ConcatenateOp>(
+        loc, toConcat, static_cast<uint64_t>(inputType.getRank()));
+
+    // dimsAttr
+    SmallVector<int64_t> collapsedDims;
+    SmallVector<int64_t> startIndexMap;
+    for (int64_t i = 0; i < rank; i++) {
+      collapsedDims.push_back(i);
+      startIndexMap.push_back(i);
+    }
+    auto dimsAttr = mhlo::GatherDimensionNumbersAttr::get(rewriter.getContext(),
+        /*offsetDims=*/{},
+        /*collapsedSliceDims=*/collapsedDims,
+        /*startIndexMap=*/startIndexMap,
+        /*indexVecDim=*/rank);
+    SmallVector<int64_t> sliceSizes(inputType.getRank(), 1);
+
+    Value gatherValue = rewriter.create<mhlo::GatherOp>(loc, outputType, data,
+        gatherIndicies, dimsAttr, rewriter.getI64TensorAttr(sliceSizes));
+    rewriter.replaceOp(op, gatherValue);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXGatherElementsOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXGatherElementsOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp b/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp
new file mode 100644
index 00000000..231d4daa
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp
@@ -0,0 +1,131 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===---------------- OneHot.cpp - Lowering OneHot Op -------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX OneHot Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+#include <numeric>
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXOneHotOpLoweringToMhlo : public OpConversionPattern<ONNXOneHotOp> {
+  ONNXOneHotOpLoweringToMhlo(MLIRContext *ctx) : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXOneHotOp onehotOp,
+      ONNXOneHotOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = onehotOp.getOperation();
+    Location loc = ONNXLoc<ONNXOneHotOp>(op);
+    ValueRange operands = adaptor.getOperands();
+    Value indices = adaptor.getIndices();
+    Value depthValue = adaptor.getDepth();
+    Value values = adaptor.getValues();
+    Type outputType = *op->result_type_begin();
+
+    IndexExprBuilderForMhlo createIE(rewriter, loc);
+    ONNXOneHotOpShapeHelper shapeHelper(op, operands, &createIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+    int64_t axis = shapeHelper.axis;
+
+    RankedTensorType indicesType =
+        indices.getType().dyn_cast<RankedTensorType>();
+    if (!indicesType || !indicesType.hasStaticShape())
+      return failure();
+    ArrayRef<int64_t> indicesShape = indicesType.getShape();
+    Type indicesElementType = indicesType.getElementType();
+
+    DenseIntElementsAttr depthAttr;
+    if (!matchPattern(depthValue, m_Constant(&depthAttr))) {
+      return failure();
+    }
+
+    int64_t depth = depthAttr.getValues<APInt>()[0].getSExtValue();
+
+    llvm::SmallVector<int64_t, 4> broadcastDims(indicesShape.size());
+    std::iota(broadcastDims.begin(), broadcastDims.begin() + axis, 0);
+    std::iota(broadcastDims.begin() + axis, broadcastDims.end(), axis + 1);
+
+    llvm::SmallVector<int64_t, 4> outputDims = llvm::to_vector<4>(indicesShape);
+    outputDims.insert(outputDims.begin() + axis, depth);
+
+    RankedTensorType indexType =
+        RankedTensorType::get(llvm::ArrayRef(outputDims), indicesElementType);
+
+    Value iota = rewriter.create<mhlo::IotaOp>(
+        loc, indexType, IntegerAttr::get(rewriter.getIntegerType(64), axis));
+    Value broadcastIndices = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, indices, GetI64ElementsAttr(broadcastDims, &rewriter));
+    Value zero = rewriter.create<mhlo::ConstantOp>(loc,
+        DenseIntElementsAttr::get(RankedTensorType::get({}, indicesElementType),
+            ArrayRef<int64_t>{0}));
+    Value broadcastZero = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, zero, rewriter.getI64TensorAttr({}));
+    Value broadcastDepth;
+    int64_t depthRank = depthValue.getType().cast<RankedTensorType>().getRank();
+    if (depthRank == 1)
+      broadcastDepth = rewriter.create<mhlo::BroadcastInDimOp>(
+          loc, indexType, depthValue, rewriter.getI64TensorAttr({0}));
+    else
+      broadcastDepth = rewriter.create<mhlo::BroadcastInDimOp>(
+          loc, indexType, depthValue, rewriter.getI64TensorAttr({}));
+    Value compareGeZero = rewriter.create<mhlo::CompareOp>(
+        loc, broadcastIndices, broadcastZero, mhlo::ComparisonDirection::GE);
+    Value positiveIndices =
+        rewriter.create<mhlo::AddOp>(loc, broadcastIndices, broadcastDepth);
+    Value normalizedIndices = rewriter.create<mhlo::SelectOp>(
+        loc, indexType, compareGeZero, broadcastIndices, positiveIndices);
+    Value compare = rewriter.create<mhlo::CompareOp>(
+        loc, normalizedIndices, iota, mhlo::ComparisonDirection::EQ);
+    Type indexElementType = rewriter.getI64Type();
+    Type valueType = values.getType().cast<ShapedType>().getElementType();
+    Value offValue = rewriter.create<mhlo::SliceOp>(loc,
+        RankedTensorType::get({1}, valueType), values,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{0}),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
+        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
+            ArrayRef<int64_t>{1}));
+    Value onValue = rewriter.create<mhlo::SliceOp>(loc,
+        RankedTensorType::get({1}, valueType), values,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{2}),
+        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
+            ArrayRef<int64_t>{1}));
+    Value offValueBroadcast = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, outputType, offValue, rewriter.getI64TensorAttr({0}));
+    Value onValueBroadcast = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, outputType, onValue, rewriter.getI64TensorAttr({0}));
+    Value result = rewriter.create<mhlo::SelectOp>(
+        loc, outputType, compare, onValueBroadcast, offValueBroadcast);
+    rewriter.replaceOp(op, {result});
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXOneHotOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXOneHotOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
new file mode 100644
index 00000000..80db9102
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
@@ -0,0 +1,103 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===----------- Pad.cpp - Lowering Pad Op ------------===//
+//
+// Copyright 2022
+//
+// =============================================================================
+//
+// This file lowers ONNX Pad Operators to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ElementsAttr/DisposableElementsAttr.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXPadOpLoweringToMhlo : public ConversionPattern {
+  ONNXPadOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(mlir::ONNXPadOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+
+    Location loc = op->getLoc();
+    ONNXPadOpAdaptor operandAdaptor(operands, op->getAttrDictionary());
+    Value data = operandAdaptor.getData();
+    Value constantValue = operandAdaptor.getConstantValue();
+    Value pads = operandAdaptor.getPads();
+    Value axes = operandAdaptor.getAxes();
+    StringRef padMode = operandAdaptor.getMode();
+
+    if (!padMode.equals_insensitive("constant"))
+      return failure();
+    // only support axes is None
+    if (!isa<ONNXNoneOp>(axes.getDefiningOp()))
+      return failure();
+    assert(isRankedShapedType(data.getType()) && "Expected Ranked ShapedType");
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    Type elemType = inputType.getElementType();
+    int64_t rank = inputType.getRank();
+
+    Type outputType = *op->result_type_begin();
+    if (!constantValue || isNoneValue(constantValue)) {
+      // Pad with zeros by default
+      constantValue = rewriter.create<mhlo::ConstantOp>(
+          loc, DenseElementsAttr::get(mlir::RankedTensorType::get({}, elemType),
+                   rewriter.getZeroAttr(elemType)));
+    } else {
+      // constantValue might be 1D tensor, reshape it to scalar
+      constantValue = rewriter.create<mhlo::ReshapeOp>(
+          loc, RankedTensorType::get({}, elemType), constantValue);
+    }
+    SmallVector<int64_t> edgePaddingLowVec(rank, 0);
+    SmallVector<int64_t> edgePaddingHighVec(rank, 0);
+    SmallVector<int64_t> interiorPaddingVec(rank, 0);
+    if (auto valueAttribute = getElementAttributeFromConstValue(pads)) {
+      // If `pads` are constants, read them."
+      int64_t idx = 0;
+      for (IntegerAttr value : valueAttribute.getValues<IntegerAttr>()) {
+        int64_t padValue = value.getInt();
+        if (padValue < 0)
+          return failure();
+        if (idx < rank)
+          edgePaddingLowVec[idx] = padValue;
+        else
+          edgePaddingHighVec[idx - rank] = padValue;
+        idx++;
+      }
+    } else {
+      assert(false && "Pads must be known at compile time");
+    }
+
+    mlir::DenseIntElementsAttr edgePaddingLow =
+        rewriter.getI64VectorAttr(edgePaddingLowVec);
+    mlir::DenseIntElementsAttr edgePaddingHigh =
+        rewriter.getI64VectorAttr(edgePaddingHighVec);
+    mlir::DenseIntElementsAttr interiorPadding =
+        rewriter.getI64VectorAttr(interiorPaddingVec);
+    Value padResult = rewriter.create<mhlo::PadOp>(loc, outputType, data,
+        constantValue, edgePaddingLow, edgePaddingHigh, interiorPadding);
+
+    rewriter.replaceOp(op, padResult);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXPadOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXPadOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp b/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp
new file mode 100644
index 00000000..1d4a1df6
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp
@@ -0,0 +1,97 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- ScatterND.cpp - Lowering ScatterND Op ----------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX ScatterND Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXScatterNDOpLoweringToMhlo
+    : public OpConversionPattern<ONNXScatterNDOp> {
+  ONNXScatterNDOpLoweringToMhlo(MLIRContext *ctx) : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXScatterNDOp scatterNDOp,
+      ONNXScatterNDOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = scatterNDOp.getOperation();
+    Location loc = ONNXLoc<ONNXScatterNDOp>(op);
+
+    // Operands and attributes.
+    Value data = adaptor.getData();
+    Value updates = adaptor.getUpdates();
+    Value indices = adaptor.getIndices();
+    auto dataType = data.getType().cast<ShapedType>();
+    auto indicesType = indices.getType().cast<ShapedType>();
+    auto updatesType = updates.getType().cast<ShapedType>();
+    int64_t dataRank = dataType.getRank();
+    int64_t updatesRank = updatesType.getRank();
+    int64_t indicesRank = indicesType.getRank();
+    assert(indicesType.hasStaticShape() &&
+           "only support indices with static shape");
+    int64_t partialIdxDim = indicesType.getDimSize(indicesRank - 1);
+
+    assert(dataRank >= 1 && "The rank of 'data' must be >= 1");
+    assert(indicesRank >= 1 && "The rank of 'indices' must be >= 1");
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+    ShapedType outputShapedType = outputType.cast<ShapedType>();
+    int64_t outputRank = outputShapedType.getRank();
+    assert(outputRank == dataRank && "Output rank not equal to data rank");
+    auto scatter_dimension_numbers =
+        mlir::mhlo::ScatterDimensionNumbersAttr::get(
+            /*context=*/rewriter.getContext(),
+            /*updateWindowDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(partialIdxDim, dataRank)),
+            /*insertedWindowDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(0, partialIdxDim)),
+            /*scatterDimsToOperandDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(0, partialIdxDim)),
+            /*indexVectorDim=*/indicesRank - 1);
+    auto scatterOp = rewriter.create<mhlo::ScatterOp>(
+        loc, outputType, data, indices, updates, scatter_dimension_numbers);
+    // config update computation function: just return the element from src.
+    Block &block = scatterOp.getUpdateComputation().emplaceBlock();
+    // add block arguments
+    auto blockArgumentType =
+        RankedTensorType::get({}, dataType.getElementType());
+    block.addArgument(blockArgumentType, loc);
+    block.addArgument(blockArgumentType, loc);
+
+    auto *lhsArg = block.args_begin();
+    auto *rhsArg = std::next(lhsArg);
+
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&block);
+      rewriter.create<mhlo::ReturnOp>(loc, *rhsArg);
+    }
+
+    rewriter.replaceOp(op, scatterOp.getResults());
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXScatterNDOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXScatterNDOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Dialect/Mlir/DialectBuilder.cpp b/src/Dialect/Mlir/DialectBuilder.cpp
index fb7dc8ad..ebf5f117 100644
--- a/src/Dialect/Mlir/DialectBuilder.cpp
+++ b/src/Dialect/Mlir/DialectBuilder.cpp
@@ -840,6 +840,14 @@ Value ShapeBuilder::shapeOf(Value val) const {
   return b().create<shape::ShapeOfOp>(loc(), val);
 }

+Value ShapeBuilder::fromExtents(ValueRange extents) const {
+  return b().create<shape::FromExtentsOp>(loc(), extents);
+}
+
+Value ShapeBuilder::toExtentTensor(Type type, Value shape) const {
+  return b().create<shape::ToExtentTensorOp>(loc(), type, shape);
+}
+
 Value ShapeBuilder::getExtent(Value val, int64_t index) const {
   return b().create<shape::GetExtentOp>(loc(), val, index);
 }
diff --git a/src/Dialect/Mlir/DialectBuilder.hpp b/src/Dialect/Mlir/DialectBuilder.hpp
index 68b9c7bf..a61128f1 100644
--- a/src/Dialect/Mlir/DialectBuilder.hpp
+++ b/src/Dialect/Mlir/DialectBuilder.hpp
@@ -203,7 +203,10 @@ struct ShapeBuilder final : DialectBuilder {

   mlir::Value dim(mlir::Value val, int64_t index) const;
   mlir::Value shapeOf(mlir::Value val) const;
+
+  mlir::Value fromExtents(mlir::ValueRange extents) const;
   mlir::Value getExtent(mlir::Value val, int64_t index) const;
+  mlir::Value toExtentTensor(mlir::Type type, mlir::Value shape) const;
 };

 //===----------------------------------------------------------------------===//
diff --git a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
index beb50392..b821ec34 100644
--- a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
+++ b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
@@ -544,6 +544,16 @@ struct ONNXPadOpShapeHelper : public ONNXOpShapeHelper {
   llvm::SmallVector<IndexExpr, 4> pads;
 };

+struct ONNXPadV13OpShapeHelper : public ONNXOpShapeHelper {
+  ONNXPadV13OpShapeHelper(mlir::Operation *op, mlir::ValueRange operands,
+      IndexExprBuilder *ieBuilder = nullptr, IndexExprScope *scope = nullptr)
+      : ONNXOpShapeHelper(op, operands, ieBuilder, scope), pads() {}
+  virtual ~ONNXPadV13OpShapeHelper() {}
+  mlir::LogicalResult computeShape() final;
+  // Additional data for PadOp.
+  llvm::SmallVector<IndexExpr, 4> pads;
+};
+
 //===----------------------------------------------------------------------===//
 // OneHot Op
 //===----------------------------------------------------------------------===//
diff --git a/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp b/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
index b00edc4a..f33bc18a 100644
--- a/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
+++ b/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
@@ -67,6 +67,49 @@ LogicalResult ONNXPadOpShapeHelper::computeShape() {
   return success();
 }

+LogicalResult ONNXPadV13OpShapeHelper::computeShape() {
+  ONNXPadOpAdaptor operandAdaptor(operands);
+  Value dataOperand = operandAdaptor.getData();
+  Value padsOperand = operandAdaptor.getPads();
+  DimsExpr outputDims;
+
+  // Get info about input data operand.
+  uint64_t dataRank = createIE->getShapedTypeRank(dataOperand);
+
+  // Initialize context and results (pads & output)
+  pads.resize(2 * dataRank); // pads two sides of each axis.
+  outputDims.resize(dataRank);
+
+  // `pads` format is : [x1_begin, x2_begin,...,x1_end, x2_end,...],
+  // where
+  // - xi_begin: the number of pad values added at the beginning of axis `i`
+  // - xi_end: the number of pad values added at the end of axis `i`.
+
+  // Calculate output dimension sizes.
+  for (uint64_t i = 0; i < dataRank; i++) {
+    // Get begin/end pads.
+    SymbolIndexExpr padBegin(createIE->getIntFromArrayAsSymbol(padsOperand, i));
+    SymbolIndexExpr padEnd(
+        createIE->getIntFromArrayAsSymbol(padsOperand, i + dataRank));
+    if (padBegin.isUndefined() || padEnd.isUndefined())
+      return op->emitError("pad parameter could not be processed");
+    // Get input dim.
+    DimIndexExpr dimInput(createIE->getShapeAsDim(dataOperand, i));
+
+    // Calculation for output size.
+    IndexExpr dimOutputFinal = (padBegin + dimInput) + padEnd;
+
+    // Save results.
+    pads[i] = padBegin;
+    pads[i + dataRank] = padEnd;
+    outputDims[i] = dimOutputFinal;
+  }
+
+  // Save the final result.
+  setOutputDims(outputDims);
+  return success();
+}
+
 } // namespace onnx_mlir

 //===----------------------------------------------------------------------===//
@@ -108,3 +151,15 @@ LogicalResult ONNXPadOp::inferShapes(
   ONNXPadOpShapeHelper shapeHelper(getOperation(), {});
   return shapeHelper.computeShapeAndUpdateType(elementType);
 }
+
+LogicalResult ONNXPadV13Op::inferShapes(
+    std::function<void(Region &)> doShapeInference) {
+  // Cannot infer shape if no shape exists.
+  if (!hasShapeAndRank(getData()) || !hasShapeAndRank(getPads()))
+    return success();
+
+  Type elementType = getData().getType().cast<ShapedType>().getElementType();
+
+  ONNXPadV13OpShapeHelper shapeHelper(getOperation(), {});
+  return shapeHelper.computeShapeAndUpdateType(elementType);
+}
diff --git a/src/Dialect/ONNX/ONNXUnsupportedOps.hpp b/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
index 758635e4..f5c07c76 100644
--- a/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
+++ b/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
@@ -56,7 +56,6 @@ UNSUPPORTED_OPS(ONNXMomentumOp)
 UNSUPPORTED_OPS(ONNXMultinomialOp)
 UNSUPPORTED_OPS(ONNXNegativeLogLikelihoodLossOp)
 UNSUPPORTED_OPS(ONNXNormalizerOp)
-UNSUPPORTED_OPS(ONNXPadV13Op)
 UNSUPPORTED_OPS(ONNXPadV11Op)
 UNSUPPORTED_OPS(ONNXPadV2Op)
 UNSUPPORTED_OPS(ONNXRandomUniformLikeOp)
diff --git a/src/Pass/Passes.hpp b/src/Pass/Passes.hpp
index 9b9c9815..84f6080a 100644
--- a/src/Pass/Passes.hpp
+++ b/src/Pass/Passes.hpp
@@ -88,6 +88,7 @@ void configureOnnxToKrnlLoweringPass(bool reportOnParallel,
 #ifdef ONNX_MLIR_ENABLE_MHLO
 /// Add pass for lowering to Mhlo IR.
 std::unique_ptr<mlir::Pass> createLowerToMhloPass();
+std::unique_ptr<mlir::Pass> createLowerToMhloPass(bool enableUnroll);
 #endif

 /// Pass for lowering krnl.dim operations to standard dialect.
diff --git a/src/Tools/onnx-mlir-opt/RegisterPasses.cpp b/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
index 4fca3de6..71280add 100644
--- a/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
+++ b/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
@@ -116,6 +116,10 @@ void registerOMPasses(int optLevel) {
     return createSimplifyShapeRelatedOpsPass();
   });

+  mlir::registerPass([]() -> std::unique_ptr<mlir::Pass> {
+    return createStandardFuncReturnPass();
+  });
+
   mlir::registerPass([]() -> std::unique_ptr<mlir::Pass> {
     return createONNXDimAnalysisPass();
   });
diff --git a/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir b/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
index 834471e3..db8b0d32 100644
--- a/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
+++ b/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
@@ -37,6 +37,24 @@ func.func @test_relu(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {

 // -----

+func.func @test_elu(%arg0 : tensor<20x40xf32>) -> tensor<20x40xf32> {
+  %0 = "onnx.Elu"(%arg0) {alpha = 1.000000e+00 : f32} : (tensor<20x40xf32>) -> tensor<20x40xf32>
+  "func.return"(%0) : (tensor<20x40xf32>) -> ()
+// CHECK-LABEL:  func.func @test_elu
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<20x40xf32>) -> tensor<20x40xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_1_:%.+]] = mhlo.constant dense<1.000000e+00> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_2_:%.+]] = mhlo.exponential [[PARAM_0_]] : tensor<20x40xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_3_:%.+]] = mhlo.subtract [[VAR_2_]], [[VAR_1_]] : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_4_:%.+]] = mhlo.compare  GE, [[PARAM_0_]], [[VAR_0_]],  NOTYPE : (tensor<20x40xf32>, tensor<20x40xf32>) -> tensor<20x40xi1>
+// CHECK:           [[VAR_5_:%.+]] = mhlo.select [[VAR_4_]], [[PARAM_0_]], [[VAR_3_]] : tensor<20x40xi1>, tensor<20x40xf32>
+// CHECK:           return [[VAR_5_]] : tensor<20x40xf32>
+// CHECK:         }
+}
+
+// -----
+
 func.func @test_relu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
   %0 = "onnx.Relu"(%arg0) : (tensor<?x10xf32>) -> tensor<?x10xf32>
   "func.return"(%0) : (tensor<?x10xf32>) -> ()
@@ -90,6 +108,23 @@ func.func @test_dynamic_sigmoid(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {

 // -----

+func.func @test_hard_sigmoid(%arg0 : tensor<20x40xf32>) -> tensor<20x40xf32> {
+  %0 = "onnx.HardSigmoid"(%arg0) {alpha = 5.000000e-01 : f32, beta = 5.000000e-01 : f32} : (tensor<20x40xf32>) -> tensor<20x40xf32>
+  "func.return"(%0) : (tensor<20x40xf32>) -> ()
+// CHECK-LABEL:  func.func @test_hard_sigmoid
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<20x40xf32>) -> tensor<20x40xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = mhlo.constant dense<5.000000e-01> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_1_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_2_:%.+]] = mhlo.constant dense<1.000000e+00> : tensor<20x40xf32>
+// CHECK:           [[VAR_3_:%.+]] = mhlo.multiply [[PARAM_0_]], [[VAR_0_]] : tensor<20x40xf32>
+// CHECK:           [[VAR_4_:%.+]] = mhlo.add [[VAR_3_]], [[VAR_0_]] : tensor<20x40xf32>
+// CHECK:           [[VAR_5_:%.+]] = mhlo.clamp [[VAR_1_]], [[VAR_4_]], [[VAR_2_]] : tensor<20x40xf32>
+// CHECK:           return [[VAR_5_]] : tensor<20x40xf32>
+// CHECK:         }
+}
+
+// -----
+
 func.func @test_abs(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
   %0 = "onnx.Abs"(%arg0) : (tensor<10x10xf32>) -> tensor<10x10xf32>
 // CHECK-LABEL:  func @test_abs
@@ -256,6 +291,15 @@ func.func @test_max(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> ten
 // CHECK-NEXT:      return [[VAR_0_]] : tensor<10x10xf32>
 }

+func.func @test_min(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> tensor<10x10xf32> {
+  %0 = "onnx.Min"(%arg0, %arg1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<10x10xf32>
+  "func.return"(%0) : (tensor<10x10xf32>) -> ()
+// CHECK-LABEL:  func @test_min
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<10x10xf32>, [[PARAM_1_:%.+]]: tensor<10x10xf32>) -> tensor<10x10xf32> {
+// CHECK-NEXT:      [[VAR_0_:%.+]] = mhlo.minimum [[PARAM_0_]], [[PARAM_1_]] : tensor<10x10xf32>
+// CHECK-NEXT:      return [[VAR_0_]] : tensor<10x10xf32>
+}
+
 func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
   %0 = "onnx.LeakyRelu"(%arg0) {alpha=0.5:f32} : (tensor<?x10xf32>) -> tensor<?x10xf32>
   "func.return"(%0) : (tensor<?x10xf32>) -> ()
@@ -275,6 +319,16 @@ func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32>

 // -----

+func.func @test_prelu_dynamic(%arg0 : tensor<?x10x12x12xf32>, %arg1: tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32> {
+  %0 = "onnx.PRelu"(%arg0, %arg1) : (tensor<?x10x12x12xf32>, tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32>
+  "func.return"(%0) : (tensor<?x10x12x12xf32>) -> ()
+// CHECK-LABEL:  func.func @test_prelu_dynamic
+// CHECK-SAME:   (%arg0: tensor<?x10x12x12xf32>, %arg1: tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32> {
+// CHECK:        [[VAR_0_:%.+]] = mhlo.multiply [[INP:%.+]], [[SLOPE:%.+]] : tensor<?x10x12x12xf32>
+// CHECK:        [[VAR_1_:%.+]] = mhlo.compare  GT, [[INP]], [[ZEROS:%.+]],  NOTYPE : (tensor<?x10x12x12xf32>, tensor<?x10x12x12xf32>) -> tensor<?x10x12x12xi1>
+// CHECK:        [[VAR_2_:%.+]] = mhlo.select [[VAR_1_]], [[INP]], [[VAR_0_]] : tensor<?x10x12x12xi1>, tensor<?x10x12x12xf32>
+}
+
 func.func @test_neg(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
   %0 = "onnx.Neg"(%arg0) : (tensor<10x10xf32>) -> tensor<10x10xf32>
   "func.return"(%0) : (tensor<10x10xf32>) -> ()
@@ -290,3 +344,10 @@ func.func @test_sin(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
 // CHECK-LABEL: func @test_sin
 // CHECK: %0 = mhlo.sine %arg0 : tensor<10x10xf32>
 }
+
+func.func @test_where(%arg0 : tensor<16x24x36xi1>, %arg1 : tensor<16x24x36xi64>, %arg2 : tensor<16x24x36xi64>) -> tensor<16x24x36xi64> {
+  %0 = "onnx.Where"(%arg0, %arg1, %arg2) : (tensor<16x24x36xi1>, tensor<16x24x36xi64>, tensor<16x24x36xi64>) -> tensor<16x24x36xi64>
+  "func.return"(%0) : (tensor<16x24x36xi64>) -> ()
+// CHECK-LABEL: func.func @test_where
+// CHECK:   %0 = mhlo.select %arg0, %arg1, %arg2 : tensor<16x24x36xi1>, tensor<16x24x36xi64>
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM-loop.mlir b/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM-loop.mlir
new file mode 100644
index 00000000..2768e8f2
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM-loop.mlir
@@ -0,0 +1,164 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-mhlo="enable-unroll=false" --canonicalize -split-input-file %s | FileCheck %s
+func.func @test_lstm(%arg0 : tensor<128x16x512xf32>, %arg1 : tensor<2x2048xf32>, %arg2 : tensor<2x1024x512xf32>, %arg3 : tensor<2x1024x256xf32>) -> tensor<128x2x16x256xf32> {
+  %0 = onnx.Constant dense<0.000000e+00> : tensor<2x16x256xf32>
+  %1 = "onnx.NoValue"() {value} : () -> none
+  %Y, %Y_h, %Y_c = "onnx.LSTM"(%arg0, %arg2, %arg3, %arg1, %1, %0, %0, %1) {direction = "bidirectional", hidden_size = 256 : si64, input_forget = 0 : si64, layout = 0 : si64} : (tensor<128x16x512xf32>, tensor<2x1024x512xf32>, tensor<2x1024x256xf32>, tensor<2x2048xf32>, none, tensor<2x16x256xf32>, tensor<2x16x256xf32>, none) -> (tensor<128x2x16x256xf32>, tensor<2x16x256xf32>, tensor<2x16x256xf32>)
+  return %Y : tensor<128x2x16x256xf32>
+// CHECK-LABEL:  func.func @test_lstm
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<128x16x512xf32>, [[PARAM_1_:%.+]]: tensor<2x2048xf32>, [[PARAM_2_:%.+]]: tensor<2x1024x512xf32>, [[PARAM_3_:%.+]]: tensor<2x1024x256xf32>) -> tensor<128x2x16x256xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_1_:%.+]] = mhlo.constant dense<127> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_2_:%.+]] = mhlo.constant dense<1> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_3_:%.+]] = mhlo.constant dense<128> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_4_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<128x1x16x256xf32>
+// CHECK-DAG:       [[VAR_5_:%.+]] = mhlo.constant dense<0> : tensor<i64>
+// CHECK-DAG:       [[VAR_6_:%.+]] = mhlo.constant dense<0> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_7_:%.+]] = "mhlo.slice"([[PARAM_2_]]) {limit_indices = dense<[1, 1024, 512]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-DAG:       [[VAR_8_:%.+]] = "mhlo.slice"([[PARAM_2_]]) {limit_indices = dense<[2, 1024, 512]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_9_:%.+]] = mhlo.reshape [[VAR_7_]] : (tensor<1x1024x512xf32>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_10_:%.+]] = mhlo.reshape [[VAR_8_]] : (tensor<1x1024x512xf32>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_11_:%.+]] = "mhlo.slice"([[PARAM_3_]]) {limit_indices = dense<[1, 1024, 256]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-DAG:       [[VAR_12_:%.+]] = "mhlo.slice"([[PARAM_3_]]) {limit_indices = dense<[2, 1024, 256]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_13_:%.+]] = mhlo.reshape [[VAR_11_]] : (tensor<1x1024x256xf32>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_14_:%.+]] = mhlo.reshape [[VAR_12_]] : (tensor<1x1024x256xf32>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_15_:%.+]] = "mhlo.transpose"([[VAR_9_]]) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_16_:%.+]] = "mhlo.transpose"([[VAR_13_]]) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_17_:%.+]] = "mhlo.transpose"([[VAR_10_]]) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-DAG:       [[VAR_18_:%.+]] = "mhlo.transpose"([[VAR_14_]]) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_19_:%.+]] = "mhlo.slice"([[PARAM_1_]]) {limit_indices = dense<[1, 2048]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-DAG:       [[VAR_20_:%.+]] = "mhlo.slice"([[PARAM_1_]]) {limit_indices = dense<[2, 2048]> : tensor<2xi64>, start_indices = dense<[1, 0]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_21_:%.+]] = mhlo.reshape [[VAR_19_]] : (tensor<1x2048xf32>) -> tensor<2048xf32>
+// CHECK-DAG:       [[VAR_22_:%.+]] = mhlo.reshape [[VAR_20_]] : (tensor<1x2048xf32>) -> tensor<2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_23_:%.+]] = "mhlo.slice"([[VAR_21_]]) {limit_indices = dense<256> : tensor<1xi64>, start_indices = dense<0> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_24_:%.+]] = "mhlo.slice"([[VAR_21_]]) {limit_indices = dense<512> : tensor<1xi64>, start_indices = dense<256> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_25_:%.+]] = "mhlo.slice"([[VAR_21_]]) {limit_indices = dense<768> : tensor<1xi64>, start_indices = dense<512> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_26_:%.+]] = "mhlo.slice"([[VAR_21_]]) {limit_indices = dense<1024> : tensor<1xi64>, start_indices = dense<768> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_27_:%.+]] = "mhlo.slice"([[VAR_21_]]) {limit_indices = dense<1280> : tensor<1xi64>, start_indices = dense<1024> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_28_:%.+]] = "mhlo.slice"([[VAR_21_]]) {limit_indices = dense<1536> : tensor<1xi64>, start_indices = dense<1280> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_29_:%.+]] = "mhlo.slice"([[VAR_21_]]) {limit_indices = dense<1792> : tensor<1xi64>, start_indices = dense<1536> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_30_:%.+]] = "mhlo.slice"([[VAR_21_]]) {limit_indices = dense<2048> : tensor<1xi64>, start_indices = dense<1792> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_31_:%.+]] = "mhlo.slice"([[VAR_22_]]) {limit_indices = dense<256> : tensor<1xi64>, start_indices = dense<0> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_32_:%.+]] = "mhlo.slice"([[VAR_22_]]) {limit_indices = dense<512> : tensor<1xi64>, start_indices = dense<256> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_33_:%.+]] = "mhlo.slice"([[VAR_22_]]) {limit_indices = dense<768> : tensor<1xi64>, start_indices = dense<512> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_34_:%.+]] = "mhlo.slice"([[VAR_22_]]) {limit_indices = dense<1024> : tensor<1xi64>, start_indices = dense<768> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_35_:%.+]] = "mhlo.slice"([[VAR_22_]]) {limit_indices = dense<1280> : tensor<1xi64>, start_indices = dense<1024> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_36_:%.+]] = "mhlo.slice"([[VAR_22_]]) {limit_indices = dense<1536> : tensor<1xi64>, start_indices = dense<1280> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_37_:%.+]] = "mhlo.slice"([[VAR_22_]]) {limit_indices = dense<1792> : tensor<1xi64>, start_indices = dense<1536> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_38_:%.+]] = "mhlo.slice"([[VAR_22_]]) {limit_indices = dense<2048> : tensor<1xi64>, start_indices = dense<1792> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_39_:%.+]]:4 = mhlo.while([[VAR_iterArg_:%.+]] = [[VAR_6_]], [[VAR_iterArg_0_:%.+]] = [[VAR_4_]], [[VAR_iterArg_1_:%.+]] = [[VAR_0_]], [[VAR_iterArg_2_:%.+]] = [[VAR_0_]]) : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:            cond {
+// CHECK:             [[VAR_42_:%.+]] = mhlo.compare  LT, [[VAR_iterArg_]], [[VAR_3_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_43_:%.+]] = mhlo.reshape [[VAR_42_]] : (tensor<1xi1>) -> tensor<i1>
+// CHECK:             mhlo.return [[VAR_43_]] : tensor<i1>
+// CHECK:           } do {
+// CHECK:             [[VAR_42_1_:%.+]] = mhlo.reshape [[VAR_iterArg_]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:             [[VAR_43_1_:%.+]] = "mhlo.dynamic_slice"([[PARAM_0_]], [[VAR_42_1_]], [[VAR_5_]], [[VAR_5_]]) {slice_sizes = dense<[1, 16, 512]> : tensor<3xi64>} : (tensor<128x16x512xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x512xf32>
+// CHECK:             [[VAR_44_:%.+]] = mhlo.reshape [[VAR_43_1_]] : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_45_:%.+]] = "mhlo.dot"([[VAR_44_]], [[VAR_15_]]) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_46_:%.+]] = "mhlo.dot"([[VAR_iterArg_1_]], [[VAR_16_]]) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:             [[VAR_47_:%.+]] = mhlo.add [[VAR_45_]], [[VAR_46_]] : tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_48_:%.+]] = "mhlo.slice"([[VAR_47_]]) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_49_:%.+]] = "mhlo.slice"([[VAR_47_]]) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_50_:%.+]] = "mhlo.slice"([[VAR_47_]]) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_51_:%.+]] = "mhlo.slice"([[VAR_47_]]) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_52_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_23_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_53_:%.+]] = mhlo.add [[VAR_48_]], [[VAR_52_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_54_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_27_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_55_:%.+]] = mhlo.add [[VAR_53_]], [[VAR_54_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_56_:%.+]] = mhlo.logistic [[VAR_55_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_57_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_25_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_58_:%.+]] = mhlo.add [[VAR_50_]], [[VAR_57_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_59_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_29_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_60_:%.+]] = mhlo.add [[VAR_58_]], [[VAR_59_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_61_:%.+]] = mhlo.logistic [[VAR_60_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_62_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_26_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_63_:%.+]] = mhlo.add [[VAR_51_]], [[VAR_62_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_64_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_30_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_65_:%.+]] = mhlo.add [[VAR_63_]], [[VAR_64_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_66_:%.+]] = mhlo.tanh [[VAR_65_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_67_:%.+]] = mhlo.multiply [[VAR_61_]], [[VAR_iterArg_2_]] : tensor<16x256xf32>
+// CHECK:             [[VAR_68_:%.+]] = mhlo.multiply [[VAR_56_]], [[VAR_66_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_69_:%.+]] = mhlo.add [[VAR_67_]], [[VAR_68_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_70_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_24_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_71_:%.+]] = mhlo.add [[VAR_49_]], [[VAR_70_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_72_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_28_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_73_:%.+]] = mhlo.add [[VAR_71_]], [[VAR_72_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_74_:%.+]] = mhlo.logistic [[VAR_73_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_75_:%.+]] = mhlo.tanh [[VAR_69_]] : tensor<16x256xf32>
+// CHECK:             [[VAR_76_:%.+]] = mhlo.multiply [[VAR_74_]], [[VAR_75_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_77_:%.+]] = mhlo.reshape [[VAR_76_]] : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:         [[VAR_78_:%.+]] = mhlo.reshape [[VAR_iterArg_]] : (tensor<1xi64>) -> tensor<1x1xi64>
+// CHECK:             [[VAR_79_:%.+]] = "mhlo.scatter"([[VAR_iterArg_0_]], [[VAR_78_]], [[VAR_77_]]) ({
+// CHECK:             ^bb0([[arg4_:%.+]]: tensor<f32>, [[arg5_:%.+]]: tensor<f32>):
+// CHECK:               mhlo.return [[arg5_]] : tensor<f32>
+// CHECK:             }) {indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<update_window_dims = [1, 2, 3], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<128x1x16x256xf32>, tensor<1x1xi64>, tensor<1x1x16x256xf32>) -> tensor<128x1x16x256xf32>
+// CHECK:             [[VAR_80_:%.+]] = mhlo.add [[VAR_iterArg_]], [[VAR_2_]] : tensor<1xi64>
+// CHECK:             mhlo.return [[VAR_80_]], [[VAR_79_]], [[VAR_76_]], [[VAR_69_]] : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:           }
+// CHECK:           [[VAR_40_:%.+]]:4 = mhlo.while([[VAR_iterArg_1_:%.+]] = [[VAR_1_]], [[VAR_iterArg_0_1_:%.+]] = [[VAR_4_]], [[VAR_iterArg_1_1_:%.+]] = [[VAR_0_]], [[VAR_iterArg_2_1_:%.+]] = [[VAR_0_]]) : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:            cond {
+// CHECK:             [[VAR_42_2_:%.+]] = mhlo.compare  GE, [[VAR_iterArg_1_]], [[VAR_6_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_43_2_:%.+]] = mhlo.reshape [[VAR_42_2_]] : (tensor<1xi1>) -> tensor<i1>
+// CHECK:             mhlo.return [[VAR_43_2_]] : tensor<i1>
+// CHECK:           } do {
+// CHECK:             [[VAR_42_3_:%.+]] = mhlo.reshape [[VAR_iterArg_1_]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:             [[VAR_43_3_:%.+]] = "mhlo.dynamic_slice"([[PARAM_0_]], [[VAR_42_3_]], [[VAR_5_]], [[VAR_5_]]) {slice_sizes = dense<[1, 16, 512]> : tensor<3xi64>} : (tensor<128x16x512xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x512xf32>
+// CHECK:             [[VAR_44_1_:%.+]] = mhlo.reshape [[VAR_43_3_]] : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_45_1_:%.+]] = "mhlo.dot"([[VAR_44_1_]], [[VAR_17_]]) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_46_1_:%.+]] = "mhlo.dot"([[VAR_iterArg_1_1_]], [[VAR_18_]]) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:             [[VAR_47_1_:%.+]] = mhlo.add [[VAR_45_1_]], [[VAR_46_1_]] : tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_48_1_:%.+]] = "mhlo.slice"([[VAR_47_1_]]) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_49_1_:%.+]] = "mhlo.slice"([[VAR_47_1_]]) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_50_1_:%.+]] = "mhlo.slice"([[VAR_47_1_]]) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_51_1_:%.+]] = "mhlo.slice"([[VAR_47_1_]]) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_52_1_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_31_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_53_1_:%.+]] = mhlo.add [[VAR_48_1_]], [[VAR_52_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_54_1_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_35_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_55_1_:%.+]] = mhlo.add [[VAR_53_1_]], [[VAR_54_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_56_1_:%.+]] = mhlo.logistic [[VAR_55_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_57_1_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_33_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_58_1_:%.+]] = mhlo.add [[VAR_50_1_]], [[VAR_57_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_59_1_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_37_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_60_1_:%.+]] = mhlo.add [[VAR_58_1_]], [[VAR_59_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_61_1_:%.+]] = mhlo.logistic [[VAR_60_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_62_1_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_34_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_63_1_:%.+]] = mhlo.add [[VAR_51_1_]], [[VAR_62_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_64_1_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_38_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_65_1_:%.+]] = mhlo.add [[VAR_63_1_]], [[VAR_64_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_66_1_:%.+]] = mhlo.tanh [[VAR_65_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_67_1_:%.+]] = mhlo.multiply [[VAR_61_1_]], [[VAR_iterArg_2_1_]] : tensor<16x256xf32>
+// CHECK:             [[VAR_68_1_:%.+]] = mhlo.multiply [[VAR_56_1_]], [[VAR_66_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_69_1_:%.+]] = mhlo.add [[VAR_67_1_]], [[VAR_68_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_70_1_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_32_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_71_1_:%.+]] = mhlo.add [[VAR_49_1_]], [[VAR_70_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_72_1_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_36_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_73_1_:%.+]] = mhlo.add [[VAR_71_1_]], [[VAR_72_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_74_1_:%.+]] = mhlo.logistic [[VAR_73_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_75_1_:%.+]] = mhlo.tanh [[VAR_69_1_]] : tensor<16x256xf32>
+// CHECK:             [[VAR_76_1_:%.+]] = mhlo.multiply [[VAR_74_1_]], [[VAR_75_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_77_1_:%.+]] = mhlo.reshape [[VAR_76_1_]] : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:         [[VAR_78_1_:%.+]] = mhlo.reshape [[VAR_iterArg_1_]] : (tensor<1xi64>) -> tensor<1x1xi64>
+// CHECK:             [[VAR_79_1_:%.+]] = "mhlo.scatter"([[VAR_iterArg_0_1_]], [[VAR_78_1_]], [[VAR_77_1_]]) ({
+// CHECK:             ^bb0([[arg4_:%.+]]: tensor<f32>, [[arg5_:%.+]]: tensor<f32>):
+// CHECK:               mhlo.return [[arg5_]] : tensor<f32>
+// CHECK:             }) {indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<update_window_dims = [1, 2, 3], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<128x1x16x256xf32>, tensor<1x1xi64>, tensor<1x1x16x256xf32>) -> tensor<128x1x16x256xf32>
+// CHECK:             [[VAR_80_1_:%.+]] = mhlo.subtract [[VAR_iterArg_1_]], [[VAR_2_]] : tensor<1xi64>
+// CHECK:             mhlo.return [[VAR_80_1_]], [[VAR_79_1_]], [[VAR_76_1_]], [[VAR_69_1_]] : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:           }
+// CHECK:           [[VAR_41_:%.+]] = "mhlo.concatenate"([[VAR_39_]]#1, [[VAR_40_]]#1) {dimension = 1 : i64} : (tensor<128x1x16x256xf32>, tensor<128x1x16x256xf32>) -> tensor<128x2x16x256xf32>
+// CHECK:           return [[VAR_41_]] : tensor<128x2x16x256xf32>
+// CHECK:         }
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM.mlir b/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM.mlir
new file mode 100644
index 00000000..9dfccb2b
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/RNN/LSTM.mlir
@@ -0,0 +1,208 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-mhlo --canonicalize -split-input-file %s | FileCheck %s
+func.func @test_lstm(%arg0 : tensor<2x16x512xf32>, %arg1 : tensor<2x2048xf32>, %arg2 : tensor<2x1024x512xf32>, %arg3 : tensor<2x1024x256xf32>) -> tensor<2x2x16x256xf32> {
+  %0 = onnx.Constant dense<0.000000e+00> : tensor<2x16x256xf32>
+  %1 = "onnx.NoValue"() {value} : () -> none
+  %Y, %Y_h, %Y_c = "onnx.LSTM"(%arg0, %arg2, %arg3, %arg1, %1, %0, %0, %1) {direction = "bidirectional", hidden_size = 256 : si64, input_forget = 0 : si64, layout = 0 : si64} : (tensor<2x16x512xf32>, tensor<2x1024x512xf32>, tensor<2x1024x256xf32>, tensor<2x2048xf32>, none, tensor<2x16x256xf32>, tensor<2x16x256xf32>, none) -> (tensor<2x2x16x256xf32>, tensor<2x16x256xf32>, tensor<2x16x256xf32>)
+  return %Y : tensor<2x2x16x256xf32>
+// CHECK-LABEL:  func.func @test_lstm
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<2x16x512xf32>, [[PARAM_1_:%.+]]: tensor<2x2048xf32>, [[PARAM_2_:%.+]]: tensor<2x1024x512xf32>, [[PARAM_3_:%.+]]: tensor<2x1024x256xf32>) -> tensor<2x2x16x256xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_1_:%.+]] = "mhlo.slice"([[PARAM_2_]]) {limit_indices = dense<[1, 1024, 512]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-DAG:       [[VAR_2_:%.+]] = "mhlo.slice"([[PARAM_2_]]) {limit_indices = dense<[2, 1024, 512]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_3_:%.+]] = mhlo.reshape [[VAR_1_]] : (tensor<1x1024x512xf32>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_4_:%.+]] = mhlo.reshape [[VAR_2_]] : (tensor<1x1024x512xf32>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_5_:%.+]] = "mhlo.slice"([[PARAM_3_]]) {limit_indices = dense<[1, 1024, 256]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-DAG:       [[VAR_6_:%.+]] = "mhlo.slice"([[PARAM_3_]]) {limit_indices = dense<[2, 1024, 256]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_7_:%.+]] = mhlo.reshape [[VAR_5_]] : (tensor<1x1024x256xf32>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_8_:%.+]] = mhlo.reshape [[VAR_6_]] : (tensor<1x1024x256xf32>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_9_:%.+]] = "mhlo.transpose"([[VAR_3_]]) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_10_:%.+]] = "mhlo.transpose"([[VAR_7_]]) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_11_:%.+]] = "mhlo.transpose"([[VAR_4_]]) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-DAG:       [[VAR_12_:%.+]] = "mhlo.transpose"([[VAR_8_]]) {permutation = dense<[1, 0]> : tensor<2xi64>} : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_13_:%.+]] = "mhlo.slice"([[PARAM_1_]]) {limit_indices = dense<[1, 2048]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-DAG:       [[VAR_14_:%.+]] = "mhlo.slice"([[PARAM_1_]]) {limit_indices = dense<[2, 2048]> : tensor<2xi64>, start_indices = dense<[1, 0]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_15_:%.+]] = mhlo.reshape [[VAR_13_]] : (tensor<1x2048xf32>) -> tensor<2048xf32>
+// CHECK-DAG:       [[VAR_16_:%.+]] = mhlo.reshape [[VAR_14_]] : (tensor<1x2048xf32>) -> tensor<2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_17_:%.+]] = "mhlo.slice"([[VAR_15_]]) {limit_indices = dense<256> : tensor<1xi64>, start_indices = dense<0> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_18_:%.+]] = "mhlo.slice"([[VAR_15_]]) {limit_indices = dense<512> : tensor<1xi64>, start_indices = dense<256> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_19_:%.+]] = "mhlo.slice"([[VAR_15_]]) {limit_indices = dense<768> : tensor<1xi64>, start_indices = dense<512> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_20_:%.+]] = "mhlo.slice"([[VAR_15_]]) {limit_indices = dense<1024> : tensor<1xi64>, start_indices = dense<768> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_21_:%.+]] = "mhlo.slice"([[VAR_15_]]) {limit_indices = dense<1280> : tensor<1xi64>, start_indices = dense<1024> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_22_:%.+]] = "mhlo.slice"([[VAR_15_]]) {limit_indices = dense<1536> : tensor<1xi64>, start_indices = dense<1280> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_23_:%.+]] = "mhlo.slice"([[VAR_15_]]) {limit_indices = dense<1792> : tensor<1xi64>, start_indices = dense<1536> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_24_:%.+]] = "mhlo.slice"([[VAR_15_]]) {limit_indices = dense<2048> : tensor<1xi64>, start_indices = dense<1792> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_25_:%.+]] = "mhlo.slice"([[VAR_16_]]) {limit_indices = dense<256> : tensor<1xi64>, start_indices = dense<0> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_26_:%.+]] = "mhlo.slice"([[VAR_16_]]) {limit_indices = dense<512> : tensor<1xi64>, start_indices = dense<256> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_27_:%.+]] = "mhlo.slice"([[VAR_16_]]) {limit_indices = dense<768> : tensor<1xi64>, start_indices = dense<512> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_28_:%.+]] = "mhlo.slice"([[VAR_16_]]) {limit_indices = dense<1024> : tensor<1xi64>, start_indices = dense<768> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_29_:%.+]] = "mhlo.slice"([[VAR_16_]]) {limit_indices = dense<1280> : tensor<1xi64>, start_indices = dense<1024> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_30_:%.+]] = "mhlo.slice"([[VAR_16_]]) {limit_indices = dense<1536> : tensor<1xi64>, start_indices = dense<1280> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_31_:%.+]] = "mhlo.slice"([[VAR_16_]]) {limit_indices = dense<1792> : tensor<1xi64>, start_indices = dense<1536> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_32_:%.+]] = "mhlo.slice"([[VAR_16_]]) {limit_indices = dense<2048> : tensor<1xi64>, start_indices = dense<1792> : tensor<1xi64>, strides = dense<1> : tensor<1xi64>} : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_33_:%.+]] = "mhlo.slice"([[PARAM_0_]]) {limit_indices = dense<[1, 16, 512]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x16x512xf32>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_34_:%.+]] = mhlo.reshape [[VAR_33_]] : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_35_:%.+]] = "mhlo.dot"([[VAR_34_]], [[VAR_9_]]) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_36_:%.+]] = "mhlo.dot"([[VAR_0_]], [[VAR_10_]]) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_37_:%.+]] = mhlo.add [[VAR_35_]], [[VAR_36_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_38_:%.+]] = "mhlo.slice"([[VAR_37_]]) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_39_:%.+]] = "mhlo.slice"([[VAR_37_]]) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_40_:%.+]] = "mhlo.slice"([[VAR_37_]]) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_41_:%.+]] = "mhlo.slice"([[VAR_37_]]) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_42_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_17_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_43_:%.+]] = mhlo.add [[VAR_38_]], [[VAR_42_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_44_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_21_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_45_:%.+]] = mhlo.add [[VAR_43_]], [[VAR_44_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_46_:%.+]] = mhlo.logistic [[VAR_45_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_47_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_19_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_48_:%.+]] = mhlo.add [[VAR_40_]], [[VAR_47_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_49_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_23_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_50_:%.+]] = mhlo.add [[VAR_48_]], [[VAR_49_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_51_:%.+]] = mhlo.logistic [[VAR_50_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_52_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_20_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_53_:%.+]] = mhlo.add [[VAR_41_]], [[VAR_52_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_54_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_24_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_55_:%.+]] = mhlo.add [[VAR_53_]], [[VAR_54_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_56_:%.+]] = mhlo.tanh [[VAR_55_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_57_:%.+]] = mhlo.multiply [[VAR_51_]], [[VAR_0_]] : tensor<16x256xf32>
+// CHECK:           [[VAR_58_:%.+]] = mhlo.multiply [[VAR_46_]], [[VAR_56_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_59_:%.+]] = mhlo.add [[VAR_57_]], [[VAR_58_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_60_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_18_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_61_:%.+]] = mhlo.add [[VAR_39_]], [[VAR_60_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_62_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_22_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_63_:%.+]] = mhlo.add [[VAR_61_]], [[VAR_62_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_64_:%.+]] = mhlo.logistic [[VAR_63_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_65_:%.+]] = mhlo.tanh [[VAR_59_]] : tensor<16x256xf32>
+// CHECK:           [[VAR_66_:%.+]] = mhlo.multiply [[VAR_64_]], [[VAR_65_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_67_:%.+]] = mhlo.reshape [[VAR_66_]] : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_68_:%.+]] = "mhlo.slice"([[PARAM_0_]]) {limit_indices = dense<[2, 16, 512]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x16x512xf32>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_69_:%.+]] = mhlo.reshape [[VAR_68_]] : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_70_:%.+]] = "mhlo.dot"([[VAR_69_]], [[VAR_9_]]) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_71_:%.+]] = "mhlo.dot"([[VAR_66_]], [[VAR_10_]]) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_72_:%.+]] = mhlo.add [[VAR_70_]], [[VAR_71_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_73_:%.+]] = "mhlo.slice"([[VAR_72_]]) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_74_:%.+]] = "mhlo.slice"([[VAR_72_]]) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_75_:%.+]] = "mhlo.slice"([[VAR_72_]]) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_76_:%.+]] = "mhlo.slice"([[VAR_72_]]) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_77_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_17_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_78_:%.+]] = mhlo.add [[VAR_73_]], [[VAR_77_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_79_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_21_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_80_:%.+]] = mhlo.add [[VAR_78_]], [[VAR_79_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_81_:%.+]] = mhlo.logistic [[VAR_80_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_82_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_19_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_83_:%.+]] = mhlo.add [[VAR_75_]], [[VAR_82_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_84_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_23_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_85_:%.+]] = mhlo.add [[VAR_83_]], [[VAR_84_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_86_:%.+]] = mhlo.logistic [[VAR_85_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_87_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_20_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_88_:%.+]] = mhlo.add [[VAR_76_]], [[VAR_87_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_89_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_24_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_90_:%.+]] = mhlo.add [[VAR_88_]], [[VAR_89_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_91_:%.+]] = mhlo.tanh [[VAR_90_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_92_:%.+]] = mhlo.multiply [[VAR_86_]], [[VAR_59_]] : tensor<16x256xf32>
+// CHECK:           [[VAR_93_:%.+]] = mhlo.multiply [[VAR_81_]], [[VAR_91_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_94_:%.+]] = mhlo.add [[VAR_92_]], [[VAR_93_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_95_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_18_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_96_:%.+]] = mhlo.add [[VAR_74_]], [[VAR_95_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_97_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_22_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_98_:%.+]] = mhlo.add [[VAR_96_]], [[VAR_97_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_99_:%.+]] = mhlo.logistic [[VAR_98_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_100_:%.+]] = mhlo.tanh [[VAR_94_]] : tensor<16x256xf32>
+// CHECK:           [[VAR_101_:%.+]] = mhlo.multiply [[VAR_99_]], [[VAR_100_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_102_:%.+]] = mhlo.reshape [[VAR_101_]] : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_103_:%.+]] = "mhlo.slice"([[PARAM_0_]]) {limit_indices = dense<[2, 16, 512]> : tensor<3xi64>, start_indices = dense<[1, 0, 0]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x16x512xf32>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_104_:%.+]] = mhlo.reshape [[VAR_103_]] : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_105_:%.+]] = "mhlo.dot"([[VAR_104_]], [[VAR_11_]]) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_106_:%.+]] = "mhlo.dot"([[VAR_0_]], [[VAR_12_]]) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_107_:%.+]] = mhlo.add [[VAR_105_]], [[VAR_106_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_108_:%.+]] = "mhlo.slice"([[VAR_107_]]) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_109_:%.+]] = "mhlo.slice"([[VAR_107_]]) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_110_:%.+]] = "mhlo.slice"([[VAR_107_]]) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_111_:%.+]] = "mhlo.slice"([[VAR_107_]]) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_112_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_25_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_113_:%.+]] = mhlo.add [[VAR_108_]], [[VAR_112_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_114_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_29_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_115_:%.+]] = mhlo.add [[VAR_113_]], [[VAR_114_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_116_:%.+]] = mhlo.logistic [[VAR_115_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_117_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_27_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_118_:%.+]] = mhlo.add [[VAR_110_]], [[VAR_117_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_119_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_31_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_120_:%.+]] = mhlo.add [[VAR_118_]], [[VAR_119_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_121_:%.+]] = mhlo.logistic [[VAR_120_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_122_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_28_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_123_:%.+]] = mhlo.add [[VAR_111_]], [[VAR_122_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_124_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_32_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_125_:%.+]] = mhlo.add [[VAR_123_]], [[VAR_124_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_126_:%.+]] = mhlo.tanh [[VAR_125_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_127_:%.+]] = mhlo.multiply [[VAR_121_]], [[VAR_0_]] : tensor<16x256xf32>
+// CHECK:           [[VAR_128_:%.+]] = mhlo.multiply [[VAR_116_]], [[VAR_126_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_129_:%.+]] = mhlo.add [[VAR_127_]], [[VAR_128_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_130_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_26_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_131_:%.+]] = mhlo.add [[VAR_109_]], [[VAR_130_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_132_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_30_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_133_:%.+]] = mhlo.add [[VAR_131_]], [[VAR_132_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_134_:%.+]] = mhlo.logistic [[VAR_133_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_135_:%.+]] = mhlo.tanh [[VAR_129_]] : tensor<16x256xf32>
+// CHECK:           [[VAR_136_:%.+]] = mhlo.multiply [[VAR_134_]], [[VAR_135_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_137_:%.+]] = mhlo.reshape [[VAR_136_]] : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_138_:%.+]] = "mhlo.slice"([[PARAM_0_]]) {limit_indices = dense<[1, 16, 512]> : tensor<3xi64>, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>} : (tensor<2x16x512xf32>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_139_:%.+]] = mhlo.reshape [[VAR_138_]] : (tensor<1x16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_140_:%.+]] = "mhlo.dot"([[VAR_139_]], [[VAR_11_]]) : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_141_:%.+]] = "mhlo.dot"([[VAR_136_]], [[VAR_12_]]) : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_142_:%.+]] = mhlo.add [[VAR_140_]], [[VAR_141_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_143_:%.+]] = "mhlo.slice"([[VAR_142_]]) {limit_indices = dense<[16, 256]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_144_:%.+]] = "mhlo.slice"([[VAR_142_]]) {limit_indices = dense<[16, 512]> : tensor<2xi64>, start_indices = dense<[0, 256]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_145_:%.+]] = "mhlo.slice"([[VAR_142_]]) {limit_indices = dense<[16, 768]> : tensor<2xi64>, start_indices = dense<[0, 512]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_146_:%.+]] = "mhlo.slice"([[VAR_142_]]) {limit_indices = dense<[16, 1024]> : tensor<2xi64>, start_indices = dense<[0, 768]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<16x1024xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_147_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_25_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_148_:%.+]] = mhlo.add [[VAR_143_]], [[VAR_147_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_149_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_29_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_150_:%.+]] = mhlo.add [[VAR_148_]], [[VAR_149_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_151_:%.+]] = mhlo.logistic [[VAR_150_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_152_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_27_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_153_:%.+]] = mhlo.add [[VAR_145_]], [[VAR_152_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_154_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_31_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_155_:%.+]] = mhlo.add [[VAR_153_]], [[VAR_154_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_156_:%.+]] = mhlo.logistic [[VAR_155_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_157_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_28_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_158_:%.+]] = mhlo.add [[VAR_146_]], [[VAR_157_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_159_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_32_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_160_:%.+]] = mhlo.add [[VAR_158_]], [[VAR_159_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_161_:%.+]] = mhlo.tanh [[VAR_160_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_162_:%.+]] = mhlo.multiply [[VAR_156_]], [[VAR_129_]] : tensor<16x256xf32>
+// CHECK:           [[VAR_163_:%.+]] = mhlo.multiply [[VAR_151_]], [[VAR_161_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_164_:%.+]] = mhlo.add [[VAR_162_]], [[VAR_163_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_165_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_26_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_166_:%.+]] = mhlo.add [[VAR_144_]], [[VAR_165_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_167_:%.+]] = "mhlo.broadcast_in_dim"([[VAR_30_]]) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<256xf32>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_168_:%.+]] = mhlo.add [[VAR_166_]], [[VAR_167_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_169_:%.+]] = mhlo.logistic [[VAR_168_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_170_:%.+]] = mhlo.tanh [[VAR_164_]] : tensor<16x256xf32>
+// CHECK:           [[VAR_171_:%.+]] = mhlo.multiply [[VAR_169_]], [[VAR_170_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_172_:%.+]] = mhlo.reshape [[VAR_171_]] : (tensor<16x256xf32>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_173_:%.+]] = "mhlo.concatenate"([[VAR_67_]], [[VAR_102_]]) {dimension = 0 : i64} : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
+// CHECK:           [[VAR_174_:%.+]] = "mhlo.concatenate"([[VAR_172_]], [[VAR_137_]]) {dimension = 0 : i64} : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
+// CHECK:           [[VAR_175_:%.+]] = "mhlo.concatenate"([[VAR_173_]], [[VAR_174_]]) {dimension = 1 : i64} : (tensor<2x1x16x256xf32>, tensor<2x1x16x256xf32>) -> tensor<2x2x16x256xf32>
+// CHECK:           return [[VAR_175_]] : tensor<2x2x16x256xf32>
+// CHECK:         }
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Tensor/DepthToSpace.mlir b/test/mlir/conversion/onnx_to_mhlo/Tensor/DepthToSpace.mlir
new file mode 100644
index 00000000..0eb34ad8
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/Tensor/DepthToSpace.mlir
@@ -0,0 +1,47 @@
+// RUN: onnx-mlir-opt --convert-onnx-to-mhlo --canonicalize %s -split-input-file | FileCheck %s
+
+func.func @test_depth_to_space(%arg0 : tensor<2x16x20x20xf32>) -> tensor<2x4x40x40xf32> {
+  %0 = "onnx.DepthToSpace"(%arg0) {blocksize = 2 : si64, mode = "CRD"} : (tensor<2x16x20x20xf32>) -> tensor<2x4x40x40xf32>
+  "func.return"(%0) : (tensor<2x4x40x40xf32>) -> ()
+  // CHECK-LABEL:  func.func @test_depth_to_space
+  // CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<2x16x20x20xf32>) -> tensor<2x4x40x40xf32> {
+  // CHECK:           [[VAR_0_:%.+]] = mhlo.reshape [[PARAM_0_]] : (tensor<2x16x20x20xf32>) -> tensor<2x4x2x2x20x20xf32>
+  // CHECK:           [[VAR_1_:%.+]] = "mhlo.transpose"([[VAR_0_]]) {permutation = dense<[0, 1, 4, 2, 5, 3]> : tensor<6xi64>} : (tensor<2x4x2x2x20x20xf32>) -> tensor<2x4x20x2x20x2xf32>
+  // CHECK:           [[VAR_2_:%.+]] = mhlo.reshape [[VAR_1_]] : (tensor<2x4x20x2x20x2xf32>) -> tensor<2x4x40x40xf32>
+  // CHECK:           return [[VAR_2_]] : tensor<2x4x40x40xf32>
+  // CHECK:         }
+}
+
+// -----
+
+func.func @test_depth_to_space_dynamic(%arg0 : tensor<2x?x20x?xf32>) -> tensor<2x?x40x?xf32> {
+  %0 = "onnx.DepthToSpace"(%arg0) {blocksize = 2 : si64, mode = "CRD"} : (tensor<2x?x20x?xf32>) -> tensor<2x?x40x?xf32>
+  "func.return"(%0) : (tensor<2x?x40x?xf32>) -> ()
+// CHECK-DAG:   [[MAP_0_:#.+]] = affine_map<()[s0] -> (s0 floordiv 4)>
+// CHECK-DAG:   [[MAP_1_:#.+]] = affine_map<()[s0] -> (s0 * 2)>
+// CHECK-LABEL:  func.func @test_depth_to_space_dynamic
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<2x?x20x?xf32>) -> tensor<2x?x40x?xf32> {
+// CHECK-DAG:       [[CST_40_:%.+]] = arith.constant 40 : index
+// CHECK-DAG:       [[CST_3_:%.+]] = arith.constant 3 : index
+// CHECK-DAG:       [[CST_20_:%.+]] = arith.constant 20 : index
+// CHECK-DAG:       [[CST_1_:%.+]] = arith.constant 1 : index
+// CHECK-DAG:       [[CST_2_:%.+]] = arith.constant 2 : index
+// CHECK-DAG:       [[VAR_0_:%.+]] = shape.shape_of [[PARAM_0_]] : tensor<2x?x20x?xf32> -> tensor<4xindex>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_1_:%.+]] = shape.get_extent [[VAR_0_]], [[CST_1_]] : tensor<4xindex>, index -> index
+// CHECK-DAG:       [[VAR_2_:%.+]] = shape.shape_of [[PARAM_0_]] : tensor<2x?x20x?xf32> -> tensor<4xindex>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_3_:%.+]] = shape.get_extent [[VAR_2_]], [[CST_3_]] : tensor<4xindex>, index -> index
+// CHECK-DAG:       [[VAR_4_:%.+]] = affine.apply [[MAP_0_]](){{.}}[[VAR_1_]]{{.}}
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_5_:%.+]] = affine.apply [[MAP_1_]](){{.}}[[VAR_3_]]{{.}}
+// CHECK-DAG:       [[VAR_6_:%.+]] = shape.from_extents [[CST_2_]], [[VAR_4_]], [[CST_2_]], [[CST_2_]], [[CST_2_]]0, [[VAR_3_]] : index, index, index, index, index, index
+// CHECK:           [[VAR_7_:%.+]] = shape.to_extent_tensor [[VAR_6_]] : !shape.shape -> tensor<6xindex>
+// CHECK:           [[VAR_8_:%.+]] = mhlo.dynamic_reshape [[PARAM_0_]], [[VAR_7_]] : (tensor<2x?x20x?xf32>, tensor<6xindex>) -> tensor<2x?x2x2x20x?xf32>
+// CHECK-DAG:       [[VAR_9_:%.+]] = "mhlo.transpose"([[VAR_8_]]) {permutation = dense<[0, 1, 4, 2, 5, 3]> : tensor<6xi64>} : (tensor<2x?x2x2x20x?xf32>) -> tensor<2x?x20x2x?x2xf32>
+// CHECK-DAG:       [[VAR_10_:%.+]] = shape.from_extents [[CST_2_]], [[VAR_4_]], [[CST_40_]], [[VAR_5_]] : index, index, index, index
+// CHECK:           [[VAR_11_:%.+]] = shape.to_extent_tensor [[VAR_10_]] : !shape.shape -> tensor<4xindex>
+// CHECK:           [[VAR_12_:%.+]] = mhlo.dynamic_reshape [[VAR_9_]], [[VAR_11_]] : (tensor<2x?x20x2x?x2xf32>, tensor<4xindex>) -> tensor<2x?x40x?xf32>
+// CHECK:           return [[VAR_12_]] : tensor<2x?x40x?xf32>
+// CHECK:         }
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir b/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir
new file mode 100644
index 00000000..a4648439
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir
@@ -0,0 +1,19 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-mhlo %s --canonicalize -split-input-file | FileCheck %s
+
+func.func @test_onehot(%arg0 : tensor<2x3x4xi64>) -> tensor<*xi64> {
+  %0 = onnx.Constant dense<64> : tensor<1xi64>
+  %1 = onnx.Constant dense<[0, 1]> : tensor<2xi64>
+  %2 = "onnx.OneHot"(%arg0, %0, %1) {axis = -1 : si64} : (tensor<2x3x4xi64>, tensor<1xi64>, tensor<2xi64>) -> tensor<*xi64>
+  "func.return"(%2) : (tensor<*xi64>) -> ()
+// CHECK-LABEL: func.func @test_onehot
+// CHECK-SAME: (%[[ARG0:.+]]: tensor<2x3x4xi64>) -> tensor<2x3x4x64xi64> {
+// CHECK: %[[IOTA:.+]] = "mhlo.iota"() {iota_dimension = 0 : i64} : () -> tensor<64xi64>
+// CHECK: %[[BCAST_IOTA:.+]] = "mhlo.broadcast_in_dim"(%[[IOTA]]) {broadcast_dimensions = dense<3> : tensor<1xi64>} : (tensor<64xi64>) -> tensor<2x3x4x64xi64>
+// CHECK: %[[BCAST_ARG0:.+]] = "mhlo.broadcast_in_dim"(%[[ARG0]]) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<2x3x4xi64>) -> tensor<2x3x4x64xi64>
+// CHECK: %[[GE_ZERO:.+]] = mhlo.compare  GE, %[[BCAST_ARG0]], %[[BCAST_ZERO:.+]],  NOTYPE : (tensor<2x3x4x64xi64>, tensor<2x3x4x64xi64>) -> tensor<2x3x4x64xi1>
+// CHECK: %[[POS_ARG:.+]] = mhlo.add %[[BCAST_ARG0]], %[[BCAST_DEPTH:.+]] : tensor<2x3x4x64xi64>
+// CHECK: %[[NORM_ARG:.+]] = mhlo.select %[[GE_ZERO]], %[[BCAST_ARG0]], %[[POS_ARG]] : tensor<2x3x4x64xi1>, tensor<2x3x4x64xi64>
+// CHECK: %[[COMPARE:.+]] = mhlo.compare EQ, %[[NORM_ARG]], %[[BCAST_IOTA]], NOTYPE : (tensor<2x3x4x64xi64>, tensor<2x3x4x64xi64>) -> tensor<2x3x4x64xi1>
+// CHECK: %[[RESULT:.+]] = mhlo.select %[[COMPARE]], %[[ON_VALUE:.+]], %[[OFF_VALUE:.+]] : tensor<2x3x4x64xi1>, tensor<2x3x4x64xi64>
+// CHECK: return %[[RESULT]] : tensor<2x3x4x64xi64>
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir b/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir
new file mode 100644
index 00000000..bffb2b21
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir
@@ -0,0 +1,23 @@
+// RUN: onnx-mlir-opt --canonicalize --convert-onnx-to-mhlo %s -split-input-file | FileCheck %s
+
+func.func @test_scatternd_1(%arg0 : tensor<8xf32>, %arg1 : tensor<4x1xi64>, %arg2 : tensor<4xf32>) -> tensor<8xf32> {
+  %0 = "onnx.ScatterND"(%arg0, %arg1, %arg2) : (tensor<8xf32>, tensor<4x1xi64>, tensor<4xf32>) -> tensor<8xf32>
+  return %0 : tensor<8xf32>
+// CHECK-LABEL: func.func @test_scatternd_1(%arg0: tensor<8xf32>, %arg1: tensor<4x1xi64>, %arg2: tensor<4xf32>) -> tensor<8xf32> {
+// CHECK-NEXT:      %0 = "mhlo.scatter"(%arg0, %arg1, %arg2) ({
+// CHECK-NEXT:      ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+// CHECK-NEXT:        mhlo.return %arg4 : tensor<f32>
+// CHECK-NEXT:      }) {indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<8xf32>, tensor<4x1xi64>, tensor<4xf32>) -> tensor<8xf32>
+// CHECK-NEXT:      return %0 : tensor<8xf32>
+}
+
+func.func @test_scatternd_2(%arg0 : tensor<4x4x4xi32>, %arg1 : tensor<2x1xi64>, %arg2 : tensor<2x4x4xi32>) -> tensor<4x4x4xi32> {
+  %0 = "onnx.ScatterND"(%arg0, %arg1, %arg2) : (tensor<4x4x4xi32>, tensor<2x1xi64>, tensor<2x4x4xi32>) -> tensor<4x4x4xi32>
+  return %0 : tensor<4x4x4xi32>
+// CHECK-LABEL: func.func @test_scatternd_2(%arg0: tensor<4x4x4xi32>, %arg1: tensor<2x1xi64>, %arg2: tensor<2x4x4xi32>) -> tensor<4x4x4xi32> {
+// CHECK-NEXT:      %0 = "mhlo.scatter"(%arg0, %arg1, %arg2) ({
+// CHECK-NEXT:      ^bb0(%arg3: tensor<i32>, %arg4: tensor<i32>):
+// CHECK-NEXT:        mhlo.return %arg4 : tensor<i32>
+// CHECK-NEXT:      }) {indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<4x4x4xi32>, tensor<2x1xi64>, tensor<2x4x4xi32>) -> tensor<4x4x4xi32>
+// CHECK-NEXT:      return %0 : tensor<4x4x4xi32>
+}
