// RUN: byteir-opt %s  | FileCheck %s

// CHECK-LABEL: func.func @main
module @IrToMhlo.2452 attributes {byre.container_module, gpu.container_module} {
  func.func @main(%arg0: memref<4x3x224x224xf32, "cuda"> {byre.argname = "Input0", byre.argtype = 1 : i32}, %arg1: memref<4x1000xf32, "cuda"> {byre.argname = "Input1", byre.argtype = 1 : i32}, %arg2: memref<64x3x7x7xf32, "cuda"> {byre.argname = "Input2", byre.argtype = 1 : i32}, %arg3: memref<64xf32, "cuda"> {byre.argname = "Input3", byre.argtype = 1 : i32}, %arg4: memref<64xf32, "cuda"> {byre.argname = "Input4", byre.argtype = 1 : i32}, %arg5: memref<64xf32, "cuda"> {byre.argname = "Input5", byre.argtype = 1 : i32}, %arg6: memref<64xf32, "cuda"> {byre.argname = "Input6", byre.argtype = 1 : i32}, %arg7: memref<64x64x3x3xf32, "cuda"> {byre.argname = "Input7", byre.argtype = 1 : i32}, %arg8: memref<64xf32, "cuda"> {byre.argname = "Input8", byre.argtype = 1 : i32}, %arg9: memref<64xf32, "cuda"> {byre.argname = "Input9", byre.argtype = 1 : i32}, %arg10: memref<64xf32, "cuda"> {byre.argname = "Input10", byre.argtype = 1 : i32}, %arg11: memref<64xf32, "cuda"> {byre.argname = "Input11", byre.argtype = 1 : i32}, %arg12: memref<64x64x3x3xf32, "cuda"> {byre.argname = "Input12", byre.argtype = 1 : i32}, %arg13: memref<64xf32, "cuda"> {byre.argname = "Input13", byre.argtype = 1 : i32}, %arg14: memref<64xf32, "cuda"> {byre.argname = "Input14", byre.argtype = 1 : i32}, %arg15: memref<64xf32, "cuda"> {byre.argname = "Input15", byre.argtype = 1 : i32}, %arg16: memref<64xf32, "cuda"> {byre.argname = "Input16", byre.argtype = 1 : i32}, %arg17: memref<64x64x3x3xf32, "cuda"> {byre.argname = "Input17", byre.argtype = 1 : i32}, %arg18: memref<64xf32, "cuda"> {byre.argname = "Input18", byre.argtype = 1 : i32}, %arg19: memref<64xf32, "cuda"> {byre.argname = "Input19", byre.argtype = 1 : i32}, %arg20: memref<64xf32, "cuda"> {byre.argname = "Input20", byre.argtype = 1 : i32}, %arg21: memref<64xf32, "cuda"> {byre.argname = "Input21", byre.argtype = 1 : i32}, %arg22: memref<64x64x3x3xf32, "cuda"> {byre.argname = "Input22", byre.argtype = 1 : i32}, %arg23: memref<64xf32, "cuda"> {byre.argname = "Input23", byre.argtype = 1 : i32}, %arg24: memref<64xf32, "cuda"> {byre.argname = "Input24", byre.argtype = 1 : i32}, %arg25: memref<64xf32, "cuda"> {byre.argname = "Input25", byre.argtype = 1 : i32}, %arg26: memref<64xf32, "cuda"> {byre.argname = "Input26", byre.argtype = 1 : i32}, %arg27: memref<128x64x3x3xf32, "cuda"> {byre.argname = "Input27", byre.argtype = 1 : i32}, %arg28: memref<128xf32, "cuda"> {byre.argname = "Input28", byre.argtype = 1 : i32}, %arg29: memref<128xf32, "cuda"> {byre.argname = "Input29", byre.argtype = 1 : i32}, %arg30: memref<128xf32, "cuda"> {byre.argname = "Input30", byre.argtype = 1 : i32}, %arg31: memref<128xf32, "cuda"> {byre.argname = "Input31", byre.argtype = 1 : i32}, %arg32: memref<128x128x3x3xf32, "cuda"> {byre.argname = "Input32", byre.argtype = 1 : i32}, %arg33: memref<128xf32, "cuda"> {byre.argname = "Input33", byre.argtype = 1 : i32}, %arg34: memref<128xf32, "cuda"> {byre.argname = "Input34", byre.argtype = 1 : i32}, %arg35: memref<128xf32, "cuda"> {byre.argname = "Input35", byre.argtype = 1 : i32}, %arg36: memref<128xf32, "cuda"> {byre.argname = "Input36", byre.argtype = 1 : i32}, %arg37: memref<128x64x1x1xf32, "cuda"> {byre.argname = "Input37", byre.argtype = 1 : i32}, %arg38: memref<128xf32, "cuda"> {byre.argname = "Input38", byre.argtype = 1 : i32}, %arg39: memref<128xf32, "cuda"> {byre.argname = "Input39", byre.argtype = 1 : i32}, %arg40: memref<128xf32, "cuda"> {byre.argname = "Input40", byre.argtype = 1 : i32}, %arg41: memref<128xf32, "cuda"> {byre.argname = "Input41", byre.argtype = 1 : i32}, %arg42: memref<128x128x3x3xf32, "cuda"> {byre.argname = "Input42", byre.argtype = 1 : i32}, %arg43: memref<128xf32, "cuda"> {byre.argname = "Input43", byre.argtype = 1 : i32}, %arg44: memref<128xf32, "cuda"> {byre.argname = "Input44", byre.argtype = 1 : i32}, %arg45: memref<128xf32, "cuda"> {byre.argname = "Input45", byre.argtype = 1 : i32}, %arg46: memref<128xf32, "cuda"> {byre.argname = "Input46", byre.argtype = 1 : i32}, %arg47: memref<128x128x3x3xf32, "cuda"> {byre.argname = "Input47", byre.argtype = 1 : i32}, %arg48: memref<128xf32, "cuda"> {byre.argname = "Input48", byre.argtype = 1 : i32}, %arg49: memref<128xf32, "cuda"> {byre.argname = "Input49", byre.argtype = 1 : i32}, %arg50: memref<128xf32, "cuda"> {byre.argname = "Input50", byre.argtype = 1 : i32}, %arg51: memref<128xf32, "cuda"> {byre.argname = "Input51", byre.argtype = 1 : i32}, %arg52: memref<256x128x3x3xf32, "cuda"> {byre.argname = "Input52", byre.argtype = 1 : i32}, %arg53: memref<256xf32, "cuda"> {byre.argname = "Input53", byre.argtype = 1 : i32}, %arg54: memref<256xf32, "cuda"> {byre.argname = "Input54", byre.argtype = 1 : i32}, %arg55: memref<256xf32, "cuda"> {byre.argname = "Input55", byre.argtype = 1 : i32}, %arg56: memref<256xf32, "cuda"> {byre.argname = "Input56", byre.argtype = 1 : i32}, %arg57: memref<256x256x3x3xf32, "cuda"> {byre.argname = "Input57", byre.argtype = 1 : i32}, %arg58: memref<256xf32, "cuda"> {byre.argname = "Input58", byre.argtype = 1 : i32}, %arg59: memref<256xf32, "cuda"> {byre.argname = "Input59", byre.argtype = 1 : i32}, %arg60: memref<256xf32, "cuda"> {byre.argname = "Input60", byre.argtype = 1 : i32}, %arg61: memref<256xf32, "cuda"> {byre.argname = "Input61", byre.argtype = 1 : i32}, %arg62: memref<256x128x1x1xf32, "cuda"> {byre.argname = "Input62", byre.argtype = 1 : i32}, %arg63: memref<256xf32, "cuda"> {byre.argname = "Input63", byre.argtype = 1 : i32}, %arg64: memref<256xf32, "cuda"> {byre.argname = "Input64", byre.argtype = 1 : i32}, %arg65: memref<256xf32, "cuda"> {byre.argname = "Input65", byre.argtype = 1 : i32}, %arg66: memref<256xf32, "cuda"> {byre.argname = "Input66", byre.argtype = 1 : i32}, %arg67: memref<256x256x3x3xf32, "cuda"> {byre.argname = "Input67", byre.argtype = 1 : i32}, %arg68: memref<256xf32, "cuda"> {byre.argname = "Input68", byre.argtype = 1 : i32}, %arg69: memref<256xf32, "cuda"> {byre.argname = "Input69", byre.argtype = 1 : i32}, %arg70: memref<256xf32, "cuda"> {byre.argname = "Input70", byre.argtype = 1 : i32}, %arg71: memref<256xf32, "cuda"> {byre.argname = "Input71", byre.argtype = 1 : i32}, %arg72: memref<256x256x3x3xf32, "cuda"> {byre.argname = "Input72", byre.argtype = 1 : i32}, %arg73: memref<256xf32, "cuda"> {byre.argname = "Input73", byre.argtype = 1 : i32}, %arg74: memref<256xf32, "cuda"> {byre.argname = "Input74", byre.argtype = 1 : i32}, %arg75: memref<256xf32, "cuda"> {byre.argname = "Input75", byre.argtype = 1 : i32}, %arg76: memref<256xf32, "cuda"> {byre.argname = "Input76", byre.argtype = 1 : i32}, %arg77: memref<512x256x3x3xf32, "cuda"> {byre.argname = "Input77", byre.argtype = 1 : i32}, %arg78: memref<512xf32, "cuda"> {byre.argname = "Input78", byre.argtype = 1 : i32}, %arg79: memref<512xf32, "cuda"> {byre.argname = "Input79", byre.argtype = 1 : i32}, %arg80: memref<512xf32, "cuda"> {byre.argname = "Input80", byre.argtype = 1 : i32}, %arg81: memref<512xf32, "cuda"> {byre.argname = "Input81", byre.argtype = 1 : i32}, %arg82: memref<512x512x3x3xf32, "cuda"> {byre.argname = "Input82", byre.argtype = 1 : i32}, %arg83: memref<512xf32, "cuda"> {byre.argname = "Input83", byre.argtype = 1 : i32}, %arg84: memref<512xf32, "cuda"> {byre.argname = "Input84", byre.argtype = 1 : i32}, %arg85: memref<512xf32, "cuda"> {byre.argname = "Input85", byre.argtype = 1 : i32}, %arg86: memref<512xf32, "cuda"> {byre.argname = "Input86", byre.argtype = 1 : i32}, %arg87: memref<512x256x1x1xf32, "cuda"> {byre.argname = "Input87", byre.argtype = 1 : i32}, %arg88: memref<512xf32, "cuda"> {byre.argname = "Input88", byre.argtype = 1 : i32}, %arg89: memref<512xf32, "cuda"> {byre.argname = "Input89", byre.argtype = 1 : i32}, %arg90: memref<512xf32, "cuda"> {byre.argname = "Input90", byre.argtype = 1 : i32}, %arg91: memref<512xf32, "cuda"> {byre.argname = "Input91", byre.argtype = 1 : i32}, %arg92: memref<512x512x3x3xf32, "cuda"> {byre.argname = "Input92", byre.argtype = 1 : i32}, %arg93: memref<512xf32, "cuda"> {byre.argname = "Input93", byre.argtype = 1 : i32}, %arg94: memref<512xf32, "cuda"> {byre.argname = "Input94", byre.argtype = 1 : i32}, %arg95: memref<512xf32, "cuda"> {byre.argname = "Input95", byre.argtype = 1 : i32}, %arg96: memref<512xf32, "cuda"> {byre.argname = "Input96", byre.argtype = 1 : i32}, %arg97: memref<512x512x3x3xf32, "cuda"> {byre.argname = "Input97", byre.argtype = 1 : i32}, %arg98: memref<512xf32, "cuda"> {byre.argname = "Input98", byre.argtype = 1 : i32}, %arg99: memref<512xf32, "cuda"> {byre.argname = "Input99", byre.argtype = 1 : i32}, %arg100: memref<512xf32, "cuda"> {byre.argname = "Input100", byre.argtype = 1 : i32}, %arg101: memref<512xf32, "cuda"> {byre.argname = "Input101", byre.argtype = 1 : i32}, %arg102: memref<1000x512xf32, "cuda"> {byre.argname = "Input102", byre.argtype = 1 : i32}, %arg103: memref<1000xf32, "cuda"> {byre.argname = "Input103", byre.argtype = 1 : i32}, %arg104: memref<f32, "cuda"> {byre.argname = "Output0", byre.argtype = 2 : i32}, %arg105: memref<64x3x7x7xf32, "cuda"> {byre.argname = "Output1", byre.argtype = 2 : i32}, %arg106: memref<64xf32, "cuda"> {byre.argname = "Output2", byre.argtype = 2 : i32}, %arg107: memref<64xf32, "cuda"> {byre.argname = "Output3", byre.argtype = 2 : i32}, %arg108: memref<64x64x3x3xf32, "cuda"> {byre.argname = "Output4", byre.argtype = 2 : i32}, %arg109: memref<64xf32, "cuda"> {byre.argname = "Output5", byre.argtype = 2 : i32}, %arg110: memref<64xf32, "cuda"> {byre.argname = "Output6", byre.argtype = 2 : i32}, %arg111: memref<64x64x3x3xf32, "cuda"> {byre.argname = "Output7", byre.argtype = 2 : i32}, %arg112: memref<64xf32, "cuda"> {byre.argname = "Output8", byre.argtype = 2 : i32}, %arg113: memref<64xf32, "cuda"> {byre.argname = "Output9", byre.argtype = 2 : i32}, %arg114: memref<64x64x3x3xf32, "cuda"> {byre.argname = "Output10", byre.argtype = 2 : i32}, %arg115: memref<64xf32, "cuda"> {byre.argname = "Output11", byre.argtype = 2 : i32}, %arg116: memref<64xf32, "cuda"> {byre.argname = "Output12", byre.argtype = 2 : i32}, %arg117: memref<64x64x3x3xf32, "cuda"> {byre.argname = "Output13", byre.argtype = 2 : i32}, %arg118: memref<64xf32, "cuda"> {byre.argname = "Output14", byre.argtype = 2 : i32}, %arg119: memref<64xf32, "cuda"> {byre.argname = "Output15", byre.argtype = 2 : i32}, %arg120: memref<128x64x3x3xf32, "cuda"> {byre.argname = "Output16", byre.argtype = 2 : i32}, %arg121: memref<128xf32, "cuda"> {byre.argname = "Output17", byre.argtype = 2 : i32}, %arg122: memref<128xf32, "cuda"> {byre.argname = "Output18", byre.argtype = 2 : i32}, %arg123: memref<128x128x3x3xf32, "cuda"> {byre.argname = "Output19", byre.argtype = 2 : i32}, %arg124: memref<128xf32, "cuda"> {byre.argname = "Output20", byre.argtype = 2 : i32}, %arg125: memref<128xf32, "cuda"> {byre.argname = "Output21", byre.argtype = 2 : i32}, %arg126: memref<128x64x1x1xf32, "cuda"> {byre.argname = "Output22", byre.argtype = 2 : i32}, %arg127: memref<128xf32, "cuda"> {byre.argname = "Output23", byre.argtype = 2 : i32}, %arg128: memref<128xf32, "cuda"> {byre.argname = "Output24", byre.argtype = 2 : i32}, %arg129: memref<128x128x3x3xf32, "cuda"> {byre.argname = "Output25", byre.argtype = 2 : i32}, %arg130: memref<128xf32, "cuda"> {byre.argname = "Output26", byre.argtype = 2 : i32}, %arg131: memref<128xf32, "cuda"> {byre.argname = "Output27", byre.argtype = 2 : i32}, %arg132: memref<128x128x3x3xf32, "cuda"> {byre.argname = "Output28", byre.argtype = 2 : i32}, %arg133: memref<128xf32, "cuda"> {byre.argname = "Output29", byre.argtype = 2 : i32}, %arg134: memref<128xf32, "cuda"> {byre.argname = "Output30", byre.argtype = 2 : i32}, %arg135: memref<256x128x3x3xf32, "cuda"> {byre.argname = "Output31", byre.argtype = 2 : i32}, %arg136: memref<256xf32, "cuda"> {byre.argname = "Output32", byre.argtype = 2 : i32}, %arg137: memref<256xf32, "cuda"> {byre.argname = "Output33", byre.argtype = 2 : i32}, %arg138: memref<256x256x3x3xf32, "cuda"> {byre.argname = "Output34", byre.argtype = 2 : i32}, %arg139: memref<256xf32, "cuda"> {byre.argname = "Output35", byre.argtype = 2 : i32}, %arg140: memref<256xf32, "cuda"> {byre.argname = "Output36", byre.argtype = 2 : i32}, %arg141: memref<256x128x1x1xf32, "cuda"> {byre.argname = "Output37", byre.argtype = 2 : i32}, %arg142: memref<256xf32, "cuda"> {byre.argname = "Output38", byre.argtype = 2 : i32}, %arg143: memref<256xf32, "cuda"> {byre.argname = "Output39", byre.argtype = 2 : i32}, %arg144: memref<256x256x3x3xf32, "cuda"> {byre.argname = "Output40", byre.argtype = 2 : i32}, %arg145: memref<256xf32, "cuda"> {byre.argname = "Output41", byre.argtype = 2 : i32}, %arg146: memref<256xf32, "cuda"> {byre.argname = "Output42", byre.argtype = 2 : i32}, %arg147: memref<256x256x3x3xf32, "cuda"> {byre.argname = "Output43", byre.argtype = 2 : i32}, %arg148: memref<256xf32, "cuda"> {byre.argname = "Output44", byre.argtype = 2 : i32}, %arg149: memref<256xf32, "cuda"> {byre.argname = "Output45", byre.argtype = 2 : i32}, %arg150: memref<512x256x3x3xf32, "cuda"> {byre.argname = "Output46", byre.argtype = 2 : i32}, %arg151: memref<512xf32, "cuda"> {byre.argname = "Output47", byre.argtype = 2 : i32}, %arg152: memref<512xf32, "cuda"> {byre.argname = "Output48", byre.argtype = 2 : i32}, %arg153: memref<512x512x3x3xf32, "cuda"> {byre.argname = "Output49", byre.argtype = 2 : i32}, %arg154: memref<512xf32, "cuda"> {byre.argname = "Output50", byre.argtype = 2 : i32}, %arg155: memref<512xf32, "cuda"> {byre.argname = "Output51", byre.argtype = 2 : i32}, %arg156: memref<512x256x1x1xf32, "cuda"> {byre.argname = "Output52", byre.argtype = 2 : i32}, %arg157: memref<512xf32, "cuda"> {byre.argname = "Output53", byre.argtype = 2 : i32}, %arg158: memref<512xf32, "cuda"> {byre.argname = "Output54", byre.argtype = 2 : i32}, %arg159: memref<512x512x3x3xf32, "cuda"> {byre.argname = "Output55", byre.argtype = 2 : i32}, %arg160: memref<512xf32, "cuda"> {byre.argname = "Output56", byre.argtype = 2 : i32}, %arg161: memref<512xf32, "cuda"> {byre.argname = "Output57", byre.argtype = 2 : i32}, %arg162: memref<512x512x3x3xf32, "cuda"> {byre.argname = "Output58", byre.argtype = 2 : i32}, %arg163: memref<512xf32, "cuda"> {byre.argname = "Output59", byre.argtype = 2 : i32}, %arg164: memref<512xf32, "cuda"> {byre.argname = "Output60", byre.argtype = 2 : i32}, %arg165: memref<1000x512xf32, "cuda"> {byre.argname = "Output61", byre.argtype = 2 : i32}, %arg166: memref<1000xf32, "cuda"> {byre.argname = "Output62", byre.argtype = 2 : i32}) attributes {byre.entry_point, device_file_name = "test/test_files/resnet18_fw_bw_device.ptx"} {
    %alloc = memref.alloc() : memref<4096xi8, "cuda">
    %alloc_0 = memref.alloc() : memref<8000xi8, "cuda">
    %alloc_1 = memref.alloc() : memref<12544xi8, "cuda">
    %alloc_2 = memref.alloc() : memref<12544xi8, "cuda">
    %alloc_3 = memref.alloc() : memref<16000xi8, "cuda">
    %alloc_4 = memref.alloc() : memref<16000xi8, "cuda">
    %alloc_5 = memref.alloc() : memref<16384xi8, "cuda">
    %alloc_6 = memref.alloc() : memref<25088xi8, "cuda">
    %alloc_7 = memref.alloc() : memref<25088xi8, "cuda">
    %alloc_8 = memref.alloc() : memref<25088xi8, "cuda">
    %alloc_9 = memref.alloc() : memref<25088xi8, "cuda">
    %alloc_10 = memref.alloc() : memref<50176xi8, "cuda">
    %alloc_11 = memref.alloc() : memref<50176xi8, "cuda">
    %alloc_12 = memref.alloc() : memref<50176xi8, "cuda">
    %alloc_13 = memref.alloc() : memref<50176xi8, "cuda">
    %alloc_14 = memref.alloc() : memref<65536xi8, "cuda">
    %alloc_15 = memref.alloc() : memref<73728xi8, "cuda">
    %alloc_16 = memref.alloc() : memref<73728xi8, "cuda">
    %alloc_17 = memref.alloc() : memref<73728xi8, "cuda">
    %alloc_18 = memref.alloc() : memref<73728xi8, "cuda">
    %alloc_19 = memref.alloc() : memref<100352xi8, "cuda">
    %alloc_20 = memref.alloc() : memref<100352xi8, "cuda">
    %alloc_21 = memref.alloc() : memref<100352xi8, "cuda">
    %alloc_22 = memref.alloc() : memref<100352xi8, "cuda">
    %alloc_23 = memref.alloc() : memref<147456xi8, "cuda">
    %alloc_24 = memref.alloc() : memref<200704xi8, "cuda">
    %alloc_25 = memref.alloc() : memref<200704xi8, "cuda">
    %alloc_26 = memref.alloc() : memref<200704xi8, "cuda">
    %alloc_27 = memref.alloc() : memref<200704xi8, "cuda">
    %alloc_28 = memref.alloc() : memref<200704xi8, "cuda">
    %alloc_29 = memref.alloc() : memref<262144xi8, "cuda">
    %alloc_30 = memref.alloc() : memref<294912xi8, "cuda">
    %alloc_31 = memref.alloc() : memref<294912xi8, "cuda">
    %alloc_32 = memref.alloc() : memref<294912xi8, "cuda">
    %alloc_33 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_34 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_35 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_36 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_37 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_38 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_39 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_40 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_41 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_42 = memref.alloc() : memref<401408xi8, "cuda">
    %alloc_43 = memref.alloc() : memref<589824xi8, "cuda">
    %alloc_44 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_45 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_46 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_47 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_48 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_49 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_50 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_51 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_52 = memref.alloc() : memref<802816xi8, "cuda">
    %alloc_53 = memref.alloc() : memref<1179648xi8, "cuda">
    %alloc_54 = memref.alloc() : memref<1179648xi8, "cuda">
    %alloc_55 = memref.alloc() : memref<1179648xi8, "cuda">
    %alloc_56 = memref.alloc() : memref<1179648xi8, "cuda">
    %alloc_57 = memref.alloc() : memref<1204224xi8, "cuda">
    %alloc_58 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_59 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_60 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_61 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_62 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_63 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_64 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_65 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_66 = memref.alloc() : memref<1605632xi8, "cuda">
    %alloc_67 = memref.alloc() : memref<2359296xi8, "cuda">
    %alloc_68 = memref.alloc() : memref<4718592xi8, "cuda">
    %alloc_69 = memref.alloc() : memref<4718592xi8, "cuda">
    %alloc_70 = memref.alloc() : memref<4718592xi8, "cuda">
    %alloc_71 = memref.alloc() : memref<4718592xi8, "cuda">
    %alloc_72 = memref.alloc() : memref<6422528xi8, "cuda">
    %alloc_73 = memref.alloc() : memref<6422528xi8, "cuda">
    %alloc_74 = memref.alloc() : memref<6422528xi8, "cuda">
    %0 = "byre.alias"(%alloc_57) {device = "cuda", offset = 0 : i64} : (memref<1204224xi8, "cuda">) -> memref<4x3x224x224xf16, "cuda">
    byre.compute @PTXOp(%arg0, %0) {BlockSize.x = 128 : i32, GridSize.x = 4704 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown0", memory_effects = [1 : i32, 2 : i32]} : memref<4x3x224x224xf32, "cuda">, memref<4x3x224x224xf16, "cuda">
    %1 = "byre.alias"(%alloc_73) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<64x3x7x7xf16, "cuda">
    byre.compute @PTXOp(%arg2, %1) {BlockSize.x = 128 : i32, GridSize.x = 74 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown1", memory_effects = [1 : i32, 2 : i32]} : memref<64x3x7x7xf32, "cuda">, memref<64x3x7x7xf16, "cuda">
    %2 = "byre.alias"(%alloc_74) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x64x112x112xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%0, %1, %2) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<3> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x3x224x224xf16, "cuda">, memref<64x3x7x7xf16, "cuda">, memref<4x64x112x112xf16, "cuda">
    %3 = "byre.alias"(%alloc_73) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x64x112x112xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%2, %arg3, %arg4, %3) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x64x112x112xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">, memref<4x64x112x112xf16, "cuda">
    %4 = "byre.alias"(%alloc_15) {device = "cuda", offset = 0 : i64} : (memref<73728xi8, "cuda">) -> memref<64x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg7, %4) {BlockSize.x = 128 : i32, GridSize.x = 288 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown3", memory_effects = [1 : i32, 2 : i32]} : memref<64x64x3x3xf32, "cuda">, memref<64x64x3x3xf16, "cuda">
    %5 = "byre.alias"(%alloc_16) {device = "cuda", offset = 0 : i64} : (memref<73728xi8, "cuda">) -> memref<64x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg12, %5) {BlockSize.x = 128 : i32, GridSize.x = 288 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown4", memory_effects = [1 : i32, 2 : i32]} : memref<64x64x3x3xf32, "cuda">, memref<64x64x3x3xf16, "cuda">
    %6 = "byre.alias"(%alloc_17) {device = "cuda", offset = 0 : i64} : (memref<73728xi8, "cuda">) -> memref<64x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg17, %6) {BlockSize.x = 128 : i32, GridSize.x = 288 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown5", memory_effects = [1 : i32, 2 : i32]} : memref<64x64x3x3xf32, "cuda">, memref<64x64x3x3xf16, "cuda">
    %7 = "byre.alias"(%alloc_18) {device = "cuda", offset = 0 : i64} : (memref<73728xi8, "cuda">) -> memref<64x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg22, %7) {BlockSize.x = 128 : i32, GridSize.x = 288 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown6", memory_effects = [1 : i32, 2 : i32]} : memref<64x64x3x3xf32, "cuda">, memref<64x64x3x3xf16, "cuda">
    %8 = "byre.alias"(%alloc_5) {device = "cuda", offset = 0 : i64} : (memref<16384xi8, "cuda">) -> memref<128x64x1x1xf16, "cuda">
    byre.compute @PTXOp(%arg37, %8) {BlockSize.x = 128 : i32, GridSize.x = 64 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown7", memory_effects = [1 : i32, 2 : i32]} : memref<128x64x1x1xf32, "cuda">, memref<128x64x1x1xf16, "cuda">
    %9 = "byre.alias"(%alloc_23) {device = "cuda", offset = 0 : i64} : (memref<147456xi8, "cuda">) -> memref<128x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg27, %9) {BlockSize.x = 128 : i32, GridSize.x = 576 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown8", memory_effects = [1 : i32, 2 : i32]} : memref<128x64x3x3xf32, "cuda">, memref<128x64x3x3xf16, "cuda">
    %10 = "byre.alias"(%alloc_30) {device = "cuda", offset = 0 : i64} : (memref<294912xi8, "cuda">) -> memref<128x128x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg32, %10) {BlockSize.x = 128 : i32, GridSize.x = 1152 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown9", memory_effects = [1 : i32, 2 : i32]} : memref<128x128x3x3xf32, "cuda">, memref<128x128x3x3xf16, "cuda">
    %11 = "byre.alias"(%alloc_31) {device = "cuda", offset = 0 : i64} : (memref<294912xi8, "cuda">) -> memref<128x128x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg42, %11) {BlockSize.x = 128 : i32, GridSize.x = 1152 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown10", memory_effects = [1 : i32, 2 : i32]} : memref<128x128x3x3xf32, "cuda">, memref<128x128x3x3xf16, "cuda">
    %12 = "byre.alias"(%alloc_32) {device = "cuda", offset = 0 : i64} : (memref<294912xi8, "cuda">) -> memref<128x128x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg47, %12) {BlockSize.x = 128 : i32, GridSize.x = 1152 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown11", memory_effects = [1 : i32, 2 : i32]} : memref<128x128x3x3xf32, "cuda">, memref<128x128x3x3xf16, "cuda">
    %13 = "byre.alias"(%alloc_14) {device = "cuda", offset = 0 : i64} : (memref<65536xi8, "cuda">) -> memref<256x128x1x1xf16, "cuda">
    byre.compute @PTXOp(%arg62, %13) {BlockSize.x = 128 : i32, GridSize.x = 256 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown12", memory_effects = [1 : i32, 2 : i32]} : memref<256x128x1x1xf32, "cuda">, memref<256x128x1x1xf16, "cuda">
    %14 = "byre.alias"(%alloc_43) {device = "cuda", offset = 0 : i64} : (memref<589824xi8, "cuda">) -> memref<256x128x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg52, %14) {BlockSize.x = 128 : i32, GridSize.x = 2304 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown13", memory_effects = [1 : i32, 2 : i32]} : memref<256x128x3x3xf32, "cuda">, memref<256x128x3x3xf16, "cuda">
    %15 = "byre.alias"(%alloc_56) {device = "cuda", offset = 0 : i64} : (memref<1179648xi8, "cuda">) -> memref<256x256x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg57, %15) {BlockSize.x = 128 : i32, GridSize.x = 4608 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown14", memory_effects = [1 : i32, 2 : i32]} : memref<256x256x3x3xf32, "cuda">, memref<256x256x3x3xf16, "cuda">
    %16 = "byre.alias"(%alloc_54) {device = "cuda", offset = 0 : i64} : (memref<1179648xi8, "cuda">) -> memref<256x256x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg67, %16) {BlockSize.x = 128 : i32, GridSize.x = 4608 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown15", memory_effects = [1 : i32, 2 : i32]} : memref<256x256x3x3xf32, "cuda">, memref<256x256x3x3xf16, "cuda">
    %17 = "byre.alias"(%alloc_55) {device = "cuda", offset = 0 : i64} : (memref<1179648xi8, "cuda">) -> memref<256x256x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg72, %17) {BlockSize.x = 128 : i32, GridSize.x = 4608 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown16", memory_effects = [1 : i32, 2 : i32]} : memref<256x256x3x3xf32, "cuda">, memref<256x256x3x3xf16, "cuda">
    %18 = "byre.alias"(%alloc_29) {device = "cuda", offset = 0 : i64} : (memref<262144xi8, "cuda">) -> memref<512x256x1x1xf16, "cuda">
    byre.compute @PTXOp(%arg87, %18) {BlockSize.x = 128 : i32, GridSize.x = 1024 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown17", memory_effects = [1 : i32, 2 : i32]} : memref<512x256x1x1xf32, "cuda">, memref<512x256x1x1xf16, "cuda">
    %19 = "byre.alias"(%alloc_67) {device = "cuda", offset = 0 : i64} : (memref<2359296xi8, "cuda">) -> memref<512x256x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg77, %19) {BlockSize.x = 128 : i32, GridSize.x = 9216 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown18", memory_effects = [1 : i32, 2 : i32]} : memref<512x256x3x3xf32, "cuda">, memref<512x256x3x3xf16, "cuda">
    %20 = "byre.alias"(%alloc_69) {device = "cuda", offset = 0 : i64} : (memref<4718592xi8, "cuda">) -> memref<512x512x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg82, %20) {BlockSize.x = 128 : i32, GridSize.x = 18432 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown19", memory_effects = [1 : i32, 2 : i32]} : memref<512x512x3x3xf32, "cuda">, memref<512x512x3x3xf16, "cuda">
    %21 = "byre.alias"(%alloc_70) {device = "cuda", offset = 0 : i64} : (memref<4718592xi8, "cuda">) -> memref<512x512x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg92, %21) {BlockSize.x = 128 : i32, GridSize.x = 18432 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown20", memory_effects = [1 : i32, 2 : i32]} : memref<512x512x3x3xf32, "cuda">, memref<512x512x3x3xf16, "cuda">
    %22 = "byre.alias"(%alloc_71) {device = "cuda", offset = 0 : i64} : (memref<4718592xi8, "cuda">) -> memref<512x512x3x3xf16, "cuda">
    byre.compute @PTXOp(%arg97, %22) {BlockSize.x = 128 : i32, GridSize.x = 18432 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown21", memory_effects = [1 : i32, 2 : i32]} : memref<512x512x3x3xf32, "cuda">, memref<512x512x3x3xf16, "cuda">
    %23 = "byre.alias"(%alloc_26) {device = "cuda", offset = 0 : i64} : (memref<200704xi8, "cuda">) -> memref<4x1000xf16, "cuda">
    byre.compute @PTXOp(%arg1, %23) {BlockSize.x = 128 : i32, GridSize.x = 32 : i32, arg_ranks = [2 : i32, 2 : i32], device = "cuda", kernel_name = "Unknown22", memory_effects = [1 : i32, 2 : i32]} : memref<4x1000xf32, "cuda">, memref<4x1000xf16, "cuda">
    %24 = "byre.alias"(%alloc_68) {device = "cuda", offset = 0 : i64} : (memref<4718592xi8, "cuda">) -> memref<1000x512xf16, "cuda">
    byre.compute @PTXOp(%arg102, %24) {BlockSize.x = 128 : i32, GridSize.x = 4000 : i32, arg_ranks = [2 : i32, 2 : i32], device = "cuda", kernel_name = "Unknown23", memory_effects = [1 : i32, 2 : i32]} : memref<1000x512xf32, "cuda">, memref<1000x512xf16, "cuda">
    %25 = "byre.alias"(%alloc_26) {device = "cuda", offset = 8000 : i64} : (memref<200704xi8, "cuda">) -> memref<1000xf16, "cuda">
    byre.compute @PTXOp(%arg103, %25) {BlockSize.x = 128 : i32, GridSize.x = 8 : i32, arg_ranks = [1 : i32, 1 : i32], device = "cuda", kernel_name = "Unknown24", memory_effects = [1 : i32, 2 : i32]} : memref<1000xf32, "cuda">, memref<1000xf16, "cuda">
    %26 = "byre.alias"(%alloc_25) {device = "cuda", offset = 0 : i64} : (memref<200704xi8, "cuda">) -> memref<4xf16, "cuda">
    byre.compute @ReduceSumOpf16f16(%23, %26) {device = "cuda", dimensions = dense<1> : tensor<1xi64>, memory_effects = [1 : i32, 2 : i32]} : memref<4x1000xf16, "cuda">, memref<4xf16, "cuda">
    %27 = "byre.alias"(%alloc_72) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x64x112x112xf16, "cuda">
    %28 = "byre.alias"(%alloc_41) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x64x112x112xi1, "cuda">
    byre.compute @PTXOp(%3, %27, %28) {BlockSize.x = 128 : i32, GridSize.x = 25088 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown25", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x64x112x112xf16, "cuda">, memref<4x64x112x112xf16, "cuda">, memref<4x64x112x112xi1, "cuda">
    %29 = "byre.alias"(%alloc_58) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @PoolMaxOpf16f16(%27, %29) {base_dilations = dense<1> : tensor<4xi64>, device = "cuda", memory_effects = [1 : i32, 2 : i32], padding = dense<[[0, 0], [0, 0], [1, 1], [1, 1]]> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<[1, 1, 2, 2]> : tensor<4xi64>} : memref<4x64x112x112xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    %30 = "byre.alias"(%alloc_59) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%29, %4, %30) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    %31 = "byre.alias"(%alloc_73) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%30, %arg8, %arg9, %31) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">, memref<4x64x56x56xf16, "cuda">
    %32 = "byre.alias"(%alloc_61) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    %33 = "byre.alias"(%alloc_22) {device = "cuda", offset = 0 : i64} : (memref<100352xi8, "cuda">) -> memref<4x64x56x56xi1, "cuda">
    byre.compute @PTXOp(%31, %32, %33) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown27", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xi1, "cuda">
    %34 = "byre.alias"(%alloc_63) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%32, %5, %34) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%34, %arg13, %arg14, %31) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">, memref<4x64x56x56xf16, "cuda">
    %35 = "byre.alias"(%alloc_66) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    %36 = "byre.alias"(%alloc_21) {device = "cuda", offset = 0 : i64} : (memref<100352xi8, "cuda">) -> memref<4x64x56x56xi1, "cuda">
    byre.compute @PTXOp(%31, %29, %35, %36) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown29", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xi1, "cuda">
    %37 = "byre.alias"(%alloc_65) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%35, %6, %37) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%37, %arg18, %arg19, %31) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">, memref<4x64x56x56xf16, "cuda">
    %38 = "byre.alias"(%alloc_64) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    %39 = "byre.alias"(%alloc_20) {device = "cuda", offset = 0 : i64} : (memref<100352xi8, "cuda">) -> memref<4x64x56x56xi1, "cuda">
    byre.compute @PTXOp(%31, %38, %39) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown31", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xi1, "cuda">
    %40 = "byre.alias"(%alloc_62) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%38, %7, %40) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%40, %arg23, %arg24, %31) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">, memref<4x64x56x56xf16, "cuda">
    %41 = "byre.alias"(%alloc_60) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    %42 = "byre.alias"(%alloc_19) {device = "cuda", offset = 0 : i64} : (memref<100352xi8, "cuda">) -> memref<4x64x56x56xi1, "cuda">
    byre.compute @PTXOp(%31, %35, %41, %42) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown33", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xi1, "cuda">
    %43 = "byre.alias"(%alloc_51) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%41, %8, %43) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<128x64x1x1xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    %44 = "byre.alias"(%alloc_73) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%43, %arg38, %arg39, %44) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">
    %45 = "byre.alias"(%alloc_44) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%41, %9, %45) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<128x64x3x3xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    %46 = "byre.alias"(%alloc_73) {device = "cuda", offset = 802816 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%45, %arg28, %arg29, %46) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">
    %47 = "byre.alias"(%alloc_45) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    %48 = "byre.alias"(%alloc_10) {device = "cuda", offset = 0 : i64} : (memref<50176xi8, "cuda">) -> memref<4x128x28x28xi1, "cuda">
    byre.compute @PTXOp(%46, %47, %48) {BlockSize.x = 128 : i32, GridSize.x = 3136 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown36", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xi1, "cuda">
    %49 = "byre.alias"(%alloc_46) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%47, %10, %49) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%49, %arg33, %arg34, %46) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">
    %50 = "byre.alias"(%alloc_47) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    %51 = "byre.alias"(%alloc_12) {device = "cuda", offset = 0 : i64} : (memref<50176xi8, "cuda">) -> memref<4x128x28x28xi1, "cuda">
    byre.compute @PTXOp(%46, %44, %50, %51) {BlockSize.x = 128 : i32, GridSize.x = 3136 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown38", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xi1, "cuda">
    %52 = "byre.alias"(%alloc_48) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%50, %11, %52) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%52, %arg43, %arg44, %44) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">
    %53 = "byre.alias"(%alloc_50) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    %54 = "byre.alias"(%alloc_11) {device = "cuda", offset = 0 : i64} : (memref<50176xi8, "cuda">) -> memref<4x128x28x28xi1, "cuda">
    byre.compute @PTXOp(%44, %53, %54) {BlockSize.x = 128 : i32, GridSize.x = 3136 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown40", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xi1, "cuda">
    %55 = "byre.alias"(%alloc_52) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%53, %12, %55) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%55, %arg48, %arg49, %44) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">
    %56 = "byre.alias"(%alloc_49) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    %57 = "byre.alias"(%alloc_13) {device = "cuda", offset = 0 : i64} : (memref<50176xi8, "cuda">) -> memref<4x128x28x28xi1, "cuda">
    byre.compute @PTXOp(%44, %50, %56, %57) {BlockSize.x = 128 : i32, GridSize.x = 3136 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown42", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xi1, "cuda">
    %58 = "byre.alias"(%alloc_39) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%56, %13, %58) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<256x128x1x1xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    %59 = "byre.alias"(%alloc_68) {device = "cuda", offset = 1024000 : i64} : (memref<4718592xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%58, %arg63, %arg64, %59) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">
    %60 = "byre.alias"(%alloc_42) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%56, %14, %60) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<256x128x3x3xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    %61 = "byre.alias"(%alloc_73) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%60, %arg53, %arg54, %61) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">
    %62 = "byre.alias"(%alloc_40) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    %63 = "byre.alias"(%alloc_7) {device = "cuda", offset = 0 : i64} : (memref<25088xi8, "cuda">) -> memref<4x256x14x14xi1, "cuda">
    byre.compute @PTXOp(%61, %62, %63) {BlockSize.x = 128 : i32, GridSize.x = 1568 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown45", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xi1, "cuda">
    %64 = "byre.alias"(%alloc_38) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%62, %15, %64) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%64, %arg58, %arg59, %61) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">
    %65 = "byre.alias"(%alloc_37) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    %66 = "byre.alias"(%alloc_8) {device = "cuda", offset = 0 : i64} : (memref<25088xi8, "cuda">) -> memref<4x256x14x14xi1, "cuda">
    byre.compute @PTXOp(%61, %59, %65, %66) {BlockSize.x = 128 : i32, GridSize.x = 1568 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown47", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xi1, "cuda">
    %67 = "byre.alias"(%alloc_36) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%65, %16, %67) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%67, %arg68, %arg69, %61) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">
    %68 = "byre.alias"(%alloc_35) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    %69 = "byre.alias"(%alloc_9) {device = "cuda", offset = 0 : i64} : (memref<25088xi8, "cuda">) -> memref<4x256x14x14xi1, "cuda">
    byre.compute @PTXOp(%61, %68, %69) {BlockSize.x = 128 : i32, GridSize.x = 1568 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown49", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xi1, "cuda">
    %70 = "byre.alias"(%alloc_34) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%68, %17, %70) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%70, %arg73, %arg74, %61) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">
    %71 = "byre.alias"(%alloc_33) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    %72 = "byre.alias"(%alloc_6) {device = "cuda", offset = 0 : i64} : (memref<25088xi8, "cuda">) -> memref<4x256x14x14xi1, "cuda">
    byre.compute @PTXOp(%61, %65, %71, %72) {BlockSize.x = 128 : i32, GridSize.x = 1568 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown51", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xi1, "cuda">
    %73 = "byre.alias"(%alloc_27) {device = "cuda", offset = 0 : i64} : (memref<200704xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%71, %18, %73) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<512x256x1x1xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    %74 = "byre.alias"(%alloc_73) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%73, %arg88, %arg89, %74) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">
    %75 = "byre.alias"(%alloc_53) {device = "cuda", offset = 0 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%71, %19, %75) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<512x256x3x3xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    %76 = "byre.alias"(%alloc_73) {device = "cuda", offset = 200704 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%75, %arg78, %arg79, %76) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">
    %77 = "byre.alias"(%alloc_53) {device = "cuda", offset = 200704 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    %78 = "byre.alias"(%alloc_2) {device = "cuda", offset = 0 : i64} : (memref<12544xi8, "cuda">) -> memref<4x512x7x7xi1, "cuda">
    byre.compute @PTXOp(%76, %77, %78) {BlockSize.x = 128 : i32, GridSize.x = 784 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown54", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xi1, "cuda">
    %79 = "byre.alias"(%alloc_53) {device = "cuda", offset = 401408 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%77, %20, %79) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%79, %arg83, %arg84, %76) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">
    %80 = "byre.alias"(%alloc_53) {device = "cuda", offset = 602112 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    %81 = "byre.alias"(%alloc_1) {device = "cuda", offset = 0 : i64} : (memref<12544xi8, "cuda">) -> memref<4x512x7x7xi1, "cuda">
    byre.compute @PTXOp(%76, %74, %80, %81) {BlockSize.x = 128 : i32, GridSize.x = 784 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown56", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xi1, "cuda">
    %82 = "byre.alias"(%alloc_53) {device = "cuda", offset = 802816 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @ConvOpf16f16f16(%80, %21, %82) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%82, %arg93, %arg94, %74) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">
    %83 = "byre.alias"(%alloc_28) {device = "cuda", offset = 0 : i64} : (memref<200704xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    %84 = "byre.alias"(%alloc_53) {device = "cuda", offset = 1003520 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x512x7x7xi1, "cuda">
    byre.compute @PTXOp(%74, %83, %84) {BlockSize.x = 128 : i32, GridSize.x = 784 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown58", memory_effects = [1 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xi1, "cuda">
    byre.compute @ConvOpf16f16f16(%83, %22, %74) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", lhs_dilation = dense<1> : tensor<2xi64>, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormTrainingOpf16f32f32f16(%74, %arg98, %arg99, %76) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">
    %85 = "byre.alias"(%alloc_68) {device = "cuda", offset = 1024000 : i64} : (memref<4718592xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    %86 = "byre.alias"(%alloc_53) {device = "cuda", offset = 1016064 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x512x7x7xi1, "cuda">
    byre.compute @PTXOp(%76, %80, %85, %86) {BlockSize.x = 128 : i32, GridSize.x = 784 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown60", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xi1, "cuda">
    %87 = "byre.alias"(%alloc_73) {device = "cuda", offset = 200704 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x512xf16, "cuda">
    byre.compute @ReduceSumOpf16f16(%85, %87) {device = "cuda", dimensions = dense<[3, 2]> : tensor<2xi64>, memory_effects = [1 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<4x512xf16, "cuda">
    %88 = "byre.alias"(%alloc) {device = "cuda", offset = 0 : i64} : (memref<4096xi8, "cuda">) -> memref<4x512xf16, "cuda">
    byre.compute @PTXOp(%87, %88) {BlockSize.x = 128 : i32, GridSize.x = 16 : i32, arg_ranks = [2 : i32, 2 : i32], device = "cuda", kernel_name = "Unknown61", memory_effects = [1 : i32, 2 : i32]} : memref<4x512xf16, "cuda">, memref<4x512xf16, "cuda">
    %89 = "byre.alias"(%alloc_73) {device = "cuda", offset = 200704 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x1000xf16, "cuda">
    byre.compute @MatmulOpf16f16f16(%88, %24, %89) {device = "cuda", lhs_contracting_dimension = 1 : i64, memory_effects = [1 : i32, 1 : i32, 2 : i32], rhs_contracting_dimension = 1 : i64} : memref<4x512xf16, "cuda">, memref<1000x512xf16, "cuda">, memref<4x1000xf16, "cuda">
    %90 = "byre.alias"(%alloc_53) {device = "cuda", offset = 1028608 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x1000xf16, "cuda">
    byre.compute @PTXOp(%25, %89, %90) {BlockSize.x = 128 : i32, GridSize.x = 32 : i32, arg_ranks = [1 : i32, 2 : i32, 2 : i32], device = "cuda", kernel_name = "Unknown62", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<1000xf16, "cuda">, memref<4x1000xf16, "cuda">, memref<4x1000xf16, "cuda">
    %91 = "byre.alias"(%alloc_53) {device = "cuda", offset = 1036608 : i64} : (memref<1179648xi8, "cuda">) -> memref<4xf16, "cuda">
    byre.compute @ReduceMaxOpf16f16(%90, %91) {device = "cuda", dimensions = dense<1> : tensor<1xi64>, memory_effects = [1 : i32, 2 : i32]} : memref<4x1000xf16, "cuda">, memref<4xf16, "cuda">
    %92 = "byre.alias"(%alloc_68) {device = "cuda", offset = 1024000 : i64} : (memref<4718592xi8, "cuda">) -> memref<4x1000xf16, "cuda">
    byre.compute @PTXOp(%91, %90, %92, %89) {BlockSize.x = 128 : i32, GridSize.x = 32 : i32, arg_ranks = [1 : i32, 2 : i32, 2 : i32, 2 : i32], device = "cuda", kernel_name = "Unknown63", memory_effects = [1 : i32, 1 : i32, 2 : i32, 2 : i32]} : memref<4xf16, "cuda">, memref<4x1000xf16, "cuda">, memref<4x1000xf16, "cuda">, memref<4x1000xf16, "cuda">
    %93 = "byre.alias"(%alloc_68) {device = "cuda", offset = 1032000 : i64} : (memref<4718592xi8, "cuda">) -> memref<4xf16, "cuda">
    byre.compute @ReduceSumOpf16f16(%89, %93) {device = "cuda", dimensions = dense<1> : tensor<1xi64>, memory_effects = [1 : i32, 2 : i32]} : memref<4x1000xf16, "cuda">, memref<4xf16, "cuda">
    %94 = "byre.alias"(%alloc_73) {device = "cuda", offset = 200704 : i64} : (memref<6422528xi8, "cuda">) -> memref<4xf16, "cuda">
    byre.compute @PTXOp(%93, %94) {BlockSize.x = 128 : i32, GridSize.x = 1 : i32, arg_ranks = [1 : i32, 1 : i32], device = "cuda", kernel_name = "Unknown64", memory_effects = [1 : i32, 2 : i32]} : memref<4xf16, "cuda">, memref<4xf16, "cuda">
    %95 = "byre.alias"(%alloc_0) {device = "cuda", offset = 0 : i64} : (memref<8000xi8, "cuda">) -> memref<4x1000xf16, "cuda">
    %96 = "byre.alias"(%alloc_3) {device = "cuda", offset = 0 : i64} : (memref<16000xi8, "cuda">) -> memref<4x1000xf32, "cuda">
    %97 = "byre.alias"(%alloc_4) {device = "cuda", offset = 0 : i64} : (memref<16000xi8, "cuda">) -> memref<4x1000xf32, "cuda">
    byre.compute @PTXOp(%94, %92, %26, %23, %arg1, %95, %96, %97) {BlockSize.x = 128 : i32, GridSize.x = 32 : i32, arg_ranks = [1 : i32, 2 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32, 2 : i32, 2 : i32], device = "cuda", kernel_name = "Unknown65", memory_effects = [1 : i32, 1 : i32, 1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4xf16, "cuda">, memref<4x1000xf16, "cuda">, memref<4xf16, "cuda">, memref<4x1000xf16, "cuda">, memref<4x1000xf32, "cuda">, memref<4x1000xf16, "cuda">, memref<4x1000xf32, "cuda">, memref<4x1000xf32, "cuda">
    byre.compute @MatmulOpf16f16f16(%95, %24, %87) {device = "cuda", lhs_contracting_dimension = 1 : i64, memory_effects = [1 : i32, 1 : i32, 2 : i32], rhs_contracting_dimension = 0 : i64} : memref<4x1000xf16, "cuda">, memref<1000x512xf16, "cuda">, memref<4x512xf16, "cuda">
    %98 = "byre.alias"(%alloc_24) {device = "cuda", offset = 0 : i64} : (memref<200704xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @PTXOp(%87, %86, %98) {BlockSize.x = 128 : i32, GridSize.x = 784 : i32, arg_ranks = [2 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown66", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x512xf16, "cuda">, memref<4x512x7x7xi1, "cuda">, memref<4x512x7x7xf16, "cuda">
    %99 = "byre.alias"(%alloc_25) {device = "cuda", offset = 0 : i64} : (memref<200704xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%74, %arg98, %98, %99, %arg163, %arg164) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">
    %100 = "byre.alias"(%alloc_26) {device = "cuda", offset = 0 : i64} : (memref<200704xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%99, %22, %100) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    %101 = "byre.alias"(%alloc_68) {device = "cuda", offset = 0 : i64} : (memref<4718592xi8, "cuda">) -> memref<512x512x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%83, %99, %101) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">
    byre.compute @PTXOp(%84, %100, %74) {BlockSize.x = 128 : i32, GridSize.x = 784 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown70", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x512x7x7xi1, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%82, %arg93, %74, %83, %arg160, %arg161) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%83, %21, %74) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%80, %83, %22) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">
    byre.compute @PTXOp(%98, %74, %81, %83) {BlockSize.x = 128 : i32, GridSize.x = 784 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown74", memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xi1, "cuda">, memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%79, %arg83, %83, %100, %arg154, %arg155) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%100, %20, %74) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%77, %100, %21) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512x512x3x3xf16, "cuda">
    %102 = "byre.alias"(%alloc_69) {device = "cuda", offset = 0 : i64} : (memref<4718592xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @PTXOp(%78, %74, %102) {BlockSize.x = 128 : i32, GridSize.x = 784 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown78", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x512x7x7xi1, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%75, %arg78, %102, %74, %arg151, %arg152) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">
    %103 = "byre.alias"(%alloc_53) {device = "cuda", offset = 0 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%74, %19, %103) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<512x256x3x3xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    %104 = "byre.alias"(%alloc_69) {device = "cuda", offset = 0 : i64} : (memref<4718592xi8, "cuda">) -> memref<512x256x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%71, %74, %104) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512x256x3x3xf16, "cuda">
    %105 = "byre.alias"(%alloc_69) {device = "cuda", offset = 2359296 : i64} : (memref<4718592xi8, "cuda">) -> memref<4x512x7x7xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%73, %arg88, %83, %105, %arg157, %arg158) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512xf32, "cuda">, memref<512xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%105, %18, %61) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x512x7x7xf16, "cuda">, memref<512x256x1x1xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%71, %105, %18) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<4x512x7x7xf16, "cuda">, memref<512x256x1x1xf16, "cuda">
    %106 = "byre.alias"(%alloc_67) {device = "cuda", offset = 0 : i64} : (memref<2359296xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @PTXOp(%61, %103, %72, %106) {BlockSize.x = 128 : i32, GridSize.x = 1568 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown85", memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xi1, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%70, %arg73, %106, %61, %arg148, %arg149) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">
    %107 = "byre.alias"(%alloc_69) {device = "cuda", offset = 2359296 : i64} : (memref<4718592xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%61, %17, %107) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    %108 = "byre.alias"(%alloc_53) {device = "cuda", offset = 0 : i64} : (memref<1179648xi8, "cuda">) -> memref<256x256x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%68, %61, %108) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">
    byre.compute @PTXOp(%69, %107, %61) {BlockSize.x = 128 : i32, GridSize.x = 1568 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown89", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xi1, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%67, %arg68, %61, %107, %arg145, %arg146) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%107, %16, %61) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%65, %107, %17) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">
    %109 = "byre.alias"(%alloc_54) {device = "cuda", offset = 0 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x256x14x14xf16, "cuda">
    byre.compute @PTXOp(%106, %61, %66, %109) {BlockSize.x = 128 : i32, GridSize.x = 1568 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown93", memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xi1, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%64, %arg58, %109, %107, %arg139, %arg140) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%107, %15, %61) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%62, %107, %15) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256x256x3x3xf16, "cuda">
    byre.compute @PTXOp(%63, %61, %107) {BlockSize.x = 128 : i32, GridSize.x = 1568 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown97", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x256x14x14xi1, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%60, %arg53, %107, %61, %arg136, %arg137) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">
    %110 = "byre.alias"(%alloc_67) {device = "cuda", offset = 0 : i64} : (memref<2359296xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%61, %14, %110) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<256x128x3x3xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%56, %61, %14) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256x128x3x3xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%58, %arg63, %109, %61, %arg142, %arg143) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256xf32, "cuda">, memref<256xf32, "cuda">
    %111 = "byre.alias"(%alloc_69) {device = "cuda", offset = 2359296 : i64} : (memref<4718592xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%61, %13, %111) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x256x14x14xf16, "cuda">, memref<256x128x1x1xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    %112 = "byre.alias"(%alloc_42) {device = "cuda", offset = 0 : i64} : (memref<401408xi8, "cuda">) -> memref<256x128x1x1xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%56, %61, %112) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<4x256x14x14xf16, "cuda">, memref<256x128x1x1xf16, "cuda">
    byre.compute @PTXOp(%111, %110, %57, %44) {BlockSize.x = 128 : i32, GridSize.x = 3136 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown104", memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xi1, "cuda">, memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%55, %arg48, %44, %46, %arg133, %arg134) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%46, %12, %111) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    %113 = "byre.alias"(%alloc_52) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<128x128x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%53, %46, %113) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">
    byre.compute @PTXOp(%54, %111, %46) {BlockSize.x = 128 : i32, GridSize.x = 3136 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown108", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xi1, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%52, %arg43, %46, %111, %arg130, %arg131) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%111, %11, %46) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    %114 = "byre.alias"(%alloc_50) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<128x128x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%50, %111, %114) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">
    byre.compute @PTXOp(%44, %46, %51, %110) {BlockSize.x = 128 : i32, GridSize.x = 3136 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown112", memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xi1, "cuda">, memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%49, %arg33, %110, %44, %arg124, %arg125) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%44, %10, %111) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    %115 = "byre.alias"(%alloc_52) {device = "cuda", offset = 294912 : i64} : (memref<802816xi8, "cuda">) -> memref<128x128x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%47, %44, %115) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128x128x3x3xf16, "cuda">
    byre.compute @PTXOp(%48, %111, %44) {BlockSize.x = 128 : i32, GridSize.x = 3136 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown116", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x128x28x28xi1, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">
    %116 = "byre.alias"(%alloc_67) {device = "cuda", offset = 802816 : i64} : (memref<2359296xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%45, %arg28, %44, %116, %arg121, %arg122) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">
    %117 = "byre.alias"(%alloc_69) {device = "cuda", offset = 2359296 : i64} : (memref<4718592xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%116, %9, %117) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<128x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    %118 = "byre.alias"(%alloc_52) {device = "cuda", offset = 589824 : i64} : (memref<802816xi8, "cuda">) -> memref<128x64x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%41, %116, %118) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128x64x3x3xf16, "cuda">
    %119 = "byre.alias"(%alloc_54) {device = "cuda", offset = 0 : i64} : (memref<1179648xi8, "cuda">) -> memref<4x128x28x28xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%43, %arg38, %110, %119, %arg127, %arg128) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128xf32, "cuda">, memref<128xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%119, %8, %31) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x128x28x28xf16, "cuda">, memref<128x64x1x1xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    %120 = "byre.alias"(%alloc_51) {device = "cuda", offset = 0 : i64} : (memref<802816xi8, "cuda">) -> memref<128x64x1x1xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%41, %119, %120) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<0> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<4x128x28x28xf16, "cuda">, memref<128x64x1x1xf16, "cuda">
    %121 = "byre.alias"(%alloc_67) {device = "cuda", offset = 0 : i64} : (memref<2359296xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @PTXOp(%31, %117, %42, %121) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown123", memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xi1, "cuda">, memref<4x64x56x56xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%40, %arg23, %121, %117, %arg118, %arg119) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%117, %7, %31) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    %122 = "byre.alias"(%alloc_62) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<64x64x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%38, %117, %122) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%39, %31, %117) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown127", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xi1, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%37, %arg18, %117, %31, %arg115, %arg116) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%31, %6, %117) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    %123 = "byre.alias"(%alloc_65) {device = "cuda", offset = 0 : i64} : (memref<1605632xi8, "cuda">) -> memref<64x64x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%35, %31, %123) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%121, %117, %36, %31) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown131", memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xi1, "cuda">, memref<4x64x56x56xf16, "cuda">
    %124 = "byre.alias"(%alloc_73) {device = "cuda", offset = 1605632 : i64} : (memref<6422528xi8, "cuda">) -> memref<4x64x56x56xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%34, %arg13, %31, %124, %arg112, %arg113) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%124, %5, %117) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    %125 = "byre.alias"(%alloc_67) {device = "cuda", offset = 0 : i64} : (memref<2359296xi8, "cuda">) -> memref<64x64x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%32, %124, %125) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%33, %117, %124) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown135", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xi1, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%30, %arg8, %124, %117, %arg109, %arg110) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">
    byre.compute @ConvBackwardDataOpf16f16f16(%117, %4, %124) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    %126 = "byre.alias"(%alloc_67) {device = "cuda", offset = 73728 : i64} : (memref<2359296xi8, "cuda">) -> memref<64x64x3x3xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%29, %117, %126) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<1> : tensor<4xi64>, window_strides = dense<1> : tensor<2xi64>} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<64x64x3x3xf16, "cuda">
    byre.compute @PTXOp(%31, %124, %117) {BlockSize.x = 128 : i32, GridSize.x = 6272 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown139", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x56x56xf16, "cuda">
    byre.compute @PoolMaxGradOpf16f16f16(%27, %117, %3) {device = "cuda", memory_effects = [1 : i32, 1 : i32, 2 : i32], padding = dense<[[0, 0], [0, 0], [1, 1], [1, 1]]> : tensor<4x2xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<[1, 1, 2, 2]> : tensor<4xi64>} : memref<4x64x112x112xf16, "cuda">, memref<4x64x56x56xf16, "cuda">, memref<4x64x112x112xf16, "cuda">
    byre.compute @PTXOp(%28, %3, %27) {BlockSize.x = 128 : i32, GridSize.x = 25088 : i32, arg_ranks = [4 : i32, 4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown140", memory_effects = [1 : i32, 1 : i32, 2 : i32]} : memref<4x64x112x112xi1, "cuda">, memref<4x64x112x112xf16, "cuda">, memref<4x64x112x112xf16, "cuda">
    byre.compute @BatchNormGradOpf16f32f16f16f32f32(%2, %arg3, %27, %3, %arg106, %arg107) {device = "cuda", epsilon = 9.99999974E-6 : f32, feature_index = 1 : i64, memory_effects = [1 : i32, 1 : i32, 1 : i32, 2 : i32, 2 : i32, 2 : i32]} : memref<4x64x112x112xf16, "cuda">, memref<64xf32, "cuda">, memref<4x64x112x112xf16, "cuda">, memref<4x64x112x112xf16, "cuda">, memref<64xf32, "cuda">, memref<64xf32, "cuda">
    %127 = "byre.alias"(%alloc_74) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<64x3x7x7xf16, "cuda">
    byre.compute @ConvBackwardFilterOpf16f16f16(%0, %3, %127) {batch_group_count = 1 : i64, device = "cuda", feature_group_count = 1 : i64, input_layout = "NCHW", kernel_layout = "NCHW", memory_effects = [1 : i32, 1 : i32, 2 : i32], output_layout = "NCHW", padding = dense<3> : tensor<4xi64>, window_strides = dense<2> : tensor<2xi64>} : memref<4x3x224x224xf16, "cuda">, memref<4x64x112x112xf16, "cuda">, memref<64x3x7x7xf16, "cuda">
    %128 = "byre.alias"(%alloc_74) {device = "cuda", offset = 18816 : i64} : (memref<6422528xi8, "cuda">) -> memref<f32, "cuda">
    byre.compute @ReduceSumOpf32f32(%96, %128) {device = "cuda", dimensions = dense<[0, 1]> : tensor<2xi64>, memory_effects = [1 : i32, 2 : i32]} : memref<4x1000xf32, "cuda">, memref<f32, "cuda">
    byre.compute @PTXOp(%128, %arg104) {BlockSize.x = 128 : i32, GridSize.x = 1 : i32, arg_ranks = [0 : i32, 0 : i32], device = "cuda", kernel_name = "Unknown143", memory_effects = [1 : i32, 2 : i32]} : memref<f32, "cuda">, memref<f32, "cuda">
    byre.compute @PTXOp(%127, %arg105) {BlockSize.x = 128 : i32, GridSize.x = 74 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown144", memory_effects = [1 : i32, 2 : i32]} : memref<64x3x7x7xf16, "cuda">, memref<64x3x7x7xf32, "cuda">
    byre.compute @PTXOp(%126, %arg108) {BlockSize.x = 128 : i32, GridSize.x = 288 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown145", memory_effects = [1 : i32, 2 : i32]} : memref<64x64x3x3xf16, "cuda">, memref<64x64x3x3xf32, "cuda">
    byre.compute @PTXOp(%125, %arg111) {BlockSize.x = 128 : i32, GridSize.x = 288 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown146", memory_effects = [1 : i32, 2 : i32]} : memref<64x64x3x3xf16, "cuda">, memref<64x64x3x3xf32, "cuda">
    byre.compute @PTXOp(%123, %arg114) {BlockSize.x = 128 : i32, GridSize.x = 288 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown147", memory_effects = [1 : i32, 2 : i32]} : memref<64x64x3x3xf16, "cuda">, memref<64x64x3x3xf32, "cuda">
    byre.compute @PTXOp(%122, %arg117) {BlockSize.x = 128 : i32, GridSize.x = 288 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown148", memory_effects = [1 : i32, 2 : i32]} : memref<64x64x3x3xf16, "cuda">, memref<64x64x3x3xf32, "cuda">
    byre.compute @PTXOp(%118, %arg120) {BlockSize.x = 128 : i32, GridSize.x = 576 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown149", memory_effects = [1 : i32, 2 : i32]} : memref<128x64x3x3xf16, "cuda">, memref<128x64x3x3xf32, "cuda">
    byre.compute @PTXOp(%115, %arg123) {BlockSize.x = 128 : i32, GridSize.x = 1152 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown150", memory_effects = [1 : i32, 2 : i32]} : memref<128x128x3x3xf16, "cuda">, memref<128x128x3x3xf32, "cuda">
    byre.compute @PTXOp(%120, %arg126) {BlockSize.x = 128 : i32, GridSize.x = 64 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown151", memory_effects = [1 : i32, 2 : i32]} : memref<128x64x1x1xf16, "cuda">, memref<128x64x1x1xf32, "cuda">
    byre.compute @PTXOp(%114, %arg129) {BlockSize.x = 128 : i32, GridSize.x = 1152 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown152", memory_effects = [1 : i32, 2 : i32]} : memref<128x128x3x3xf16, "cuda">, memref<128x128x3x3xf32, "cuda">
    byre.compute @PTXOp(%113, %arg132) {BlockSize.x = 128 : i32, GridSize.x = 1152 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown153", memory_effects = [1 : i32, 2 : i32]} : memref<128x128x3x3xf16, "cuda">, memref<128x128x3x3xf32, "cuda">
    byre.compute @PTXOp(%14, %arg135) {BlockSize.x = 128 : i32, GridSize.x = 2304 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown154", memory_effects = [1 : i32, 2 : i32]} : memref<256x128x3x3xf16, "cuda">, memref<256x128x3x3xf32, "cuda">
    byre.compute @PTXOp(%15, %arg138) {BlockSize.x = 128 : i32, GridSize.x = 4608 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown155", memory_effects = [1 : i32, 2 : i32]} : memref<256x256x3x3xf16, "cuda">, memref<256x256x3x3xf32, "cuda">
    byre.compute @PTXOp(%112, %arg141) {BlockSize.x = 128 : i32, GridSize.x = 256 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown156", memory_effects = [1 : i32, 2 : i32]} : memref<256x128x1x1xf16, "cuda">, memref<256x128x1x1xf32, "cuda">
    byre.compute @PTXOp(%17, %arg144) {BlockSize.x = 128 : i32, GridSize.x = 4608 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown157", memory_effects = [1 : i32, 2 : i32]} : memref<256x256x3x3xf16, "cuda">, memref<256x256x3x3xf32, "cuda">
    byre.compute @PTXOp(%108, %arg147) {BlockSize.x = 128 : i32, GridSize.x = 4608 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown158", memory_effects = [1 : i32, 2 : i32]} : memref<256x256x3x3xf16, "cuda">, memref<256x256x3x3xf32, "cuda">
    byre.compute @PTXOp(%104, %arg150) {BlockSize.x = 128 : i32, GridSize.x = 9216 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown159", memory_effects = [1 : i32, 2 : i32]} : memref<512x256x3x3xf16, "cuda">, memref<512x256x3x3xf32, "cuda">
    byre.compute @PTXOp(%21, %arg153) {BlockSize.x = 128 : i32, GridSize.x = 18432 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown160", memory_effects = [1 : i32, 2 : i32]} : memref<512x512x3x3xf16, "cuda">, memref<512x512x3x3xf32, "cuda">
    byre.compute @PTXOp(%18, %arg156) {BlockSize.x = 128 : i32, GridSize.x = 1024 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown161", memory_effects = [1 : i32, 2 : i32]} : memref<512x256x1x1xf16, "cuda">, memref<512x256x1x1xf32, "cuda">
    byre.compute @PTXOp(%22, %arg159) {BlockSize.x = 128 : i32, GridSize.x = 18432 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown162", memory_effects = [1 : i32, 2 : i32]} : memref<512x512x3x3xf16, "cuda">, memref<512x512x3x3xf32, "cuda">
    byre.compute @PTXOp(%101, %arg162) {BlockSize.x = 128 : i32, GridSize.x = 18432 : i32, arg_ranks = [4 : i32, 4 : i32], device = "cuda", kernel_name = "Unknown163", memory_effects = [1 : i32, 2 : i32]} : memref<512x512x3x3xf16, "cuda">, memref<512x512x3x3xf32, "cuda">
    %129 = "byre.alias"(%alloc_74) {device = "cuda", offset = 0 : i64} : (memref<6422528xi8, "cuda">) -> memref<1000x512xf16, "cuda">
    byre.compute @MatmulOpf16f16f16(%88, %95, %129) {device = "cuda", lhs_contracting_dimension = 0 : i64, memory_effects = [1 : i32, 1 : i32, 2 : i32], output_transpose, rhs_contracting_dimension = 0 : i64} : memref<4x512xf16, "cuda">, memref<4x1000xf16, "cuda">, memref<1000x512xf16, "cuda">
    byre.compute @PTXOp(%129, %arg165) {BlockSize.x = 128 : i32, GridSize.x = 4000 : i32, arg_ranks = [2 : i32, 2 : i32], device = "cuda", kernel_name = "Unknown165", memory_effects = [1 : i32, 2 : i32]} : memref<1000x512xf16, "cuda">, memref<1000x512xf32, "cuda">
    byre.compute @ReduceSumOpf32f32(%97, %arg166) {device = "cuda", dimensions = dense<0> : tensor<1xi64>, memory_effects = [1 : i32, 2 : i32]} : memref<4x1000xf32, "cuda">, memref<1000xf32, "cuda">
    return
  }
}

