//===- CanonicalizeMatmulEpilogue.cpp ---------------------------*---C++-*-===//
//
// Copyright 2024 ByteDance Ltd. and/or its affiliates. All rights reserved.
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//===----------------------------------------------------------------------===//

#include "byteir/Dialect/Linalg/Transforms/CanonicalizeMatmulEpilogue.h"
#include "byteir/Dialect/GPU/Transforms/Utils.h"
#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/Dialect/Linalg/IR/Linalg.h"
#include "mlir/Dialect/Linalg/Transforms/Transforms.h"
#include "mlir/Dialect/Linalg/Utils/Utils.h"
#include "mlir/Dialect/SCF/IR/SCF.h"
#include "mlir/Dialect/Tensor/IR/Tensor.h"
#include "mlir/Dialect/Utils/StructuredOpsUtils.h"

#include "PassDetail.h"

using namespace llvm;
using namespace mlir;

#define DEBUG_TYPE "canonicalize-matmul-epilogue"

namespace {

static LogicalResult
modifyUseToGetValueIntoStoreSet(RewriterBase &rewriter,
                                linalg::GenericOp genericOp) {
  SmallVector<Value> newInputs;
  SmallVector<Value> newOutputs;
  SmallVector<Type> newResultTypes;
  SmallVector<AffineMap> maps;
  OpOperand *inOperand = nullptr;
  OpOperand *initOperand = nullptr;
  for (OpOperand *in : genericOp.getDpsInputOperands()) {
    // if operand is generated by a op which has MainLoop Marker or it's a
    // linalg.matmul
    if (hasMarker(
            in->get().getDefiningOp(),
            ArrayRef{getMatmulMainLoopMarker(), getMMAPatternAttrName()})) {
      inOperand = in;
    } else {
      newInputs.push_back(in->get());
      maps.push_back(genericOp.getMatchingIndexingMap(in));
    }
  }
  // assert has only one dps init
  if (genericOp.getNumDpsInits() != 1)
    return failure();
  initOperand = genericOp.getDpsInitOperand(0);

  if (inOperand == nullptr || initOperand == nullptr)
    return failure();
  maps.push_back(genericOp.getMatchingIndexingMap(inOperand));
  newOutputs.push_back(inOperand->get());
  newResultTypes.push_back(inOperand->get().getType());

  OpBuilder::InsertionGuard g(rewriter);
  rewriter.setInsertionPoint(genericOp);

  Location loc = genericOp.getLoc();
  SmallVector<utils::IteratorType> iterTypes(genericOp.getNumLoops(),
                                             utils::IteratorType::parallel);
  auto newOp = rewriter.create<linalg::GenericOp>(
      loc, newResultTypes, newInputs, newOutputs, maps, iterTypes,
      /*bodyBuild=*/nullptr, linalg::getPrunedAttributeList(genericOp));
  rewriter.inlineRegionBefore(genericOp.getRegion(), newOp.getRegion(),
                              newOp.getRegion().begin());

  // Repair the payload entry block.
  Block &payload = newOp.getRegion().front();
  payload.getArgument(inOperand->getOperandNumber())
      .replaceAllUsesWith(payload.getArgument(initOperand->getOperandNumber()));
  payload.eraseArgument(inOperand->getOperandNumber());

  rewriter.replaceOp(genericOp, newOp.getResults());
  return success();
}

// This pass modify IR on linalg tensor level.
// 1. Modify epilogue linalg.generic to avoid write result to a new buffer.
// Actually we can reuse input buffer.
// 2. Use shared_outs argument to replace tensor.empty buffer in scf.forall. As
// the thread block will not modify different slice of tensor.
class CanonicalizeMatmulEpiloguePass
    : public CanonicalizeMatmulEpilogueBase<CanonicalizeMatmulEpiloguePass> {
public:
  CanonicalizeMatmulEpiloguePass() = default;

  void runOnOperation() override {
    func::FuncOp funcOp = getOperation();
    if (!hasGemmTileConfig(funcOp))
      return;
    auto forallOptional = getForallOpMappedToBlock(funcOp);
    if (!forallOptional)
      return;
    scf::ForallOp forallOp = *forallOptional;

    SmallVector<linalg::GenericOp> epilogueOps;

    // find epilogue, linalg.generic with getEpilogueMarker
    forallOp.walk([&](linalg::GenericOp genericOp) {
      if (!hasMarker(genericOp, getEpilogueMarker()))
        return;
      epilogueOps.push_back(genericOp);
    });

    if (epilogueOps.empty()) {
      return;
    }
    assert(epilogueOps.size() == 1);
    linalg::GenericOp epilogueOp = epilogueOps[0];
    IRRewriter rewriter(epilogueOp);

    // modify the epilogue to get the value into the store set
    if (failed(modifyUseToGetValueIntoStoreSet(rewriter, epilogueOp))) {
      llvm::errs() << "failed in modifyUseToGetValueIntoStoreSet\n";
      return signalPassFailure();
    }

    // get scf.forall shared_outs
    auto forallSharedOuts = forallOp.getRegionOutArgs();
    auto forallDpsInits = forallOp.getDpsInitsMutable();
    for (const auto &[sharedOut, dpsInit] :
         llvm::zip(forallSharedOuts, forallDpsInits)) {
      // Get sharedOut's defining op and replace defining op in scf.forall
      Value emptyValueOptional = dpsInit.get();
      tensor::EmptyOp emptyOp =
          emptyValueOptional.getDefiningOp<tensor::EmptyOp>();
      if (emptyOp == nullptr)
        continue;

      emptyValueOptional.replaceUsesWithIf(
          sharedOut, [&](OpOperand &opOperand) {
            // Only replace uses in the forall block
            return opOperand.getOwner()->getBlock() == forallOp.getBody();
          });
    }
  }
};
} // namespace

std::unique_ptr<OperationPass<func::FuncOp>>
mlir::createCanonicalizeMatmulEpiloguePass() {
  return std::make_unique<CanonicalizeMatmulEpiloguePass>();
}